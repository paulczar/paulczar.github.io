<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Paul Czarkowski</title>
    <link>http://tech.paulcz.net/</link>
    <description>Recent content on Paul Czarkowski</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 02 Jan 2016 14:44:30 -0600</lastBuildDate>
    <atom:link href="http://tech.paulcz.net/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Deploying a HA Docker Swarm Cluster</title>
      <link>http://tech.paulcz.net/2016/01/running-ha-docker-swarm/</link>
      <pubDate>Sat, 02 Jan 2016 14:44:30 -0600</pubDate>
      
      <guid>http://tech.paulcz.net/2016/01/running-ha-docker-swarm/</guid>
      <description>

&lt;p&gt;Given Docker&amp;rsquo;s propensity for creating easy to use tools it shouldn&amp;rsquo;t come as a surprise that Docker Swarm is one of the easier to understand and run of the &amp;ldquo;Docker Clustering&amp;rdquo; options currently out there. I recently built some &lt;a href=&#34;http://terraform.io&#34;&gt;Terraform&lt;/a&gt; configs for deploying a &lt;a href=&#34;https://github.com/openstack/osops-tools-contrib/tree/master/terraform/dockerswarm-coreos&#34;&gt;Highly Available Docker Swarm cluster on Openstack&lt;/a&gt; and learned a fair bit about Swarm in the process.&lt;/p&gt;

&lt;p&gt;This guide is meant to be a platform agnostic howto on installing and running a Highly Available Docker Swarm to show you the ideas and concepts that may not be as easy to understand from just reading some config management code.&lt;/p&gt;

&lt;h2 id=&#34;coreos:f092910207480b0f11ab4bcafecee8ad&#34;&gt;CoreOS&lt;/h2&gt;

&lt;p&gt;The reason for using &lt;a href=&#34;http://coreos.com&#34;&gt;CoreOS&lt;/a&gt; here is that to make Swarm run in High Availability mode as well as being able to support docker networking between hosts we need to use service discovery.  We can choose to use &lt;code&gt;etcd&lt;/code&gt;, &lt;code&gt;consul&lt;/code&gt;, or &lt;code&gt;zookeeper&lt;/code&gt; here, CoreOS comes with &lt;code&gt;etcd&lt;/code&gt; thus makes it an excellent choice for running Docker Swarm.&lt;/p&gt;

&lt;p&gt;You will need three servers capable of running &lt;a href=&#34;http://coreos.com&#34;&gt;CoreOS&lt;/a&gt;.  See the &amp;ldquo;Try Out CoreOS&amp;rdquo; section of their website for various installation methods for different infrastructure. For this guide I will use the official &lt;a href=&#34;https://github.com/coreos/coreos-vagrant&#34;&gt;CoreOS Vagrant Example&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;skip the rest of this section if you install CoreOS for a different platform&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Clone down the Vagrant example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/coreos/coreos-vagrant.git vagrant-docker-swarm 
Cloning into &#39;vagrant-docker-swarm&#39;...
remote: Counting objects: 411, done.
remote: Total 411 (delta 0), reused 0 (delta 0), pack-reused 411
Receiving objects: 100% (411/411), 100.33 KiB | 0 bytes/s, done.
Resolving deltas: 100% (181/181), done.
Checking connectivity... done.
cd vagrant-docker-swarm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Edit the &lt;code&gt;Vagrantfile&lt;/code&gt; to set &lt;code&gt;$num_instances = 3&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;on Unix-like systems you can do this easily with sed&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sed -i &#39;s/\$num_instances = 1/\$num_instances = 3/&#39; Vagrantfile
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Get a new etcd discovery-url:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;if you are on a windows box and don&amp;rsquo;t have curl you can paste the url into a web browser to get the discovery-url&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl https://discovery.etcd.io/new\?size\=3
https://discovery.etcd.io/6a9c62105f04dac40a29b90fbed322ef
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a cloud-init file called &lt;code&gt;user-data&lt;/code&gt; in the base of the repo using the discovery-url from above:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#cloud-config

coreos:
  etcd2:
    discovery: https://discovery.etcd.io/888fd1e440faf680a7abb3fd934da6fd
    advertise-client-urls: http://$public_ipv4:2379
    initial-advertise-peer-urls: http://$public_ipv4:2380
    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
    listen-peer-urls: http://$public_ipv4:2380,http://$public_ipv4:7001
  units:
    - name: etcd2.service
      command: start

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Start up the CoreOS VMs and log into the first one to check everything worked ok:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vagrant up
Bringing machine &#39;core-01&#39; up with &#39;virtualbox&#39; provider...
Bringing machine &#39;core-02&#39; up with &#39;virtualbox&#39; provider...
Bringing machine &#39;core-03&#39; up with &#39;virtualbox&#39; provider...
...
$ vagrant ssh core-01
$ etcdctl member list
3c5901a3db54efa3: name=f1bae7bba7714ed7b4585c6b1256ddb2 peerURLs=http://172.17.8.101:2380 clientURLs=http://172.17.8.101:2379
9eeb141350af8439: name=5c8e57890d114d7d9d7aef662033a6e0 peerURLs=http://172.17.8.103:2380 clientURLs=http://172.17.8.103:2379
ebcc652087dfe6e8: name=de426249d3b34e23a5706d99b4900665 peerURLs=http://172.17.8.102:2380 clientURLs=http://172.17.8.102:2379
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;docker-swarm:f092910207480b0f11ab4bcafecee8ad&#34;&gt;Docker Swarm&lt;/h2&gt;

&lt;p&gt;Now that we have several CoreOS servers with a working etcd cluster we can move on to setting up Docker Swarm.&lt;/p&gt;

&lt;p&gt;We need to modify docker to listen on tcp port &lt;code&gt;2376&lt;/code&gt; as well as registering itself to service discovery (which will allow us to set up overlay networking later on).  We do this by creating a file &lt;code&gt;custom.conf&lt;/code&gt; in &lt;code&gt;/etc/systemd/system/docker.service.d/&lt;/code&gt; on each server.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;if not using vagrant change &lt;code&gt;eth1&lt;/code&gt; to match the primary interface for your server&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Service]
Environment=&amp;quot;DOCKER_OPTS=-H=0.0.0.0:2376 -H unix:///var/run/docker.sock --cluster-advertise eth1:2376 --cluster-store etcd://127.0.0.1:2379&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then need to reload the &lt;code&gt;systemctl&lt;/code&gt; daemon and then restart docker for these changes to take effect.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo systemctl daemon-reload
sudo systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check that you can access docker via tcp on one of your hosts:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H tcp://172.17.8.101:2376 info
Containers: 0
Images: 0
Engine Version: 1.9.1
Storage Driver: overlay
 Backing Filesystem: extfs
Execution Driver: native-0.2
Logging Driver: json-file
Kernel Version: 4.3.3-coreos
Operating System: CoreOS 899.1.0
CPUs: 1
Total Memory: 997.4 MiB
Name: core-01
ID: BK64:WF3J:5JU6:VYLI:YJSO:CAQH:HPYM:MPTG:FMTA:VLE3:HSMP:F4VQ
Cluster store: etcd://127.0.0.1:2379/docker

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;rsquo;re now ready to run Docker Swarm itself. There are two extra components to running Docker Swarm, a Swarm Agent and a Swarm Manager.&lt;/p&gt;

&lt;p&gt;The Swarm Agent watches the local Docker service via it&amp;rsquo;s TCP port and registers it into service discovery (etcd in our case).  We will run this on each server like so:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;set the &amp;ndash;addr= argument to match the primary IP of each node&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d --name swarm-agent \
    --net=host swarm:latest \
        join --addr=172.17.8.101:2376 \
        etcd://127.0.0.1:2379
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The Swarm Manager watches service discovery and exposes a TCP port (2375) which when accessed by a Docker client will perform actions and schedule containers across the Swarm cluster.&lt;/p&gt;

&lt;p&gt;To ensure High Availability of our cluster we&amp;rsquo;ll run a Swarm Manager on each server:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d --name swarm-manager 
    --net=host swarm:latest manage \
    etcd://127.0.0.1:2379
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Assuming everything went smoothly we can now access the swarm cluster via the Swarm Managers TCP port on any of the servers:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H tcp://172.17.8.101:2375 info
Containers: 6
Images: 5
Role: primary
Strategy: spread
Filters: health, port, dependency, affinity, constraint
Nodes: 3
 core-01: 172.17.8.101:2376
  └ Status: Healthy
  └ Containers: 2
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 1.023 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=4.3.3-coreos, operatingsystem=CoreOS 899.1.0, storagedriver=overlay
 core-02: 172.17.8.102:2376
  └ Status: Healthy
  └ Containers: 2
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 1.023 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=4.3.3-coreos, operatingsystem=CoreOS 899.1.0, storagedriver=overlay
 core-03: 172.17.8.103:2376
  └ Status: Healthy
  └ Containers: 2
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 1.023 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=4.3.3-coreos, operatingsystem=CoreOS 899.1.0, storagedriver=overlay
CPUs: 3
Total Memory: 3.068 GiB
Name: core-01
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our next step is to create an overlay network using the &lt;code&gt;docker network&lt;/code&gt; command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H tcp://172.17.8.101:2375 network create --driver overlay my-net
614913b275dee43a63b48d08b4f5e52f7c0e531d70c63eeb8bb35624470da0c4

$ docker -H tcp://172.17.8.101:2375 network ls                            
NETWORK ID          NAME                DRIVER
86ecb0cf32c6        core-02/none        null                
c7a291ed8366        core-01/host        host                
3747364c5961        core-03/none        null                
8245d6d3ac67        core-02/host        host                
614913b275de        my-net              overlay             
61ead145e9dd        core-01/bridge      bridge              
c9457c4f4588        core-03/bridge      bridge              
b8a6c75cb3b9        core-03/host        host                
bdc4d5ccd778        core-02/bridge      bridge              
66afdc892361        core-01/none        null
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally we&amp;rsquo;ll create a Container on one host and then check that it is accessible from another:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;replace the node==XXXX argument with the hostname of one of your hosts, make sure to use a different node for each docker command&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -it --name=web --net=my-net \
    -H tcp://172.17.8.101:2375 \
    --env=&amp;quot;constraint:node==core-01&amp;quot; nginx
e0fe18c946a5692806608f939d4d6f31c670e3f42bf3942a77142bed2095983e

$ docker run -it --rm --net=my-net \
    -H tcp://172.17.8.101:2375 \
    --env=&amp;quot;constraint:node==core02&amp;quot; busybox wget -O- http://web
Connecting to web (10.0.0.2:80)
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you&amp;rsquo;ve been following along you have successfully deployed a Highly Available Docker Swarm cluster.  From here you could use a load balancer to load balance the Swarm Manager port (2375) or even use Round Robin DNS.&lt;/p&gt;

&lt;p&gt;You may have notice there is no authentication or authorization on this and anybody with a Docker binary and TCP access to your hosts could spin up docker containers. This is fairly easily fixed by using Docker&amp;rsquo;s TLS cert based authorization, I&amp;rsquo;ll cover setting that up in a future blog post.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Openstacks and Ecosystems</title>
      <link>http://tech.paulcz.net/2016/01/openstacks-and-ecosystems/</link>
      <pubDate>Sat, 02 Jan 2016 13:00:42 -0600</pubDate>
      
      <guid>http://tech.paulcz.net/2016/01/openstacks-and-ecosystems/</guid>
      <description>&lt;p&gt;I have recently had a number of lengthy discussions on the &lt;a href=&#34;https://twitter.com/zehicle/status/678736665792356352&#34;&gt;Twitter&lt;/a&gt; about Interop, Users, and Ecosystems. Specifically about our need to focus on the OpenStack ecosystem to extend the OpenStack IaaS user experience to something a bit more platform[ish].&lt;/p&gt;

&lt;p&gt;I wrote a post for &lt;a href=&#34;http://sysadvent.blogspot.com/2015/12/day-16-merry-paasmas-and-very.html&#34;&gt;SysAdvent&lt;/a&gt; this year on developing applications on top of OpenStack using a collection of OpenSource tools to create a PaaS and CI/CD pipelines. I think it turned out quite well and really helped reinforce my beliefs on the subject.&lt;/p&gt;

&lt;p&gt;My buddy and future OpenStack Board member &lt;a href=&#34;https://twitter.com/jjasghar&#34;&gt;JJ Asghar&lt;/a&gt; has been spearheading a new &lt;a href=&#34;https://wiki.openstack.org/wiki/Osops&#34;&gt;OpenStack Operators Project&lt;/a&gt;. I plan to contribute to this project by creating some examples of deploying tools that provide higher level services on top of the OpenStack IaaS layer.&lt;/p&gt;

&lt;p&gt;Given that I am very bullish about the &lt;a href=&#34;http://docker.com&#34;&gt;Docker&lt;/a&gt; ecosystem it makes sense that my first contribution would be focussed on running one of the several &amp;ldquo;Docker container scheduling/cluster&amp;rdquo; tools.&lt;/p&gt;

&lt;p&gt;After playing around with a few of them, I settled on starting with Docker Swarm as its one of the easier to understand and run and doesn&amp;rsquo;t require any special tooling other than a recent install of the Docker binary to use.&lt;/p&gt;

&lt;p&gt;To increase simplicity I chose to use Hashicorp&amp;rsquo;s &lt;a href=&#34;http://terraform.io&#34;&gt;Terraform&lt;/a&gt; and use only the most basic of the OpenStack services to ensure a fairly high likelyhood that it will run on most fairly up to date OpenStack clouds.&lt;/p&gt;

&lt;p&gt;Based on the project&amp;rsquo;s suggestion I posted the Terraform files up to the &lt;a href=&#34;https://github.com/openstack/osops-tools-contrib/tree/master/terraform/dockerswarm-coreos&#34;&gt;osops-tools-contrib&lt;/a&gt; along with fairly comprehensive documentation on using it.&lt;/p&gt;

&lt;p&gt;I hope this and future work I plan to do to create similar examples will help the OpenStack Community out in some small way.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Optimizing your Dockerfiles</title>
      <link>http://tech.paulcz.net/2015/03/optimizing-your-dockerfiles/</link>
      <pubDate>Sat, 07 Mar 2015 13:25:29 -0600</pubDate>
      
      <guid>http://tech.paulcz.net/2015/03/optimizing-your-dockerfiles/</guid>
      <description>

&lt;p&gt;Docker images are &amp;ldquo;supposed&amp;rdquo; to be small and fast. However unless you&amp;rsquo;re precompiling GO binaries and dropping them in the &lt;code&gt;busybox&lt;/code&gt; image they can get quite large and complicated. Without a well constructed &lt;code&gt;Dockerfile&lt;/code&gt; to improve build cache hits your docker builds can become unnecessarily slow.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Dockerfile&lt;/code&gt;&amp;rsquo;s are regularly [and incorrectly] treated like &lt;code&gt;bash&lt;/code&gt; scripts and therefore are often written out as a series of commands which you would &lt;code&gt;curl | sudo bash&lt;/code&gt; from a website to install.  This usually makes for an inefficient and slow &lt;code&gt;Dockerfile&lt;/code&gt;&lt;/p&gt;

&lt;!--more --&gt;

&lt;h2 id=&#34;order-matters:6226be07e9066526e21065089ae006e9&#34;&gt;Order Matters&lt;/h2&gt;

&lt;p&gt;When you&amp;rsquo;re building a new &lt;code&gt;Dockerfile&lt;/code&gt; for an application there can be a lot of trial and error in determining what packages are needed and what commands need to run. Optimizing your &lt;code&gt;Dockerfile&lt;/code&gt; ensures that the build cache will hit more often and each build between changes will be faster.&lt;/p&gt;

&lt;p&gt;The general rule of thumb is to sort your commands by frequency of change, the time it takes to run the command and how sharable it is with other images.&lt;/p&gt;

&lt;p&gt;This means that commands like &lt;code&gt;WORKDIR&lt;/code&gt;, &lt;code&gt;CMD&lt;/code&gt;, &lt;code&gt;ENV&lt;/code&gt; should go towards the bottom while a &lt;code&gt;RUN apt-get -y update&lt;/code&gt; should go towards the top as it takes longer to run and can be shared with all of your images.&lt;/p&gt;

&lt;p&gt;Finally any &lt;code&gt;ADD&lt;/code&gt; ( or other commands that invalidate cache ) commands should go as far down the bottom as possible as this is where you&amp;rsquo;re likely to make lots of changes that will invalidate the cache of subsequent commands.&lt;/p&gt;

&lt;h2 id=&#34;choose-your-base-image-wisely:6226be07e9066526e21065089ae006e9&#34;&gt;Choose your base image wisely&lt;/h2&gt;

&lt;p&gt;There&amp;rsquo;s a lot of base images to choose from from the bare OS images like &lt;code&gt;ubuntu:trusty&lt;/code&gt; to application specific ones for &lt;code&gt;python:2&lt;/code&gt; or &lt;code&gt;java:7&lt;/code&gt;.  Common sense might tell you to use &lt;code&gt;ruby:2&lt;/code&gt; to run an ruby based app and &lt;code&gt;python:3&lt;/code&gt; to run a python app.  However now you have two base images with little in common that you need to download and build.  Instead if you use &lt;code&gt;ubuntu:trusty&lt;/code&gt; for both then you only need to download the base image once.&lt;/p&gt;

&lt;h2 id=&#34;use-layers-to-your-advantage:6226be07e9066526e21065089ae006e9&#34;&gt;Use Layers to your advantage&lt;/h2&gt;

&lt;p&gt;Each command in a &lt;code&gt;Dockerfile&lt;/code&gt; is an extra layer. You can very quickly end up with an image that&amp;rsquo;s 30+ layers.  This is not necessarily a problem, but by joining &lt;code&gt;RUN&lt;/code&gt; commands together, and using a single &lt;code&gt;EXPOSE&lt;/code&gt; line to list all of your open ports you can reduce the number of layers.&lt;/p&gt;

&lt;p&gt;By grouping &lt;code&gt;RUN&lt;/code&gt; commands together intelligently you can share more layers between containers.  Of course if you have a common set of packages across multiple containers then you should look at creating a seperate base image containing these that all of your images are built from.&lt;/p&gt;

&lt;p&gt;For each layer that you can share across multiple images you can save a ton of disk space.&lt;/p&gt;

&lt;h2 id=&#34;volume-contaimers:6226be07e9066526e21065089ae006e9&#34;&gt;Volume contaimers&lt;/h2&gt;

&lt;p&gt;If you use Volume containers,  don&amp;rsquo;t bother trying to save space by using a small image,  Use the image of the application you&amp;rsquo;ll be serving data to.  If you do that and &lt;code&gt;docker commit&lt;/code&gt; the data volume you not only have your data commited to the container, but the actual application as well which is very useful for debugging.&lt;/p&gt;

&lt;h2 id=&#34;cheat:6226be07e9066526e21065089ae006e9&#34;&gt;Cheat&lt;/h2&gt;

&lt;p&gt;If you&amp;rsquo;ve built an image and discover when you run it that there&amp;rsquo;s a package missing add it to the bottom of your &lt;code&gt;Dockerfile&lt;/code&gt; rather than in the &lt;code&gt;RUN apt-get&lt;/code&gt; command at the top.  This means you can rebuild the image faster.  Once your image is correct and working you can reorganize your &lt;code&gt;Dockerfile&lt;/code&gt; to clean such changes up before commiting it to source control.&lt;/p&gt;

&lt;h2 id=&#34;example:6226be07e9066526e21065089ae006e9&#34;&gt;Example&lt;/h2&gt;

&lt;p&gt;A &lt;code&gt;Dockerfile&lt;/code&gt; for installing graphite would look something like this if it was written like a &lt;code&gt;bash&lt;/code&gt; script:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM ubuntu:trusty
MAINTAINER Paul Czarkowski &amp;quot;paul@paulcz.net&amp;quot;

RUN apt-get -yq update

# Apache
RUN \
  apt-get -yqq install \
    apache2 \
    apache2-utils \
    libapache2-mod-python \
    python-dev \
    python-pip \
    python-cairo \
    python-pysqlite2 \
    python-mysqldb \
    python-jinja2
    sqlite3 \
    curl \ 
    wget \
    git \
    software-properties-common

RUN \
  curl -sSL https://bootstrap.pypa.io/get-pip.py | python &amp;amp;&amp;amp; \
    pip install whisper \
    carbon \
    graphite-web \
    &#39;Twisted&amp;lt;12.0&#39; \
    &#39;django&amp;lt;1.6&#39; \
    django-tagging

# Add start scripts etc
ADD . /app

RUN mkdir -p /app/wsgi
RUN useradd -d /app -c &#39;application&#39; -s &#39;/bin/false&#39; graphite
RUN chmod +x /app/bin/*
RUN chown -R graphite:graphite /app
RUN chown -R graphite:graphite /opt/graphite
RUN rm -f /etc/apache2/sites-enabled/*

ADD ./apache-graphite.conf /etc/apache2/sites-enabled/apache-graphite.conf

# Expose ports.
EXPOSE 80 
EXPOSE 2003 
EXPOSE 2004 
EXPOSE 7002

ENV APACHE_CONFDIR /etc/apache2
ENV APACHE_ENVVARS $APACHE_CONFDIR/envvars
ENV APACHE_RUN_USER www-data
ENV APACHE_RUN_GROUP www-data
ENV APACHE_RUN_DIR /var/run/apache2
ENV APACHE_PID_FILE $APACHE_RUN_DIR/apache2.pid
ENV APACHE_LOCK_DIR /var/lock/apache2
ENV APACHE_LOG_DIR /var/log/apache2

WORKDIR /app

# Define default command.
CMD [&amp;quot;/app/bin/start_graphite&amp;quot;]

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However an optmized version of this same Dockerfile based on what was discussed earlier would look like the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 1 - Common Header / Packages
FROM ubuntu:trusty
MAINTAINER Paul Czarkowski &amp;quot;paul@paulcz.net&amp;quot;

RUN apt-get -yq update \
  &amp;amp;&amp;amp; apt-get -yqq install \
    wget \
    curl \
    git \
    software-properties-common

# 2 - Python
RUN \
  apt-get -yqq install \
    python-dev \
    python-pip \
    python-pysqlite2 \
    python-mysqldb

# 3 - Apache
RUN \
  apt-get -yqq install \
    apache2 \
    apache2-utils

# 4 - Apache ENVs
ENV APACHE_CONFDIR /etc/apache2
ENV APACHE_ENVVARS $APACHE_CONFDIR/envvars
ENV APACHE_RUN_USER www-data
ENV APACHE_RUN_GROUP www-data
ENV APACHE_RUN_DIR /var/run/apache2
ENV APACHE_PID_FILE $APACHE_RUN_DIR/apache2.pid
ENV APACHE_LOCK_DIR /var/lock/apache2
ENV APACHE_LOG_DIR /var/log/apache2

# 5 - Graphite and Deps
RUN \
  apt-get -yqq install \
    libapache2-mod-python \
    python-cairo \
    python-jinja2 \
    sqlite3

RUN \
    pip install whisper \
    carbon \
    graphite-web \
    &#39;Twisted&amp;lt;12.0&#39; \
    &#39;django&amp;lt;1.6&#39; \
    django-tagging

# 6 - Other
EXPOSE 80 2003 2004 7002

WORKDIR /app

VOLUME /opt/graphite/data

# Define default command.
CMD [&amp;quot;/app/bin/start_graphite&amp;quot;]

# 7 - First use of ADD
ADD . /app

# 8 - Final setup
RUN mkdir -p /app/wsgi \
  &amp;amp;&amp;amp; useradd -d /app -c &#39;application&#39; -s &#39;/bin/false&#39; graphite \
  &amp;amp;&amp;amp; chmod +x /app/bin/* \
  &amp;amp;&amp;amp; chown -R graphite:graphite /app \
  &amp;amp;&amp;amp; chown -R graphite:graphite /opt/graphite \
  &amp;amp;&amp;amp; rm -f /etc/apache2/sites-enabled/* \
  &amp;amp;&amp;amp; mv /app/apache-graphite.conf /etc/apache2/sites-enabled/apache-graphite.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;1-common-header-packages:6226be07e9066526e21065089ae006e9&#34;&gt;1 - Common Header / Packages&lt;/h3&gt;

&lt;p&gt;This is our most shareable layer.  All the images running on the same host should start with this.  You can see I&amp;rsquo;ve added a few things like &lt;code&gt;curl&lt;/code&gt; and &lt;code&gt;git&lt;/code&gt; which while they&amp;rsquo;re not necessarily needed they&amp;rsquo;re useful for debugging and because they&amp;rsquo;re in such a shareable layer,  they don&amp;rsquo;t take up much room.&lt;/p&gt;

&lt;h3 id=&#34;2-python-3-apache:6226be07e9066526e21065089ae006e9&#34;&gt;2 - Python, 3 - Apache&lt;/h3&gt;

&lt;p&gt;Here we get to our language specifications.   I&amp;rsquo;ve included the Python and Apache sections here because it&amp;rsquo;s not super clear which should go first.&lt;/p&gt;

&lt;p&gt;If we put python first,  then any other image that uses Apache can get a few free python packages,  If we put Apache first then we could have a Ruby app that also includes that layer and get Apache for free ( hell you can just give it python for free anyways ).&lt;/p&gt;

&lt;h3 id=&#34;4-apache-envs:6226be07e9066526e21065089ae006e9&#34;&gt;4 - Apache Envs&lt;/h3&gt;

&lt;p&gt;I&amp;rsquo;m calling these out seperately for a few reasons.&lt;/p&gt;

&lt;p&gt;Firstly, they should come either directly directly after the Apache section so that it&amp;rsquo;s easier to make them common ( and cached ) between multiple images.   You might not think it matters since calls like &lt;code&gt;ENV&lt;/code&gt; are so cheap, but I have seen random &lt;code&gt;ENV&lt;/code&gt; calls take 10 seconds or so.  If you have a lot, then its good to keep them cached, but you also don&amp;rsquo;t want a changed &lt;code&gt;ENV&lt;/code&gt; to invalidated the cache of installing Apache.&lt;/p&gt;

&lt;p&gt;They&amp;rsquo;re a pretty good example of something you might want to start with at the bottom of your container and move them up higher once you&amp;rsquo;re unlikely to change them again.&lt;/p&gt;

&lt;p&gt;Secondly, to mention that I really wish Docker provided a way to specify multiple ENVS on the same line so that I can reduce the number of layers I end up with.&lt;/p&gt;

&lt;h3 id=&#34;5-graphite-and-deps:6226be07e9066526e21065089ae006e9&#34;&gt;5 - Graphite and Deps&lt;/h3&gt;

&lt;p&gt;This contains some Graphite specific &lt;code&gt;apt&lt;/code&gt; and &lt;code&gt;pip&lt;/code&gt; packages.  You could join them into a single command by joining them with &lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt; but I kept them seperate so that if &lt;code&gt;pip&lt;/code&gt; package requirements change it won&amp;rsquo;t need to also reget the &lt;code&gt;apt&lt;/code&gt; packages.&lt;/p&gt;

&lt;h3 id=&#34;6-other:6226be07e9066526e21065089ae006e9&#34;&gt;6 - Other&lt;/h3&gt;

&lt;p&gt;This contains a bunch of cheap commands like &lt;code&gt;ADD&lt;/code&gt; and &lt;code&gt;VOLUME&lt;/code&gt; they&amp;rsquo;re probably less likely to change than the previous package installs, but are also cheaper to run, so its less important if their cache is invalidated.&lt;/p&gt;

&lt;p&gt;Keep them towards the bottom though as you don&amp;rsquo;t want any changes to them to invalidate the cache for a more costly command.&lt;/p&gt;

&lt;h3 id=&#34;7-first-add:6226be07e9066526e21065089ae006e9&#34;&gt;7 - First ADD&lt;/h3&gt;

&lt;p&gt;You should wait until the last possible moment to use the &lt;code&gt;ADD&lt;/code&gt; command as any commands after it are never cached.&lt;/p&gt;

&lt;h3 id=&#34;8-final-setup:6226be07e9066526e21065089ae006e9&#34;&gt;8 - Final setup&lt;/h3&gt;

&lt;p&gt;I have grouped these final commands into a single layer and they&amp;rsquo;re after the &lt;code&gt;ADD&lt;/code&gt; commands as they manipulate files that come from the &lt;code&gt;ADD&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&#34;fin:6226be07e9066526e21065089ae006e9&#34;&gt;FIN.&lt;/h2&gt;

&lt;p&gt;Hopefully this has given you some insight into how to build a better &lt;code&gt;Dockerfile&lt;/code&gt;.  These are all things I have learned from experience in building my own Docker images and while they may not apply to all situations ( or may be flat out wrong ) they defintely seem to improve my development experience.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Factorish and The Twelve-Fakter App</title>
      <link>http://tech.paulcz.net/2015/01/factorish_and_the_12_fakter_app/</link>
      <pubDate>Tue, 06 Jan 2015 13:29:27 -0600</pubDate>
      
      <guid>http://tech.paulcz.net/2015/01/factorish_and_the_12_fakter_app/</guid>
      <description>

&lt;p&gt;Unless you&amp;rsquo;ve been living under a rock (in which case I envy you) you&amp;rsquo;ve heard a fair bit about The &lt;a href=&#34;http://12factor.net&#34;&gt;Twelve-Factor App&lt;/a&gt;. A wonderful stateless application that is completely disposable and can run anywhere from your own physical servers to &lt;a href=&#34;http://deis.io&#34;&gt;Deis&lt;/a&gt;, &lt;a href=&#34;http://cloudfoundry.org&#34;&gt;Cloud Foundry&lt;/a&gt; or &lt;a href=&#34;http://heroku.com&#34;&gt;Heroku&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Chances are you&amp;rsquo;re stuck writing and running an application that is decidely not 12Factor, nor will it ever be.  In a perfect world you&amp;rsquo;d scrap it and rewrite it as a dozen microservices that are loosely coupled but run and work indepently of eachother. The reality however is you could never get the okay to do that.&lt;/p&gt;

&lt;p&gt;Fortunately with the rise of &lt;a href=&#34;http://docker.com&#34;&gt;Docker&lt;/a&gt; and its ecosystem it has become easier to not only write 12Factor apps, but also to fake it by producing a Docker container that acts like a 12Factor app, but contains something that is decidedly not.  I call this the 12Fakter app.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve been playing with this concept for a while, but over Christmas I spent a bunch of time trying to figure out the best ways to fake out the 12 Factors and feel that I&amp;rsquo;ve come up with something that works pretty well and in the process created a Vagrant based development sandbox called &lt;a href=&#34;http://github.com/paulczar/factorish&#34;&gt;Factorish&lt;/a&gt; which I used to create &lt;a href=&#34;http://github.com/paulczar/12fakter-wordpress&#34;&gt;12fakter-wordpress&lt;/a&gt; and &lt;a href=&#34;https://github.com/paulczar/docker-elk_confd&#34;&gt;elk_confd&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;fakter-i-codebase:c0fb869c851342d199f99065fef4d125&#34;&gt;Fakter I. Codebase&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;One codebase tracked in revision control, many deploys&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The goal here is to have both your app and deployment tooling in the same codebase which is stored in source control.  This means adding a &lt;code&gt;Dockerfile&lt;/code&gt;, and &lt;code&gt;Vagrantfile&lt;/code&gt; and other pieces of tooling into your codebase.  If however you have a monolithic codebase that contains more than just your app you can create a seperate codebase ( use git! ) containing this tooling and have that tooling collect the application from its existing codebase.&lt;/p&gt;

&lt;p&gt;You should be able to achieve this by either merging &lt;a href=&#34;http://github.com/paulczar/factorish&#34;&gt;Factorish&lt;/a&gt; into your existing git repo,  or fork it and use the &lt;code&gt;Dockerfile&lt;/code&gt; in it to pull the actual application code in as part of the build process.&lt;/p&gt;

&lt;h2 id=&#34;fakter-ii-dependencies:c0fb869c851342d199f99065fef4d125&#34;&gt;Fakter II. Dependencies&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Explicitly declare and isolate dependencies&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is a really easy win with Docker,  The very nature of Docker both Explicitly declares your dependencies in the form of the &lt;code&gt;Dockerfile&lt;/code&gt; and Isolates them in the form of the built Docker image.&lt;/p&gt;

&lt;h3 id=&#34;declaration:c0fb869c851342d199f99065fef4d125&#34;&gt;Declaration&lt;/h3&gt;

&lt;h4 id=&#34;app-example-dockerfile:c0fb869c851342d199f99065fef4d125&#34;&gt;/app/example/Dockerfile&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;FROM python:2

# Base deps layer
RUN \
  apt-get update &amp;amp;&amp;amp; apt-get install -yq \
  make \
  ca-certificates \
  net-tools \
  sudo \
  wget \
  vim \
  strace \
  lsof \
  netcat \
  lsb-release \
  locales \
  socat \
  supervisor \
  --no-install-recommends &amp;amp;&amp;amp; \
  locale-gen en_US.UTF-8

# etcdctl and confd layer
RUN \
  curl -sSL -o /usr/local/bin/etcdctl https://s3-us-west-2.amazonaws.com/opdemand/etcdctl-v0.4.6 \
  &amp;amp;&amp;amp; chmod +x /usr/local/bin/etcdctl \
  &amp;amp;&amp;amp; curl -sSL -o /usr/local/bin/confd https://github.com/kelseyhightower/confd/releases/download/v0.7.1/confd-0.7.1-linux-amd64 \
  &amp;amp;&amp;amp; chmod +x /usr/local/bin/confd

ADD . /app
WORKDIR /app

# app layer
RUN \
  useradd -d /app -c &#39;application&#39; -s &#39;/bin/false&#39; app &amp;amp;&amp;amp; \
  chmod +x /app/bin/* &amp;amp;&amp;amp; \
  pip install -r /app/example/requirements.txt

# Define default command.
CMD [&amp;quot;/app/bin/boot&amp;quot;]

# Expose ports.
EXPOSE 8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You might notice I have sets of commands joined together with &lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt; in my &lt;code&gt;Dockerfile&lt;/code&gt;, I do this to control the docker layers more to try and end up with fewer more meaningful layers.&lt;/p&gt;

&lt;h3 id=&#34;isolation:c0fb869c851342d199f99065fef4d125&#34;&gt;Isolation&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ docker build -t factorish/example example
Sending build context to Docker daemon 20.99 kB
Sending build context to Docker daemon
Step 0 : FROM python:2
 ---&amp;gt; 96e13ecb4dba
...
...
Step 8 : EXPOSE 8080
 ---&amp;gt; Running in 8dc9a04eaf78
 ---&amp;gt; 374cb835239c
Removing intermediate container 8dc9a04eaf78
Successfully built 374cb835239c
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;fakter-iii-configuration:c0fb869c851342d199f99065fef4d125&#34;&gt;Fakter III. Configuration&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Store config in the environment&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Another easy win with Docker.   You can pass in environment variables in the &lt;code&gt;Dockerfile&lt;/code&gt; as we as when running the docker container using the &lt;code&gt;-e&lt;/code&gt; option like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d -e TEXT=bacon factorish/example
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However chances are your app reads from a config file rather than environment variables. There are [at least] two fairly simple ways to achieve this.&lt;/p&gt;

&lt;h3 id=&#34;sed-inline-replacement:c0fb869c851342d199f99065fef4d125&#34;&gt;sed inline replacement&lt;/h3&gt;

&lt;p&gt;use a startup script to edit your config file and replace values in it with the values of the environment variables using &lt;code&gt;sed&lt;/code&gt; before runnin your app:&lt;/p&gt;

&lt;h4 id=&#34;app-bin-boot:c0fb869c851342d199f99065fef4d125&#34;&gt;/app/bin/boot&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
sed -i &amp;quot;s/xxxTEXTxxx/${TEXT}&amp;quot; /app/example/example.conf
python /app/example/app.py
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;confd-templating:c0fb869c851342d199f99065fef4d125&#34;&gt;confd templating&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kelseyhightower/confd&#34;&gt;confd&lt;/a&gt; is a tool written specifically for templating config files from data sources such as environment variables.  This is a much better option as it also opens up the ability to use service discovery tooling like &lt;a href=&#34;https://coreos.com/using-coreos/etcd/&#34;&gt;etcd&lt;/a&gt; (also supported in Factorish) rather than environment variables.&lt;/p&gt;

&lt;h4 id=&#34;app-conf-d-example-conf-toml:c0fb869c851342d199f99065fef4d125&#34;&gt;/app/conf.d/example.conf.toml&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;[template]
src   = &amp;quot;example.conf&amp;quot;
dest  = &amp;quot;/app/example/example.conf&amp;quot;
keys = [&amp;quot;/services/example&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;app-templates-example-conf:c0fb869c851342d199f99065fef4d125&#34;&gt;/app/templates/example.conf&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;[example]
text: {{ getv &amp;quot;/services/example/text&amp;quot; }}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;The &lt;code&gt;{{ }}&lt;/code&gt; syntax above is the golang/confd macros used to perform tasks like fetching variables from etcd or environment.&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&#34;app-bin-boot-1:c0fb869c851342d199f99065fef4d125&#34;&gt;/app/bin/boot&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
confd -onetime
python /app/example/app.py
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;fakter-iv-backing-services:c0fb869c851342d199f99065fef4d125&#34;&gt;Fakter IV. Backing Services&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Treat backing services as attached resources&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Anything that is needed to store persistent data should be treated as an external dependency to your application.  As far as your app is concerned there should be no difference between a local MySQL server or Amazon&amp;rsquo;s RDS.&lt;/p&gt;

&lt;p&gt;This is easier for some backing services than others.  For example if your app requires a MySQL database its relatively straight forward.  Whereas a local filesystem for storing images is harder, but can be solved:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Docker: volume mounts, data containers&lt;/li&gt;
&lt;li&gt;Remote Storage: netapp, nfs, fuse-s3fs&lt;/li&gt;
&lt;li&gt;Clustered FS: drdb, gluster&lt;/li&gt;
&lt;li&gt;Ghetto: rsync + concerned&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The docker volume mounts actually work really well in a vagrant based development environment because you can pass your code all the way into the container from your workstation,  however there are definitely some security considerations to think about if you want to do volume mounts in production.&lt;/p&gt;

&lt;h3 id=&#34;example:c0fb869c851342d199f99065fef4d125&#34;&gt;Example&lt;/h3&gt;

&lt;p&gt;A fictional &lt;strong&gt;PHP&lt;/strong&gt; based blog about bacon requires a database and a filestore:&lt;/p&gt;

&lt;h4 id=&#34;app-templates-config-php:c0fb869c851342d199f99065fef4d125&#34;&gt;/app/templates/config.php&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;define(&#39;DB_NAME&#39;, &#39;{{ getv &amp;quot;/db/name&amp;quot; }}&#39;);
define(&#39;DB_USER&#39;, &#39;{{ getv &amp;quot;/db/user&amp;quot; }}&#39;);
define(&#39;DB_PASSWORD&#39;, &#39;{{ getv &amp;quot;/db/pass&amp;quot; }}&#39;);
define(&#39;DB_HOST&#39;, &#39;{{ getv &amp;quot;/db/host&amp;quot; }}&#39;);
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;docker-run-command:c0fb869c851342d199f99065fef4d125&#34;&gt;Docker Run command&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d -e DB_NAME=bacon -e DB_USER=bacon \
  -e DB_PASSWORD=bacon $DB_HOST=my.database.com \
  -v /mnt/nfs/bacon:/app/bacon factorish/bacon-blog
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;confd will use the environment variables passed in via the &lt;code&gt;docker run&lt;/code&gt; command to fill out the variables called in the &lt;code&gt;{{ }}&lt;/code&gt; macros.  Note that confd transforms the environment variables so that the environment variable &lt;code&gt;DB_USER&lt;/code&gt; will be read by &lt;code&gt;{{ getv &amp;quot;/db/user&amp;quot; }}&lt;/code&gt;.  This is done to normalize the macro across the various data source options.&lt;/p&gt;

&lt;h2 id=&#34;fakter-v-build-release-run:c0fb869c851342d199f99065fef4d125&#34;&gt;Fakter V. Build, Release, Run&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Strictly separate build and run stages&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;build:c0fb869c851342d199f99065fef4d125&#34;&gt;Build&lt;/h3&gt;

&lt;p&gt;Converts a code repo into an executable bundle. Sound familiar?  Yup, we&amp;rsquo;ve already solved this with our &lt;code&gt;Dockerfile&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;release:c0fb869c851342d199f99065fef4d125&#34;&gt;Release&lt;/h3&gt;

&lt;p&gt;Takes the build and combines it with the current configuration. In a purely docker based system this can be split between the &lt;strong&gt;Build&lt;/strong&gt; (versioning and defaults) and &lt;strong&gt;Run&lt;/strong&gt; (current config) stages. However systems like Heroku and Deis have a seperate step for this which they handle internally.&lt;/p&gt;

&lt;h3 id=&#34;run:c0fb869c851342d199f99065fef4d125&#34;&gt;Run&lt;/h3&gt;

&lt;p&gt;Runs the application by launching a set of the app&amp;rsquo;s processes against a selected release.  In a docker based system this is simply the &lt;code&gt;$ docker run&lt;/code&gt; command which can be called via a deploy script, or a init script (systemd/runit) or a scheduler like &lt;a href=&#34;https://coreos.com/using-coreos/clustering/&#34;&gt;fleet&lt;/a&gt; or &lt;a href=&#34;http://mesos.apache.org/&#34;&gt;mesos&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;fakter-vi-processes:c0fb869c851342d199f99065fef4d125&#34;&gt;Fakter VI. Processes&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Execute the app as one or more stateless processes&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Your application inside the docker container should behave like a standard linux process running in the foreground and be stateless and share-nothing.  Being inside a docker container means that this is hidden and therefore we can fairly easily fake this but you do need to think about process management and logging which are discussed later and is further explored &lt;a href=&#34;http://tech.paulcz.net/2014/12/multi-process-docker-images-done-right/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;fakter-vii-port-binding:c0fb869c851342d199f99065fef4d125&#34;&gt;Fakter VII. Port binding&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Export services via port binding&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Your application should appear to be completely self contained and not require runtime injection of a webserver.  Thankfully this is pretty easy to fake in a docker container as any extra processes are isolated in the container and effectively invisible to the outside.&lt;/p&gt;

&lt;p&gt;It is still preferable to use a native language based web library such as jetty (java) or flask (python) but for languages like PHP using apache or nginx is ok.&lt;/p&gt;

&lt;p&gt;Docker itself takes care of the port binding by use of the &lt;code&gt;-p&lt;/code&gt; option on the command line.  It&amp;rsquo;s useful to register the port and host IP to somewhere ( etcd ) to allow for loadbalancers and other services to easily locate your application.&lt;/p&gt;

&lt;h2 id=&#34;fakter-viii-concurrency:c0fb869c851342d199f99065fef4d125&#34;&gt;Fakter VIII. Concurrency&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Scale out via the process model&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We should be able to scale up or down simply by creating or destroying docker containers containing the application.  Any upstream load balancers as an external dependency would need to be notified of the container starting ( usually a fairly easy API call) and stopping.  But these are external dependencies and should be solved outside of your application itself.&lt;/p&gt;

&lt;p&gt;Inside the container your application should not daemonize or write pid files (if unavoidable, not too difficult to script around) and use tooling like &lt;code&gt;upstart&lt;/code&gt; or &lt;code&gt;supervisord&lt;/code&gt; if there is more than one process that needs to be run.&lt;/p&gt;

&lt;h2 id=&#34;fakter-ix-disposability:c0fb869c851342d199f99065fef4d125&#34;&gt;Fakter IX. Disposability&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Maximize robustness with fast startup and graceful shutdown&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Docker helps a lot with this.   We want to ensure that we&amp;rsquo;re optimized for fast yet reliable startup as well as graceful shutdown.  Your app should be able to be shut down gracefully when &lt;code&gt;docker kill&lt;/code&gt; is called and just as importantly there should be minimal if any external effect if the application crashes or stops ungracefully.&lt;/p&gt;

&lt;p&gt;The container itself should kill itself if the app inside it stops working right.  If your app is running behind a &lt;a href=&#34;http://tech.paulcz.net/2014/12/multi-process-docker-images-done-right/&#34;&gt;supervisor&lt;/a&gt; this can be a achieved with a really lightweight healthcheck script like this.&lt;/p&gt;

&lt;h4 id=&#34;app-bin-healhthcheck:c0fb869c851342d199f99065fef4d125&#34;&gt;/app/bin/healhthcheck&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
while [[ ! -z $(netstat -lnt | awk &amp;quot;\$6 == \&amp;quot;LISTEN\&amp;quot; &amp;amp;&amp;amp; \$4 ~ \&amp;quot;.$PORT\&amp;quot; &amp;amp;&amp;amp; \$1 ~ \&amp;quot;tcp.?\&amp;quot;&amp;quot;) ]] ; do
  [[ -n $ETCD_HOST ]] &amp;amp;&amp;amp; etcdctl set /service/web/hosts/$HOST $PORT --ttl 10 &amp;gt;/dev/null
  sleep 5
done
kill `cat /var/run/supervisord.pid`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You&amp;rsquo;ll note that I&amp;rsquo;m also publishing host and port values to etcd if &lt;code&gt;$ETCD_HOST&lt;/code&gt; is set.  This can then be used to notify loadbalancers and the like when services start or stop.&lt;/p&gt;

&lt;h2 id=&#34;fakter-x-dev-prod-parity:c0fb869c851342d199f99065fef4d125&#34;&gt;Fakter X. Dev/prod parity&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Keep development, staging, and production as similar as possible&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;By following the previous fackters we&amp;rsquo;ve done most of the work to make this possible.  We use Vagrant in development to deploy your app (and any backing services) using the appropriate provisioning methodology ( the same ones we&amp;rsquo;d use for production).&lt;/p&gt;

&lt;p&gt;By wrapping the application in a docker container it is portable across just about any system that is capable of running docker.&lt;/p&gt;

&lt;p&gt;By provisioning with the same tooling to both dev and prod (and any other envs),  any deployment of development (should happen frequently) is also a test of most of the tooling used to deploy to production.&lt;/p&gt;

&lt;h2 id=&#34;fakter-xi-logs:c0fb869c851342d199f99065fef4d125&#34;&gt;Fakter XI. Logs&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Treat logs as event streams&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Your application ( even inside the container ) should always log to stdout. By writing to stdout of your process we can utilize the docker logging subsystem which when combined with tooling like &lt;a href=&#34;https://registry.hub.docker.com/u/progrium/logspout/&#34;&gt;logspout&lt;/a&gt; makes it very easy to push all logs to a central system.&lt;/p&gt;

&lt;p&gt;If your app &lt;em&gt;has&lt;/em&gt; to write to a logfile you should be able to configure that log file to be &lt;code&gt;/dev/stdout&lt;/code&gt; which should cause it to write to stdout of the process. If your app only writes to syslog then configure it to write to a remote syslog. Basically do whatever you can to ensure you don&amp;rsquo;t log to the local filesystem.&lt;/p&gt;

&lt;h3 id=&#34;example-1:c0fb869c851342d199f99065fef4d125&#34;&gt;Example&lt;/h3&gt;

&lt;p&gt;This example shows running &lt;code&gt;Supervisord&lt;/code&gt; as your primary process in the docker container and &lt;code&gt;nginx&lt;/code&gt; writing logs to stdout which in turn are written to the containers &lt;code&gt;stdout&lt;/code&gt;.  A more thorough writeup on using &lt;a href=&#34;http://tech.paulcz.net/2014/12/multi-process-docker-images-done-right/&#34;&gt;supervisor&lt;/a&gt; inside docker containers can be found &lt;a href=&#34;http://tech.paulcz.net/2014/12/multi-process-docker-images-done-right/&#34;&gt;here&lt;/a&gt;:&lt;/p&gt;

&lt;h4 id=&#34;etc-supervisor-conf-d-nginx:c0fb869c851342d199f99065fef4d125&#34;&gt;/etc/supervisor/conf.d/nginx&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;[supervisord]
logfile=/dev/null
pidfile=/var/run/supervisord.pid
nodaemon=true

[program:nginx]
command=/usr/sbin/nginx
redirect_stderr=true
stdout_logfile=/dev/stdout
stdout_logfile_maxbytes=0
auto_start=true
autorestart=true
user=root
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;etc-nginx-sites-enabled-app:c0fb869c851342d199f99065fef4d125&#34;&gt;/etc/nginx/sites-enabled/app&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;worker_processes 1;
daemon off;
error_log /dev/stdout;
http {
  access_log /dev/stdout;
  server {
    listen            *:8080;
    root              /app/bacon-blog;
    index             index.php;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For a more detailed post on using logspout to produce consumable logs check out &lt;a href=&#34;https://twitter.com/behemphi&#34;&gt;@behemphi&lt;/a&gt;&amp;rsquo;s blog post - &lt;a href=&#34;http://stackengine.com/docker-logs-aggregating-ease/&#34;&gt;Docker Logs – Aggregating with Ease&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;fakter-xii-admin-processes:c0fb869c851342d199f99065fef4d125&#34;&gt;Fakter XII. Admin processes&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Run admin/management tasks as one-off processes&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This one is pretty easy.  Tasks such as database migrates should be run in one off throw-away containers.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -t -e DB_SERVER=user@pass:db.server.com myapp:1.3.2 rake db:migrate
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusion:c0fb869c851342d199f99065fef4d125&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Most of the fakters above are relatively straight forward to utilize and can be built upon slowly, no need to perfect things before working on them.  They can also be utilized with any existing provisioning / config management tooling that you already have.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re already using &lt;a href=&#34;http://chef.io&#34;&gt;chef&lt;/a&gt; for deploying your application you can use the &lt;a href=&#34;https://supermarket.chef.io/cookbooks/docker&#34;&gt;docker cookbook&lt;/a&gt; to start running docker containers instead and write out confd templates rather than the final config file which confd will then use to do the final configuration of your app from the environment variables you pass through to the &lt;code&gt;docker_run&lt;/code&gt; resource in the cookbook.&lt;/p&gt;

&lt;p&gt;Making your application act like a 12Factor app may not be enough to run it on a purely hosted PAAS like Heroku, but chances are you&amp;rsquo;ll be able to run it on a Docker based PAAS like Deis.  You can go full stack with Mesos or CoreOS+Fleet+ETCD or you can stick to Ubuntu servers running docker.&lt;/p&gt;

&lt;p&gt;The flexibility that the 12fakter application gives you means that you can move to a more modern infrastructure at your own pace when it makes sense without having to abandon or completely rewrite your existing applications.&lt;/p&gt;

&lt;p&gt;Please check out &lt;a href=&#34;http://github.com/paulczar/factorish&#34;&gt;Factorish&lt;/a&gt; and some of the example 12fakter apps like &lt;a href=&#34;http://github.com/paulczar/12fakter-wordpress&#34;&gt;12fakter-wordpress&lt;/a&gt; and &lt;a href=&#34;https://github.com/paulczar/docker-elk_confd&#34;&gt;elk_confd&lt;/a&gt;. to see how easy it can be to start making your applications act like 12Factor apps.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multi Process Docker Images Done Right</title>
      <link>http://tech.paulcz.net/2014/12/multi-process-docker-images-done-right/</link>
      <pubDate>Mon, 22 Dec 2014 21:31:03 -0600</pubDate>
      
      <guid>http://tech.paulcz.net/2014/12/multi-process-docker-images-done-right/</guid>
      <description>

&lt;h2 id=&#34;for-some-values-of-right:1c79ecc8ccdca915b8029a73add2dd3b&#34;&gt;For some values of &amp;lsquo;right&amp;rsquo;&lt;/h2&gt;

&lt;p&gt;Almost since &lt;a href=&#34;http://docker.com&#34;&gt;Docker&lt;/a&gt; was first introduced to the world there has been a fairly strong push to keeping containers to be single process.   This makes a lot of sense and definitely plays into the &lt;a href=&#34;http://12factor.net&#34;&gt;12 Factor&lt;/a&gt; way of thinking where all application output should be pushed to &lt;code&gt;stdout&lt;/code&gt; and docker itself with tools like &lt;a href=&#34;https://github.com/progrium/logspout&#34;&gt;logspout&lt;/a&gt; now has fairly strong tooling to deal with those logs.&lt;/p&gt;

&lt;p&gt;Sometimes however it just makes sense to run more than one process in a container,  a perfect example would be running &lt;a href=&#34;https://github.com/kelseyhightower/confd&#34;&gt;confd&lt;/a&gt; as well as your application in order to modify the application&amp;rsquo;s config file based on changes in service discovery systems like &lt;a href=&#34;https://github.com/coreos/etcd&#34;&gt;etcd&lt;/a&gt;.   The &lt;a href=&#34;https://docs.docker.com/articles/ambassador_pattern_linking/&#34;&gt;ambassador&lt;/a&gt; container way of working can achieve similar things, but I&amp;rsquo;m not sure that running two containers with a process each to run your application is any better than running one container with two processes.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re going run multiple processes you have a few options to do it.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Start the container with the first process adnd then use the new &lt;code&gt;docker exec&lt;/code&gt; command to start the second.&lt;/li&gt;
&lt;li&gt;Start them in sequence in a &lt;code&gt;bash&lt;/code&gt; script and background all but the last process with a &lt;code&gt;&amp;amp;&lt;/code&gt; at the end of the line.&lt;/li&gt;
&lt;li&gt;Use a Process Supervisor such as Supervisord or Runit.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I haven&amp;rsquo;t really messed around with the first option, maybe it could work out, but you&amp;rsquo;d lose the logs from the second process as it would need to output via the first process&amp;rsquo; &lt;code&gt;stdout&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;the-bash-script:1c79ecc8ccdca915b8029a73add2dd3b&#34;&gt;The Bash Script&lt;/h2&gt;

&lt;p&gt;Up until recently the way I have been running multiple processes is via the &lt;code&gt;bash&lt;/code&gt; script method, but it feels really clumsy and fragile and while it works I&amp;rsquo;ve never been particularly fond of it.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s an snippet from such a script from my &lt;a href=&#34;https://github.com/paulczar/docker-elk_confd&#34;&gt;docker-elk_confd&lt;/a&gt; project which builds out the [ELK]() stack using values in &lt;code&gt;etcd&lt;/code&gt; to orchestrate clustering and configuration via &lt;code&gt;confd&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo Starting ${APP_NAME}

confd -node $ETCD -config-file /app/confd.toml -confdir /app &amp;amp;
/opt/elasticsearch/bin/elasticsearch -p /app/elasticsearch.pid &amp;amp;

# while the port is listening, publish to etcd
while [[ ! -z $(netstat -lnt | awk &amp;quot;\$6 == \&amp;quot;LISTEN\&amp;quot; &amp;amp;&amp;amp; \$4 ~ \&amp;quot;.$PUBLISH\&amp;quot; &amp;amp;&amp;amp; \$1 ~ \&amp;quot;$PROTO.?\&amp;quot;&amp;quot;) ]] ; do
  publish_to_etcd
  sleep 5 # sleep for half the TTL
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see I&amp;rsquo;ve started two processes &lt;code&gt;elasticsearch&lt;/code&gt; and &lt;code&gt;confd&lt;/code&gt; both backgrounded and then I finish with a loop which publishes data to etcd every 5 seconds until the &lt;code&gt;elasticsearch&lt;/code&gt; process quits listening on its published tcp port.  This works, but it leaves me feeling a bit icky.&lt;/p&gt;

&lt;h2 id=&#34;process-supervisor:1c79ecc8ccdca915b8029a73add2dd3b&#34;&gt;Process Supervisor&lt;/h2&gt;

&lt;p&gt;I have used various supervisors in containers before but never really liked the experience as I could never get all the logs out to &lt;code&gt;stdout&lt;/code&gt; and using the standard docker logging mechanisms so I&amp;rsquo;ve always gone back to the &lt;code&gt;bash&lt;/code&gt; script method.  Recently while working on the ELK project mentioned above I decided to give using a process supervisor another chance.&lt;/p&gt;

&lt;p&gt;My primary measure of success for using a supervisor going forward was to come up with a way to push all output to the supervisor&amp;rsquo;s stdout so that I can use the regular docker logging.&lt;/p&gt;

&lt;p&gt;I decided to try with &lt;a href=&#34;http://supervisord.org&#34;&gt;supervisor&lt;/a&gt; as a starting point because it is a fairly small install and has an easily templatable config.   At about the same time I was looking at this I found a &lt;a href=&#34;http://supervisord.org&#34;&gt;blog post&lt;/a&gt; ( I believe it was linked in a recent Docker Weekly ) that talked about using &lt;code&gt;supervisor&lt;/code&gt; in docker containers.  They had even (sortof) solved the logging problem,  however the logging was appended with debug lines and made it messy and difficult to read.  I figured there had to be a cleaner way.&lt;/p&gt;

&lt;p&gt;Reading through the documentation I saw that you can specify a file to log each supervised process to.   I just needed a way to hijack that config item to write to supervisor&amp;rsquo;s stdout instead.   Turns out that&amp;rsquo;s quite easy as there&amp;rsquo;s a special device &lt;code&gt;/dev/stdout&lt;/code&gt; which links to &lt;code&gt;/dev/self/fd/1&lt;/code&gt; which is the &lt;code&gt;stdout&lt;/code&gt; for the running application.   I quickly threw together a test and it did indeed pipe the logs from the process through &lt;code&gt;stdout&lt;/code&gt; of supervisor.&lt;/p&gt;

&lt;p&gt;I end up with a &lt;code&gt;/etc/supervisord.conf&lt;/code&gt; ( which is written out by confd before supervisor is started ) file that looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[supervisord]
logfile=/dev/null
pidfile=/var/run/supervisord.pid
nodaemon=true

[program:publish_etcd]
command=/app/bin/publish_etcd
redirect_stderr=true
stdout_logfile=/dev/stdout
stdout_logfile_maxbytes=0
auto_start=true
autorestart=true

[program:confd]
command=confd -node %(ENV_ETCD)s -config-file /app/confd.toml -confdir /app
redirect_stderr=true
stdout_logfile=/dev/stdout
stdout_logfile_maxbytes=0
auto_start=true
autorestart=true

[program:elasticsearch]
command=/opt/elasticsearch/bin/elasticsearch
redirect_stderr=true
stdout_logfile=/dev/stdout
stdout_logfile_maxbytes=0
auto_start=true
autorestart=true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and my boot script that docker runs the following to launch my app:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo Starting ${APP_NAME}
/usr/bin/supervisord -c /etc/supervisor/supervisord.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All output from &lt;code&gt;Elasticsearch&lt;/code&gt;, &lt;code&gt;confd&lt;/code&gt;, &lt;code&gt;supervisord&lt;/code&gt; now output via the docker logging systems so that I can see what is going on by running:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker logs elasticsearch
docker logs -f 7270755ce94c03dda930fbdedeee7722dddf6fdbbf8902aaee52c9f94f2147ca
2014-12-23T04:46:02Z 7270755ce94c confd[37]: INFO /opt/elasticsearch/config/elasticsearch.yml has md5sum 08a09998560b7b786eca1e594b004ddc should be d83b49b485b5acad2666aa03b1ee90a0
2014-12-23T04:46:02Z 7270755ce94c confd[37]: INFO Target config /opt/elasticsearch/config/elasticsearch.yml out of sync
2014-12-23T04:46:02Z 7270755ce94c confd[37]: INFO Target config /opt/elasticsearch/config/elasticsearch.yml has been updated
2014-12-23T04:46:02Z 7270755ce94c confd[37]: INFO /etc/supervisor/supervisord.conf has mode -rw-r--r-- should be -rwxr-xr-x
2014-12-23T04:46:02Z 7270755ce94c confd[37]: INFO /etc/supervisor/supervisord.conf has md5sum 99dc7e8a1178ede9ae9794aaecbca436 should be ad9bc3735991d133a09f4fc665e2305f
2014-12-23T04:46:02Z 7270755ce94c confd[37]: INFO Target config /etc/supervisor/supervisord.conf out of sync
2014-12-23T04:46:02Z 7270755ce94c confd[37]: INFO Target config /etc/supervisor/supervisord.conf has been updated
Starting elasticsearch
2014-12-23 04:46:02,245 CRIT Supervisor running as root (no user in config file)
2014-12-23 04:46:02,251 INFO supervisord started with pid 51
2014-12-23 04:46:03,255 INFO spawned: &#39;publish_etcd&#39; with pid 54
2014-12-23 04:46:03,258 INFO spawned: &#39;elasticsearch&#39; with pid 55
2014-12-23 04:46:03,260 INFO spawned: &#39;confd&#39; with pid 56
==&amp;gt; sleeping for 20 seconds, then testing if elasticsearch is up.
[2014-12-23 04:46:04,146][INFO ][node                     ] [Sultan] version[1.4.2], pid[55], build[927caff/2014-12-16T14:11:12Z]
[2014-12-23 04:46:04,149][INFO ][node                     ] [Sultan] initializing ...
[2014-12-23 04:46:04,156][INFO ][plugins                  ] [Sultan] loaded [], sites []
2014-12-23 04:46:05,158 INFO success: publish_etcd entered RUNNING state, process has stayed up for &amp;gt; than 1 seconds (startsecs)
2014-12-23 04:46:05,159 INFO success: elasticsearch entered RUNNING state, process has stayed up for &amp;gt; than 1 seconds (startsecs)
2014-12-23 04:46:05,161 INFO success: confd entered RUNNING state, process has stayed up for &amp;gt; than 1 seconds (startsecs)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One last thing that I should mention.  the &lt;code&gt;publish_etcd&lt;/code&gt; talk in the supervisor config is running a script that contains the &lt;code&gt;while&lt;/code&gt; loop to make sure that &lt;code&gt;elasticsearch&lt;/code&gt; is listening on the approriate port, If that loop is broken it means that&lt;code&gt;elasticsearch&lt;/code&gt; is not responding and it sends a kill signal to &lt;code&gt;supervisor&lt;/code&gt; which then causes the container to shoot itself in the head because the rest of the  processes running are useless without &lt;code&gt;elasticsearch&lt;/code&gt; running.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>BreadOps - Continuous Delivery of Fresh Baked Bread</title>
      <link>http://tech.paulcz.net/2014/12/breadops-continous-delivery-of-fresh-baked-bread/</link>
      <pubDate>Wed, 03 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>http://tech.paulcz.net/2014/12/breadops-continous-delivery-of-fresh-baked-bread/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;https://lh3.googleusercontent.com/-rMMuR_Itcmk/VH9eSsVZ18I/AAAAAAAAOc8/bJBp9UaoMI0/s1024/20141203_124907.jpg&#34; alt=&#34;the best way to eat fresh bread&#34; /&gt;

&amp;ldquo;See how this sparkly devop princess bakes bread every day with almost no effort at all with this one weird trick&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Store bought bread is shit.  Even the  &amp;ldquo;artisanal&amp;rdquo; bread at most supermarkets is little better than cake baked in a bread shaped mold ( seriously check next time you&amp;rsquo;re at a supermarket ).  You might be lucky and have a really good bread baker near you,  but like butchers and other important crafts they have all but disappeared.  My solution to this was to start baking bread myself.  I did a ton of research, started my own sourdough starter ( 5 years and going strong! ) and started baking bread regularly.&lt;/p&gt;

&lt;p&gt;Reading Boyd&amp;rsquo;s excellent blog post on &lt;a href=&#34;http://stackengine.com/laundryops-practical-devops-at-home/&#34;&gt;LaundryOps&lt;/a&gt; made me realize that I should write this up as I had somewhat unwittingly applied these DevOps practices to baking bread.&lt;/p&gt;

&lt;!--more --&gt;

&lt;p&gt;For a while it was tough going making bread all the time,  starters and doughs are fickle beasts and required constant care and feeding ( literally, you have to feed a sourdough starter at least twice a day ).   After almost giving up several times I started to apply what I now know as &lt;em&gt;devops techniques&lt;/em&gt;. Over time I worked to constantly improve my processes specifically optimizing for my time.  I can even apply CAMS across it:&lt;/p&gt;

&lt;h2 id=&#34;culture:5777482caf0981cefc6ea3a7ef676968&#34;&gt;Culture&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Lactobacillus &amp;hellip; HAHAHAHA Bakers joke!&lt;/li&gt;
&lt;li&gt;A long fermentation time which converts more of the sugars into gas and makes the proteins more digestible&amp;hellip; Wait still wrong culture.&lt;/li&gt;
&lt;li&gt;Minimal human interaction ( approx 5 minutes per loaf ) reduces impact on normal life.&lt;/li&gt;
&lt;li&gt;Healthy Fresh bread for you and your family&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;automation:5777482caf0981cefc6ea3a7ef676968&#34;&gt;Automation&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Slow ferment reduces requirement to knead to almost zero.&lt;/li&gt;
&lt;li&gt;Refrigerating the dough allows me to take control of the timetable.&lt;/li&gt;
&lt;li&gt;Usable for 5-7 days from the fridge.  &lt;strong&gt;Multiple loaves from the one batch.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;measurement:5777482caf0981cefc6ea3a7ef676968&#34;&gt;Measurement&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;How much active time did I spend on it ?&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;How long did it take to ferment?&lt;/li&gt;
&lt;li&gt;How is the crust?  How is the crumb ?&lt;/li&gt;
&lt;li&gt;Is it better or worse than the previous iteration ?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;sharing:5777482caf0981cefc6ea3a7ef676968&#34;&gt;Sharing&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;A &lt;em&gt;lot&lt;/em&gt; of people have received loaves of bread from me during experimentation.&lt;/li&gt;
&lt;li&gt;Ensure this process is approachable by others.&lt;/li&gt;
&lt;li&gt;This blog post.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The final technique that I came up with is not exactly unique and is similar to a number of several hundred page thirty dollar books aimed at making baking bread more accessible to the home cook, where it differs is that this is just a short blog post and is free as in beer and speech.  It&amp;rsquo;s fairly descriptive and may seem like a lot of work, but I tend to average a little over 5 minutes of active time per loaf of bread which is a negligible amount of time.&lt;/p&gt;

&lt;h2 id=&#34;ingredients-and-tools:5777482caf0981cefc6ea3a7ef676968&#34;&gt;Ingredients and Tools&lt;/h2&gt;

&lt;p&gt;I worked hard to ensure that you don&amp;rsquo;t need to use any specialized tools.   No need for a standmixer or other expensive tools.  There are only two things (listed first) that you might not have in your kitchen, and they&amp;rsquo;re both so versatile that you &lt;em&gt;should&lt;/em&gt; have them.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh4.googleusercontent.com/v_kKOKFKupOdtI2_44MImjl5L8GYzq7QYMs1BxRyGgc=w1229-h692-no&#34; alt=&#34;things what you need to make bread&#34; /&gt;
&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Kitchen Scale[1]

&lt;ul&gt;
&lt;li&gt;capable of measuring to the gram&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Baking Stone[2]

&lt;ul&gt;
&lt;li&gt;large square one, the biggest that will fit in your oven&lt;/li&gt;
&lt;li&gt;A cast iron pan or dutch oven will do in a pinch.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;One large and one small mixing bowl&lt;/li&gt;
&lt;li&gt;Plastic Wrap

&lt;ul&gt;
&lt;li&gt;always cover your dough.&lt;/li&gt;
&lt;li&gt;a showercap makes a great reusable cover.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Flour

&lt;ul&gt;
&lt;li&gt;almost any flour will work, even All Purpose&lt;/li&gt;
&lt;li&gt;I recomend you start with King Arthur Bread Flour.&lt;/li&gt;
&lt;li&gt;I usually do a mix of KABF and stone ground whole wheat.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Salt

&lt;ul&gt;
&lt;li&gt;any salt as long as its not iodized.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Water

&lt;ul&gt;
&lt;li&gt;tap water is fine.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Yeast ( if using )

&lt;ul&gt;
&lt;li&gt;I used Fleischmansn&amp;rsquo;s Active Dry.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Olive Oil

&lt;ul&gt;
&lt;li&gt;For oiling the bowls used in the final shaping.&lt;/li&gt;
&lt;li&gt;I usually line the bowl with floured cheesecloth instead.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;[1] Measuring by volume is a mugs game.  Flour and Salt across different brands have different sized grains and this are different weights for the same volume.&lt;/p&gt;

&lt;p&gt;[2] you could get away with using a baking tray, but a baking stone or a cast iron skillet will give you the best results.&lt;/p&gt;

&lt;h2 id=&#34;the-starter:5777482caf0981cefc6ea3a7ef676968&#34;&gt;The Starter&lt;/h2&gt;

&lt;p&gt;I use a &lt;a href=&#34;http://www.sourdoughhome.com/index.php?content=startermyway2&#34;&gt;sourdough starter&lt;/a&gt;, and I would recommend that you do the same &amp;hellip; But unless you already have a starter or have a friend who will give you some ( I&amp;rsquo;d be happy to give you some of mine ) You&amp;rsquo;ll probably want to use regular yeast which is what I&amp;rsquo;ll describe below.  If you have a sourdough starter then skip this step.&lt;/p&gt;

&lt;p&gt;Mix 50g Flour, 50g Water, 3g yeast until it forms a paste and then cover with plastic wrap:
&lt;img src=&#34;https://lh3.googleusercontent.com/-IKFdhJcTr7c/VHtUZeGewaI/AAAAAAAAObE/NQpFh4jQiCs/w1228-h691-no/20141124_123134.jpg&#34; alt=&#34;Starter&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;After 4-6 hours it should be all bubby and smell yeasty and ready for The Mix:
&lt;img src=&#34;https://lh6.googleusercontent.com/-_ap54clqH38/VH9em5xPMRI/AAAAAAAAOdk/syJCBF_2AJA/s640/20141201_085112.jpg&#34; alt=&#34;fermented starter&#34; /&gt;
&lt;/p&gt;

&lt;h2 id=&#34;the-mix:5777482caf0981cefc6ea3a7ef676968&#34;&gt;The Mix&lt;/h2&gt;

&lt;p&gt;Add the Starter and 375g of water to the large bowl and mix with a fork until it looks like milk:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Metric protip: a gram of water is the same as a milliliter of water&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh4.googleusercontent.com/-0Se5tsoJoH4/VHtUWeoTVdI/AAAAAAAAOa0/57hyMA0sBPk/w1228-h691-no/20141124_140942.jpg&#34; alt=&#34;don&#39;t drink it silly!&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;Add 500g of flour and 15g salt.  Mix it until fully incorporated ( we&amp;rsquo;re not kneading here, just making sure there is no dry flour left ):
&lt;img src=&#34;https://lh5.googleusercontent.com/-FGpOS40YpWM/VH9elHUFGCI/AAAAAAAAOdc/tKkrmoXplWw/s640/20141201_085637.jpg&#34; alt=&#34;mixed&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;em&gt;These quantities form a ratio that can be used to make batches as big or as small as your want.  I usually double it which gives me a decent sized loaf every two days.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;the-warm-ferment:5777482caf0981cefc6ea3a7ef676968&#34;&gt;The Warm Ferment&lt;/h2&gt;

&lt;p&gt;Cover the bowl and leave it at room temperature for 4 to 8 hours.  The timing doesn&amp;rsquo;t have to be precise,  you&amp;rsquo;re just looking for sings of the yeast to start fermenting the flour and creating air bubbles.  Cover and refrigerate for at least 24 hours, up to 5 days:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh3.googleusercontent.com/-DRtoK892ZEI/VH-U6UZeLFI/AAAAAAAAOd8/pmbYgIUPmJU/s640/20141105_101952.jpg&#34; alt=&#34;fermentation activated&#34; /&gt;
&lt;/p&gt;

&lt;h2 id=&#34;the-shaping:5777482caf0981cefc6ea3a7ef676968&#34;&gt;The Shaping&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Whenever you are working with the dough, it is important to try to lose as little air as possible.  Do not punch it down.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh6.googleusercontent.com/-bXb_SAwYxSY/VHtUTmXk2SI/AAAAAAAAOak/ME5g9N5mkpM/w1228-h691-no/20141129_143244.jpg&#34; alt=&#34;ready for shaping&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;Lightly flour the bench and turn out your dough.   Cut off a piece to use and put the rest back in the bowl and back into the fridge:
&lt;img src=&#34;https://lh6.googleusercontent.com/-B3N3OIQFnsA/VHtUSAIq2OI/AAAAAAAAOac/GsZeRXf4yjk/w1228-h691-no/20141129_143342.jpg&#34; alt=&#34;fresh from the cold ferment&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Protip: You can use the final piece of dough as the starter for the next batch and skip a step.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Gently stretch the piece dough out into a squaring shape and then fold each edge into the middle:
&lt;img src=&#34;https://lh3.googleusercontent.com/-jU4S-UI5kvw/VHtUQoyLJYI/AAAAAAAAOaU/VyQFf1qb06Y/w1228-h691-no/20141129_143427.jpg&#34; alt=&#34;folding&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;Turn the dough over and shape it into a ball:
&lt;img src=&#34;https://lh3.googleusercontent.com/-wHWpKg-1IVM/VHtUPA1T5QI/AAAAAAAAOaM/ANaEWZyDuJ0/w1228-h691-no/20141129_143521.jpg&#34; alt=&#34;dough balls&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;Watch this &lt;a href=&#34;https://www.youtube.com/watch?feature=player_detailpage&amp;amp;v=4VdVrib2PQo#t=20&#34;&gt;video&lt;/a&gt; (not mine) for the technique used to shape the dough into balls.
&lt;div class=&#34;embed video-player&#34;&gt;
&lt;iframe class=&#34;youtube-player&#34; type=&#34;text/html&#34; width=&#34;640&#34; height=&#34;385&#34; src=&#34;http://www.youtube.com/embed/4VdVrib2PQo&#34; allowfullscreen frameborder=&#34;0&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;Oil your shaping bowl and flip your ball over so that the top of the dough is facing down in the bowl and cover it:
&lt;img src=&#34;https://lh5.googleusercontent.com/-xbzq6cwYJis/VHtUNHuDbiI/AAAAAAAAOaE/8EdixPbGN1k/w1228-h691-no/20141129_144021.jpg&#34; alt=&#34;final shaping&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;You can easily make Ciabatta ( just fold it over itself in one direction and crimp the edges ), or Baguettes ( harder to explain, but youtube has plenty of guides ), or any other style of bread that you prefer.   Even works great for pizza bases!&lt;/p&gt;

&lt;h2 id=&#34;the-final-ferment:5777482caf0981cefc6ea3a7ef676968&#34;&gt;The Final Ferment&lt;/h2&gt;

&lt;p&gt;Let the dough warm back up to room temperature and the yeasts to wake up and get active again.   This will probably take about two hours.  Crank the oven on to 450F after about an hour.  This gives the oven a good solid hour to get up to temperature and stabilize.&lt;/p&gt;

&lt;h2 id=&#34;the-bake:5777482caf0981cefc6ea3a7ef676968&#34;&gt;The Bake&lt;/h2&gt;

&lt;p&gt;Turn the oven up to 500F, open the door, and slide the rack with the pizza stone out so that you can get to it without burning yourself.   upend the bowl onto the stone and run a knife quickly over the top to create a shallow cut:
&lt;img src=&#34;https://lh3.googleusercontent.com/-Pzk8zAWIWt0/VHtULim3WvI/AAAAAAAAOZ8/NTrxiDnOXmk/w1228-h691-no/20141129_155656.jpg&#34; alt=&#34;ready for baking&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;Bake for 5 minutes, then turn the oven back down to 450F and bake for another 20 minutes:
&lt;img src=&#34;https://lh4.googleusercontent.com/-hX_Gpo4YIOE/VHtTrWwzT8I/AAAAAAAAOZ0/EY1snf5ymew/w1228-h691-no/20141129_180734.jpg&#34; alt=&#34;baked&#34; /&gt;
&lt;/p&gt;

&lt;h2 id=&#34;the-cooldown:5777482caf0981cefc6ea3a7ef676968&#34;&gt;The Cooldown&lt;/h2&gt;

&lt;p&gt;Avoid the tempation to cut into the bread while its still hot.  Bread continues to cook as it cools down and its important to not allow steam and heat to escape.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh4.googleusercontent.com/-8v2AonhRmTE/VH9eUSh3hxI/AAAAAAAAOdE/kx4in89KNTk/s640/20141202_184332.jpg&#34; alt=&#34;some other styles&#34; /&gt;
&lt;/p&gt;

&lt;h2 id=&#34;the-eatening:5777482caf0981cefc6ea3a7ef676968&#34;&gt;The Eatening&lt;/h2&gt;

&lt;p&gt;Fresh bread is best enjoyed simply with some high quality extra virgin olive oil and balsamic vinegar:
&lt;img src=&#34;https://lh3.googleusercontent.com/-_AuYSwAc_8A/VHtTnddC1CI/AAAAAAAAOZc/9cgRejB_s4g/w1228-h691-no/20141129_181010.jpg&#34; alt=&#34;ready to eat&#34; /&gt;
&lt;/p&gt;

&lt;h2 id=&#34;tips-and-tricks:5777482caf0981cefc6ea3a7ef676968&#34;&gt;Tips and Tricks&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Make your starter from Ikea&amp;rsquo;s &lt;a href=&#34;http://www.ikea.com/us/en/catalog/products/00229031/&#34;&gt;BRÖDMIX FLERKORN&lt;/a&gt; which contains all sorts of interesting grains as well as Yeast and &amp;ldquo;Sourdough Powder&amp;rdquo; whatever that is.&lt;/li&gt;
&lt;li&gt;Make &lt;a href=&#34;http://food.paulcz.net/2010/05/sourdough-pancakes.html&#34;&gt;sourdough pancakes&lt;/a&gt; from your leftover starter.&lt;/li&gt;
&lt;li&gt;Shape your dough into a &lt;a href=&#34;http://food.paulcz.net/2013/02/austin-style-pizza.html&#34;&gt;pizza dough&lt;/a&gt; and bake it in a cast iron skillet.&lt;/li&gt;
&lt;li&gt;You don&amp;rsquo;t have to make round loafs. use a sandwhich loaf tin, or make ciabatta or baguettes.  It&amp;rsquo;s all it the shaping.&lt;/li&gt;
&lt;li&gt;If you&amp;rsquo;re brave make your dough wetter and ferment for longer.   you&amp;rsquo;ll get cazy large holes in your crumb.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>about</title>
      <link>http://tech.paulcz.net/page/about/</link>
      <pubDate>Sun, 30 Nov 2014 09:04:08 -0600</pubDate>
      
      <guid>http://tech.paulcz.net/page/about/</guid>
      <description>&lt;p&gt;Hi.  My name is Paul.  I like stuff.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>EZBake - A new way to converge docker containers with chef</title>
      <link>http://tech.paulcz.net/2014/05/ezbake-a-new-way-to-converge-docker-containers-with-chef/</link>
      <pubDate>Tue, 13 May 2014 00:00:00 +0000</pubDate>
      
      <guid>http://tech.paulcz.net/2014/05/ezbake-a-new-way-to-converge-docker-containers-with-chef/</guid>
      <description>&lt;p&gt;&lt;code&gt;EZ Bake&lt;/code&gt; came from an idea I had while watching the &lt;a href=&#34;https://twitter.com/hangops&#34;&gt;HangOps&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=clLFKIeSADo&amp;amp;feature=youtu.be&#34;&gt;episode 2014-04-11&lt;/a&gt; in which they were talking about &lt;code&gt;Docker&lt;/code&gt; and Config Management being complementary rather than adversary.&lt;/p&gt;

&lt;p&gt;I have expermented with using &lt;code&gt;Chef&lt;/code&gt; and &lt;code&gt;Docker&lt;/code&gt; together in the &lt;a href=&#34;http://tech.paulcz.net/2013/09/creating-immutable-servers-with-chef-and-docker-dot-io.html&#34;&gt;past&lt;/a&gt; but wanted to tackle the problem from a slightly different angle.  I&amp;rsquo;ve recently been working on some PAAS stuff, both &lt;a href=&#34;http://deis.io&#34;&gt;Deis&lt;/a&gt; and &lt;a href=&#34;http://solum.io&#34;&gt;Solum&lt;/a&gt; these both utilize the tooling from &lt;a href=&#34;https://github.com/flynn/flynn&#34;&gt;Flynn&lt;/a&gt; which builds heroku style &lt;code&gt;buildpacks&lt;/code&gt; in &lt;code&gt;Docker&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;EZ Bake takes chef recipes designed for &lt;code&gt;chef-solo&lt;/code&gt; ( but could easily be extended to do the same for &lt;code&gt;chef-zero&lt;/code&gt;, or &lt;code&gt;chef-client&lt;/code&gt; with a server) in a tarball via &lt;code&gt;stdin&lt;/code&gt; and converges a docker node using that recipe.&lt;/p&gt;

&lt;p&gt;This methodology seems a little weird at first,  but it gives you the ability to ship your Chef cookbooks as self-contained tarballs, or even more interestingly use the &lt;code&gt;git archive&lt;/code&gt; command from your git repository to do this automatically and then pipe that directly to the &lt;code&gt;docker run&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;In order to recognize and run your cookbook ( or repo ) it needs to contain the following files: &lt;code&gt;Berksfile&lt;/code&gt;, &lt;code&gt;solo.json&lt;/code&gt;, &lt;code&gt;solo.rb&lt;/code&gt; in the root of your cookbook.   There is some provision for providing different locations for these via environment variables.   This is pre-ChefDK and will probably become easier with ChefDK.&lt;/p&gt;

&lt;p&gt;I have provided an example in the ezbake repo that will install Java7 in the container.&lt;/p&gt;

&lt;p&gt;This example shows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Converging a container using a local chef recipe&lt;/li&gt;
&lt;li&gt;Committing the container to an image on completion&lt;/li&gt;
&lt;li&gt;Removing the build container&lt;/li&gt;
&lt;li&gt;Running the new image&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;$ git clone paulczar/ezbake
$ cd ezbake/examples
$ ID=$(tar cf - . | sudo docker run -i -a stdin paulczar/ezbake) \
  &amp;amp;&amp;amp; sudo docker attach $ID \
  &amp;amp;&amp;amp; sudo docker commit $ID java7 
  &amp;amp;&amp;amp; sudo docker rm $ID

Running Berkshelf to collect your cookbooks:
Installing java (1.22.0) from site: &#39;http://cookbooks.opscode.com/api/v1/cookbooks&#39;
Converging your container:
[2014-04-12T22:10:24+00:00] INFO: Forking chef instance to converge...
....
[2014-04-12T22:16:52+00:00] INFO: Chef Run complete in 154.563192281 seconds
[2014-04-12T22:16:52+00:00] INFO: Running report handlers
[2014-04-12T22:16:52+00:00] INFO: Report handlers complete

$ sudo docker run -t java7 java -version
java version &amp;quot;1.7.0_51&amp;quot;
Java(TM) SE Runtime Environment (build 1.7.0_51-b13)
Java HotSpot(TM) 64-Bit Server VM (build 24.51-b03, mixed mode)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This could easily be built into a CI pipeline.   a git webhook could call jenkins which would clone the repo and then use a command like  &lt;code&gt;git archive master | docker run -i -a stdin paulczar/ezbake&lt;/code&gt; to converge a container from it.&lt;/p&gt;

&lt;p&gt;It could also very easily be used in &lt;code&gt;Deis&lt;/code&gt; or &lt;code&gt;Solum&lt;/code&gt; as an alternative to a Heroku buildpack.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Running DEIS.IO on Rackspace Cloud</title>
      <link>http://tech.paulcz.net/2014/02/running-deis-io-on-rackspace-cloud/</link>
      <pubDate>Sun, 23 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>http://tech.paulcz.net/2014/02/running-deis-io-on-rackspace-cloud/</guid>
      <description>

&lt;p&gt;I recently did a presentation at the Cloud Austin meetup titled &lt;a href=&#34;http://tech.paulcz.net/presentation-cloud-austin-deis/#/&#34;&gt;Docking with Unicorns&lt;/a&gt; about new PAAS on the block &lt;a href=&#34;http://deis.io&#34;&gt;DEIS&lt;/a&gt;.   Building out DEIS is quite easy,  make more easy by some tight integration they have with Rackspace Cloud.    If you&amp;rsquo;re interested in what deis is go through my slides linked above, and the documentation on their website.    If you want to build out an environment to kick the tires a bit,  then click &amp;lsquo;Read on&amp;rsquo; below and follow me down the rabbit hole.&lt;/p&gt;

&lt;h2 id=&#34;chef-setup:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Chef setup&lt;/h2&gt;

&lt;p&gt;Chef offers a free hosted service for up to five servers.  That&amp;rsquo;s plenty for this exercise so go to the &lt;a href=&#34;https://www.getchef.com/account&#34;&gt;registration page&lt;/a&gt; and create yourself a user.  At some point it will prompt you to generate and save a key, do that and download it.&lt;/p&gt;

&lt;p&gt;Once you have signed up you can download a knife config file and generate a validation key from the &lt;a href=&#34;https://manage.opscode.com/organizations&#34;&gt;Organizations&lt;/a&gt; page.  We can save those down and then move them to a local working directory.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh5.googleusercontent.com/-3R-Z-bRi_s0/UwpipiLhhWI/AAAAAAAAN0Q/W6q_Rb7NFy8/w1240-h663-no/opscode-org-page.png&#34; alt=&#34;chef org setup&#34; /&gt;
&lt;/p&gt;

&lt;h3 id=&#34;prepare-working-environment:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Prepare Working Environment&lt;/h3&gt;

&lt;p&gt;Create a &lt;code&gt;~/paas&lt;/code&gt; working directory and configure your local chef tools like this ( change the Download location to match the files you downloaded above ) :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mkdir -p ~/paas/.chef
$ cd ~/paas
$ mv ~/Downloads/&amp;lt;username&amp;gt;.pem .chef/
$ mv ~/Downloads/knife.rb .chef/
$ mv ~/Downloads/&amp;lt;username&amp;gt;-validator.pem .chef/

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;clone-the-deis-repository:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Clone the Deis Repository&lt;/h3&gt;

&lt;p&gt;Clone the deis project into your paas working directory:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd ~/paas
$ git clone https://github.com/opdemand/deis.git
Cloning into &#39;deis&#39;...
remote: Reusing existing pack: 5651, done.
Receiving objects: 100% (5651/5651), 2.16 MiB | 1.37 MiB/s, done.
remote: Total 5651 (delta 0), reused 0 (delta 0)
Resolving deltas: 100% (3131/3131), done.
Checking connectivity... done

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;install-pre-reqs:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Install Pre-reqs&lt;/h3&gt;

&lt;p&gt;Assuming you have a working &lt;code&gt;Ruby 1.9.3+&lt;/code&gt; and the &lt;code&gt;bundler&lt;/code&gt; gem installed you should be able to use the &lt;code&gt;Gemfile&lt;/code&gt; from the deis project to ensure you have all the necessary tools:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd ~/paas/deis
$ bundle install
bundle install
Fetching gem metadata from https://rubygems.org/.......
Fetching additional metadata from https://rubygems.org/..
Using i18n (0.6.9)
Using multi_json (1.8.4)
Using activesupport (3.2.16)
Using addressable (2.3.5)
...
Using bundler (1.5.2)
Your bundle is complete!
Use `bundle show [gemname]` to see where a bundled gem is installed.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;I had some errors installing the eventmachine gem and had to follow &lt;a href=&#34;https://github.com/gitlabhq/gitlabhq/issues/1051#issuecomment-9176547&#34;&gt;this fix&lt;/a&gt; to get bundle install to work&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;test-chef-connectivity:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Test Chef Connectivity&lt;/h3&gt;

&lt;p&gt;To make sure we configured chef correctly and installed knife as part of the bundle we can run a quick knife command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec knife client list
&amp;lt;USERNAME&amp;gt;-validator
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;create-an-environment-for-deis:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Create an Environment for Deis&lt;/h3&gt;

&lt;p&gt;Deis is currently hardcoded to use the &lt;code&gt;_default&lt;/code&gt; chef environment.    There is a current &lt;a href=&#34;https://github.com/opdemand/deis/issues/523&#34;&gt;issue&lt;/a&gt; on their github to resolve this.   Once that is done I&amp;rsquo;ll update these instructions to create a &lt;code&gt;deis&lt;/code&gt; environment.&lt;/p&gt;

&lt;h3 id=&#34;upload-the-deis-cookbooks:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Upload the Deis Cookbooks&lt;/h3&gt;

&lt;p&gt;If that went well we can upload our cookbooks:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd ~/paas/deis
$ bundle exec berks install
Installing apt (2.3.8) from site: &#39;http://cookbooks.opscode.com/api/v1/cookbooks&#39;
Installing docker (0.31.0) from site: &#39;http://cookbooks.opscode.com/api/v1/cookbooks&#39;
Installing rsyslog (1.10.2) from site: &#39;http://cookbooks.opscode.com/api/v1/cookbooks&#39;
Installing sudo (2.3.0) from site: &#39;http://cookbooks.opscode.com/api/v1/cookbooks&#39;
...
$ bundle exec berks upload
Using apt (2.3.8)
Using docker (0.31.0)
Using rsyslog (1.10.2)
Using sudo (2.3.0)
Installing deis (0.5.1) from git: &#39;https://github.com/opdemand/deis-cookbook.git&#39; with branch: &#39;master&#39; at ref: &#39;6361706a1d3245d2a061ed55f5dd4b7cb60d5e5c&#39;
Using git (2.7.0)
...
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;create-deis-databags:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Create Deis Databags&lt;/h3&gt;

&lt;p&gt;Deis uses some databags to help manage application state.  We can create them like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec knife data bag create deis-formations
Created data_bag[deis-formations]
$ bundle exec knife data bag create deis-apps
Created data_bag[deis-apps]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;prepare-infrastructure:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Prepare Infrastructure&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;m using Rackspace cloud servers for this as I have the (&lt;a href=&#34;http://developer.rackspace.com/blog/developer-love-welcome-to-the-rackspace-cloud-developer-discount.html)[Rackspace&#34;&gt;http://developer.rackspace.com/blog/developer-love-welcome-to-the-rackspace-cloud-developer-discount.html)[Rackspace&lt;/a&gt; Developer Discount] which is enough discount to host this for free.&lt;/p&gt;

&lt;p&gt;Since Deis will want your rackspace credentials to configure worker nodes I recomment creating a user under (&lt;a href=&#34;https://mycloud.rackspace.com/account#users/create)[User&#34;&gt;https://mycloud.rackspace.com/account#users/create)[User&lt;/a&gt; Management] in your account to use for this.&lt;/p&gt;

&lt;h3 id=&#34;create-a-cloud-load-balancer:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Create a Cloud Load Balancer&lt;/h3&gt;

&lt;p&gt;Log into mycloud.rackspace.com and click on the (&lt;a href=&#34;https://mycloud.rackspace.com/load_balancers)[Load&#34;&gt;https://mycloud.rackspace.com/load_balancers)[Load&lt;/a&gt; Balancers] button.  Select the Dallas Region (DFW) and hit &lt;code&gt;Create Load Balancer&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Set the Name to &lt;code&gt;deis&lt;/code&gt; and check the region is set to &lt;code&gt;Dallas (DFW)&lt;/code&gt; and hit &lt;code&gt;Create Load Balancer&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://lh4.googleusercontent.com/-E4cZvoKWlYU/Uwpiqr9xOKI/AAAAAAAAN0o/P3vGqPC8A98/w793-h592-no/rackspace-create-lb.png&#34; alt=&#34;creating load balancer&#34; /&gt;
&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Take note of the public IP of the Load Balancer, we&amp;rsquo;ll need it later.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://lh4.googleusercontent.com/-ORvf6nzEduU/Uwpiqk5eP0I/AAAAAAAAN0k/WZ-NaJn3eJg/w770-h567-no/rackspace-lb.png&#34; alt=&#34;load balancer created&#34; /&gt;
&lt;/p&gt;

&lt;h3 id=&#34;wildcard-dns:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Wildcard DNS&lt;/h3&gt;

&lt;p&gt;Deis&amp;rsquo; proxy layer requires you to set up Wildcard DNS to point to your proxy layer.  There are many ways to achieve this here are two options:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Rackspace Cloud DNS can host wildcard DNS entries, if you already have DNS hosted by rackspace using Cloud DNS simply add an A record for &lt;code&gt;*.deis&lt;/code&gt; under your domain and point it to the IP of your load balancer.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The (&lt;a href=&#34;http://xip.io)[xip.io&#34;&gt;http://xip.io)[xip.io&lt;/a&gt;] domain does wildcard DNS based on your IP.  We can use this with our Cloud Load Balancer to load balance our applications.   My Load Balancer has a public IP of &lt;code&gt;50.56.167.26&lt;/code&gt; therefore my wildcard domain will be &lt;code&gt;50.56.167.26.xip.io&lt;/code&gt;.   Remember this.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;configure-knife-for-rackspace:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Configure Knife for Rackspace&lt;/h3&gt;

&lt;p&gt;The bundle install above already installed the rackspace knife plugin so we just need to add some details to &lt;code&gt;.chef/knife.rb&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat &amp;lt;&amp;lt;&#39;EOF&#39; &amp;gt;&amp;gt; $HOME/.chef/knife.rb
knife[:rackspace_api_username] = &amp;quot;#{ENV[&#39;OS_USERNAME&#39;]}&amp;quot;
knife[:rackspace_api_key]      = &amp;quot;#{ENV[&#39;OS_PASSWORD&#39;]}&amp;quot;
knife[:rackspace_version]      = &#39;v2&#39;
knife[:rackspace_region]       = :dfw
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;install-rackspace-nova-client:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Install Rackspace Nova Client&lt;/h3&gt;

&lt;p&gt;We also need the Nova client:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo pip install rackspace-novaclient
$ cat &amp;lt;&amp;lt;&#39;EOF&#39; &amp;gt;&amp;gt; ~/paas/.chef/openrc
export OS_AUTH_URL=https://identity.api.rackspacecloud.com/v2.0/
export OS_AUTH_SYSTEM=rackspace
export OS_REGION_NAME=DFW
export OS_USERNAME=&amp;lt;RACKSPACE_USERNAME&amp;gt;
export NOVA_RAX_AUTH=1
export OS_PASSWORD=&amp;lt;RACKSPACE_API_KEY&amp;gt;
export OS_NO_CACHE=1
export OS_TENANT_NAME=&amp;lt;RACKSPACE_USERNAME&amp;gt;
EOF
$ source ~/paas/.chef/openrc
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;test-rackspace-connectivity:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Test Rackspace Connectivity&lt;/h3&gt;

&lt;p&gt;Make sure you can connect to Rackspace with Knife:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec knife rackspace server list
Instance ID  Name  Public IP  Private IP  Flavor  Image  State
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Make sure you can connect to Rackspace with nova:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ nova list
+--------------------------------------+-----------------+--------+------------+-------------+----------------------------------------------------------------------------------------+
| ID                                   | Name            | Status | Task State | Power State | Networks                                                                               |
+--------------------------------------+-----------------+--------+------------+-------------+----------------------------------------------------------------------------------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;build-base-images-for-controller-and-nodes:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Build base images for Controller and Nodes.&lt;/h2&gt;

&lt;p&gt;This isn&amp;rsquo;t strictly necessary,  but will help build your nodes quicker on subsequent builds.&lt;/p&gt;

&lt;h3 id=&#34;launce-a-new-instance:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Launce a new instance:&lt;/h3&gt;

&lt;p&gt;If we create a base image and pre-install some software we&amp;rsquo;ll get a faster booting system for auto-provisioning:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec knife rackspace server create \
  --image &#39;80fbcb55-b206-41f9-9bc2-2dd7aac6c061&#39; \
  --node-name &#39;deis-base-image&#39; \
  --flavor &#39;performance1-1&#39;
...
...
Instance ID: 56760bf1-b977-405e-9348-f70b15a14b87
Host ID: 97da00a12312a7e455bda70c6dfab8833953e2a03b081aeedfd68152
Name: deis-base-image
Flavor: 1 GB Performance
Image: Ubuntu 12.04 
Metadata: []
Public DNS Name: 23-253-69-98.xip.io
Public IP Address: 23.253.69.98
Private IP Address: 10.208.101.31
Password: **************
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Take note of the &lt;code&gt;Instance ID&lt;/code&gt;, &lt;code&gt;Public IP Address&lt;/code&gt; and &lt;code&gt;Password&lt;/code&gt;.  We&amp;rsquo;ll need them later.&lt;/p&gt;

&lt;h3 id=&#34;add-users-keys-to-instance:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Add users / keys to instance&lt;/h3&gt;

&lt;p&gt;We&amp;rsquo;re going to add our ssh key as well as a local &lt;code&gt;deis-ops&lt;/code&gt; user to the image to make it easier to manage and troubleshoot later:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ DEIS_IP=&amp;lt;IP_OF_SERVER&amp;gt;
$ ssh-copy-id root@$DEIS_IP
root@162.242.144.193&#39;s password: 
Number of key(s) added: 1
Now try logging into the machine, with:   &amp;quot;ssh &#39;root@162.242.144.193&#39;&amp;quot;
and check to make sure that only the key(s) you wanted were added.
$ ssh root@$DEIS_IP
Welcome to Ubuntu 12.04.3 LTS (GNU/Linux 3.2.0-55-virtual x86_64)

 * Documentation:  https://help.ubuntu.com/

  System information as of Sun Feb 23 18:34:40 UTC 2014

  System load:  0.08              Processes:           60
  Usage of /:   5.5% of 19.68GB   Users logged in:     0
  Memory usage: 6%                IP address for eth0: 162.242.144.193
  Swap usage:   0%                IP address for eth1: 10.208.135.114

  Graph this data and manage this system at https://landscape.canonical.com/

Last login: Sun Feb 23 18:33:02 2014 from cpe-24-27-47-27.austin.res.rr.com
root@deis-base-image:~# useradd --comment &#39;deis ops user&#39; --home-dir &#39;/home/deis-ops&#39; \
  --shell &#39;/bin/bash&#39; --create-home deis-ops
root@deis-base-image:~# mkdir -p /home/deis-ops/.ssh &amp;amp;&amp;amp; \
   cp /root/.ssh/authorized_keys /home/deis-ops/.ssh/authorized_keys &amp;amp;&amp;amp; \
  chown -R deis-ops:deis-ops /home/deis-ops &amp;amp;&amp;amp; \
  chmod 0700 /home/deis-ops/.ssh &amp;amp;&amp;amp; \
  chmod 0600 /home/deis-ops/.ssh/authorized_keys &amp;amp;&amp;amp; \
  echo &#39;deis-ops ALL=(ALL) NOPASSWD:ALL&#39; &amp;gt; /etc/sudoers.d/deis-ops &amp;amp;&amp;amp; \
  chmod 0440 /etc/sudoers.d/deis-ops
root@deis-base-image:~# exit
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check that you can log in with these new creds:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh deis-ops@$DEIS_IP
deis$ sudo bash
root@deis$ exit
deis$ exit
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;finish-preparing-node-image:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Finish preparing node image&lt;/h3&gt;

&lt;p&gt;Next we&amp;rsquo;re going to update the kernel and prepare the base node image.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh deis-ops@$DEIS_IP &#39;sudo apt-get update&#39;
$ scp contrib/rackspace/*.sh deis-ops@$DEIS_IP:~/
$ ssh deis-ops@$DEIS_IP &#39;sudo ~/prepare-node-image.sh&#39;
$ ssh deis-ops@$DEIS_IP &#39;sudo apt-get install -yq linux-image-generic-lts-raring linux-headers-generic-lts-raring&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;create-an-image-from-this-server:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Create an image from this server&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ nova image-create deis-base-image deis-node-image
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After a few minutes you should see this response to running &lt;code&gt;nova image-list&lt;/code&gt;, if you&amp;rsquo;re impatient like me wrap your command with a &lt;code&gt;watch&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ watch &#39;nova image-list | grep deis&#39;
| df958d26-6515-4dd9-a449-920e74ea93a2 | deis-base-image                                              | ACTIVE | 0fc7f68b-176d-49a9-82ff-2d5893d32acd |

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once the image is active we can move onto the next steps.&lt;/p&gt;

&lt;h3 id=&#34;prepare-controller-image:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Prepare controller image&lt;/h3&gt;

&lt;p&gt;Next we want to prepare the VM for the controller image:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh deis-ops@$DEIS_IP &#39;sudo ~/prepare-controller-image.sh&#39;
$ ssh deis-ops@$DEIS_IP &#39;sudo apt-get install -yq linux-image-generic-lts-raring linux-headers-generic-lts-raring&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;create-an-image-from-this-server-1:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Create an image from this server&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ nova image-create deis-base-image deis-base-image
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After a few minutes you should see this response to running &lt;code&gt;nova image-list&lt;/code&gt;, if you&amp;rsquo;re impatient like me wrap your command with a &lt;code&gt;watch&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ watch &#39;nova image-list | grep deis-node&#39;
| f2236fa6-1e2d-4746-ac87-a3dd6b2de811 | deis-node-image                                              | ACTIVE | 633d5d88-54b3-463c-80fe-c119f4eb33a3 |

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;delete-the-instance:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Delete the instance&lt;/h3&gt;

&lt;p&gt;No need to keep the instance around and keep paying for it once you have the image:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec knife rackspace server list | grep deis  
42899699-68e7-4785-9f49-e0050f86249a  deis-base-image  162.242.144.193  10.208.135.114  performance1-1  80fbcb55-b206-41f9-9bc2-2dd7aac6c061  active
$ bundle exec knife rackspace server delete 42899699-68e7-4785-9f49-e0050f86249a --purge
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;create-the-deis-controller-server:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Create the Deis Controller server&lt;/h2&gt;

&lt;h3 id=&#34;launch-the-server:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Launch the Server&lt;/h3&gt;

&lt;p&gt;Launch the server from the image you created earlier:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ nova image-list | grep  deis-base-image
| a58c9895-6349-442a-bba7-99611900209d | deis-base-image
$ knife rackspace server create \
  --image a58c9895-6349-442a-bba7-99611900209d \
  --rackspace-metadata &amp;quot;{\&amp;quot;Name\&amp;quot;: \&amp;quot;deis-controller\&amp;quot;}&amp;quot; \
  --rackspace-disk-config MANUAL \
  --server-name deis-controller \
  --node-name deis-controller \
  --flavor &#39;performance1-2&#39;
Instance ID: bb713170-9322-424a-8837-863a4b396705
Name: deis-controller
Flavor: 2 GB Performance
Image: deis-base-image
...
Public IP Address: 23.253.104.13
Private IP Address: 10.208.132.190
Password: CQwDU4m97nvF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Take note of the &lt;code&gt;Instance ID&lt;/code&gt; and &lt;code&gt;Public IP Address&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If you have an easy to manage domain add an A record for &lt;code&gt;deis&lt;/code&gt; to it for the Public IP address.  If not
add an entry to your hosts file ( or do both! I did ):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo sh -c &amp;quot;echo &#39;&amp;lt;IP_OF_SERVER&amp;gt; deis&#39; &amp;gt;&amp;gt; /etc/hosts&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;modify-chef-admin-group:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Modify Chef Admin Group&lt;/h3&gt;

&lt;p&gt;On the Chef management website click (&lt;a href=&#34;https://manage.opscode.com/groups/admins/edit)[Groups&#34;&gt;https://manage.opscode.com/groups/admins/edit)[Groups&lt;/a&gt;] and add the &lt;code&gt;deis-controller&lt;/code&gt; client and your validator client to the &lt;code&gt;admins&lt;/code&gt; group.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh5.googleusercontent.com/-oSqB1Tdnn4c/UwpioPAXpJI/AAAAAAAANz4/xa8BdmRuTzQ/w579-h580-no/chef-admins.png&#34; alt=&#34;chef admins group&#34; /&gt;
&lt;/p&gt;

&lt;h3 id=&#34;converge-the-deis-controller-server:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Converge the Deis Controller Server&lt;/h3&gt;

&lt;p&gt;Edit the &lt;code&gt;deis-controller&lt;/code&gt; node via this command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ EDITOR=vi knife node edit deis-controller
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;make it look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;name&amp;quot;: &amp;quot;deis-controller&amp;quot;,
  &amp;quot;chef_environment&amp;quot;: &amp;quot;_default&amp;quot;,
  &amp;quot;normal&amp;quot;: {
    &amp;quot;tags&amp;quot;: [

    ]
  },
  &amp;quot;run_list&amp;quot;: [
    &amp;quot;recipe[deis::controller]&amp;quot;
  ]
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;then converge the node by running chef client on it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh deis-ops@deis sudo chef-client
[2014-02-23T19:25:32+00:00] INFO: Forking chef instance to converge...
[2014-02-23T19:25:32+00:00] INFO: *** Chef 11.6.2 ***
[2014-02-23T19:25:33+00:00] INFO: Run List is [recipe[deis::controller]]
[2014-02-23T19:25:33+00:00] INFO: Run List expands to [deis::controller]
[2014-02-23T19:25:33+00:00] INFO: Starting Chef Run for deis-controller
[2014-02-23T19:25:33+00:00] INFO: Running start handlers
[2014-02-23T19:25:33+00:00] INFO: Start handlers complete.
...
$
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;testing-deis:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Testing Deis&lt;/h2&gt;

&lt;h3 id=&#34;install-the-deis-client-with-pip:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Install the Deis Client with pip&lt;/h3&gt;

&lt;p&gt;The Deis client is written in python and can be installed by &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo pip install deis  
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;register-admin-user:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Register Admin User&lt;/h3&gt;

&lt;p&gt;First user to register becomes the Admin:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ deis register http://deis:8000
username: admin
password: 
password (confirm): 
email: admin@example.com
Registered admin
Logged in as admin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Push your public key to deis:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ deis keys:add ~/.ssh/id_rsa.pub 
Uploading SSH_KEY to Deis...done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;check the web server is serving content by browsing to (&lt;a href=&#34;http://deis)[http://deis&#34;&gt;http://deis)[http://deis&lt;/a&gt;] and entering your admin credentials.&lt;/p&gt;

&lt;h3 id=&#34;teach-deis-your-provider-credentials:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Teach Deis your provider credentials&lt;/h3&gt;

&lt;p&gt;Deis will automatically provision worker nodes if you teach it your credentials.&lt;/p&gt;

&lt;p&gt;We already have our Rackspace credentials saved to &lt;code&gt;~/paas/.chef/openrc&lt;/code&gt; but Deis wants them named differently:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export RACKSPACE_USERNAME=$OS_USERNAME
$ export RACKSPACE_API_KEY=$OS_PASSWORD
$ deis providers:discover
No EC2 credentials discovered.
Discovered Rackspace credentials: ****************
Import Rackspace credentials? (y/n) : y
Uploading Rackspace credentials... done
No DigitalOcean credentials discovered.
No Vagrant VMs discovered.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;deploy-formations-layers:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Deploy Formations &amp;amp; Layers&lt;/h2&gt;

&lt;h3 id=&#34;formation:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Formation&lt;/h3&gt;

&lt;p&gt;Formations are collections of infrastructure for serving applications.   We&amp;rsquo;ll call our first Formation &lt;code&gt;dev&lt;/code&gt; for development.&lt;/p&gt;

&lt;p&gt;Create formation (using the wildcard domain from our cloud load balancer created earlier in the &lt;code&gt;--domain&lt;/code&gt; argument):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ deis formations:create dev --domain=50.56.167.26.xip.io
Creating formation... done, created dev
See `deis help layers:create` to begin building your formation
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;layers:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Layers&lt;/h3&gt;

&lt;p&gt;Layers are a heterogenerous collection of nodes that perform one of two function:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Proxy - Directs traffic to the appropriate container running the application.&lt;/li&gt;
&lt;li&gt;Runtime - Runs the containers that hold the applications.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We&amp;rsquo;re going to create a layer called &lt;code&gt;nodes&lt;/code&gt; that will perform both the proxy and runtime functions:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ deis layers:create dev nodes rackspace-dfw --proxy=y --runtime=y
Creating nodes layer... done in 4s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;note&lt;/em&gt; There&amp;rsquo;s currently a &lt;a href=&#34;https://github.com/opdemand/deis/issues/541&#34;&gt;bug&lt;/a&gt; that causes the first creation of a layer to fail.  if that happens run the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;deis formations:create dev --domain=50.56.167.26.xip.io
Creating formation... done, created dev

See `deis help layers:create` to begin building your formation
$ deis layers:create dev nodes rackspace-dfw --proxy=y --runtime=y
Creating nodes layer... 500 INTERNAL SERVER ERROR
&amp;lt;h1&amp;gt;Server Error (500)&amp;lt;/h1&amp;gt;
$ deis layers:destroy dev nodes
Destroying nodes layer... done in 0s
$ deis layers:create dev nodes rackspace-dfw --proxy=y --runtime=y
Creating nodes layer... done in 2s

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;build-nodes:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Build Nodes&lt;/h3&gt;

&lt;p&gt;Next we tell deis to spin up two Cloud Servers which will become members of the &lt;code&gt;nodes&lt;/code&gt; layer.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ deis nodes:scale dev nodes=2
Scaling nodes... but first, coffee!
done in 345s
Use `deis create --formation=dev` to create an application
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This can sometimes take longer than the &lt;code&gt;deis&lt;/code&gt; cli timeout.   Don&amp;rsquo;t fear,  just wait a bit longer, this could be a great time to explore the &lt;code&gt;deis&lt;/code&gt; cli by running &lt;code&gt;deis help&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;update-cloud-load-balancer:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Update Cloud Load Balancer&lt;/h2&gt;

&lt;p&gt;Add these two nodes to the (&lt;a href=&#34;https://mycloud.rackspace.com/load_balancers)[Cloud&#34;&gt;https://mycloud.rackspace.com/load_balancers)[Cloud&lt;/a&gt; Load Balancer] we created earlier.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh6.googleusercontent.com/-yaJfxoyDk4M/UwpioEndiOI/AAAAAAAANz0/aXannmisdbE/w903-h407-no/cloud-servers-list.png&#34; alt=&#34;cloud server list&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;This is simple to do through the GUI:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Click on your load balancer and under &lt;code&gt;Nodes&lt;/code&gt; click the &lt;code&gt;Add Cloud Servers&lt;/code&gt; button.&lt;/li&gt;
&lt;li&gt;Check the box beside the two &lt;code&gt;dev-nodes&lt;/code&gt; servers and click &lt;code&gt;Add Selected Servers&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://lh6.googleusercontent.com/-zm6sB7l7YVk/Uwpin4BNJPI/AAAAAAAANzw/b-_J2ieyIuE/w773-h476-no/cloud-lb-nodes.png&#34; alt=&#34;cloud lb servers&#34; /&gt;
&lt;/p&gt;

&lt;h2 id=&#34;deploy-an-application:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Deploy an Application&lt;/h2&gt;

&lt;p&gt;So great, you have a PaaS, but what do you do now?  Deploy some apps of course!&lt;/p&gt;

&lt;h3 id=&#34;nodejs-example-app:59de91c1dc1797a6214d8b570db9750b&#34;&gt;NodeJS Example App&lt;/h3&gt;

&lt;p&gt;Download the NodeJS example application so like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mkdir -p ~/paas/apps
$ cd ~paas/apps
$ git clone https://github.com/opdemand/example-nodejs-express.git
$ cd example-nodejs-express
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;create-an-application-in-deis:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Create an Application in Deis&lt;/h3&gt;

&lt;p&gt;Use the Deis command line tool to create a new application:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ deis create      
Creating application... done, created exotic-sandwich
Git remote deis added
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;push-your-application-to-deis:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Push your Application to Deis&lt;/h3&gt;

&lt;p&gt;This will push, deploy and Launch the app.  The first one will take some time as deis has to download some docker images,  subsequent apps will be much faster:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git push deis master                     
git push deis master
Counting objects: 184, done.
Delta compression using up to 4 threads.
Compressing objects: 100% (89/89), done.
Writing objects: 100% (184/184), 28.77 KiB | 0 bytes/s, done.
Total 184 (delta 103), reused 165 (delta 92)
-----&amp;gt; Node.js app detected
-----&amp;gt; Requested node range: 0.10.x
-----&amp;gt; Resolved node version: 0.10.26
-----&amp;gt; Downloading and installing node
-----&amp;gt; Installing dependencies
       npm WARN package.json example-nodejs-express@0.0.1 No repository field.
       npm http GET https://registry.npmjs.org/express
       npm http 200 https://registry.npmjs.org/express
...
-----&amp;gt; Caching node_modules directory for future builds
-----&amp;gt; Cleaning up node-gyp and npm artifacts
-----&amp;gt; Building runtime environment
-----&amp;gt; Discovering process types
       Procfile declares types -&amp;gt; web
-----&amp;gt; Compiled slug size is 5.5M
-----&amp;gt; Building Docker image
Uploading context 5.698 MB
Uploading context 
Step 0 : FROM deis/slugrunner
 ---&amp;gt; bb0a27915014
Step 1 : RUN mkdir -p /app
 ---&amp;gt; Running in 1ae5cdeaad9a
 ---&amp;gt; 6e6467466d48
Step 2 : ADD slug.tgz /app
 ---&amp;gt; 191a4345b1e4
Step 3 : ENTRYPOINT [&amp;quot;/runner/init&amp;quot;]
 ---&amp;gt; Running in d322512d5865
 ---&amp;gt; 2866cf3e37c9
Successfully built 2866cf3e37c9
-----&amp;gt; Pushing image to private registry
       Launching... done, v2

-----&amp;gt; exotic-sandwich deployed to Deis
       http://exotic-sandwich.50.56.167.26.xip.io

       To learn more, use `deis help` or visit http://deis.io

To ssh://git@deis:2222/exotic-sandwich.git
 * [new branch]      master -&amp;gt; master

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;did-it-work:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Did it work ?&lt;/h2&gt;

&lt;p&gt;Open your web browser to the URL in the output of the previous command.  In my case this was &lt;code&gt;http://exotic-sandwich.50.56.167.26.xip.io&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If everything worked the text in the browser window should read &lt;code&gt;Powered by Deis&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh6.googleusercontent.com/-cxuysxM_oM8/UwpipfiKFMI/AAAAAAAAN0U/M7T9dC6xJ-E/w446-h171-no/deis-app-1.png&#34; alt=&#34;deis app&#34; /&gt;
&lt;/p&gt;

&lt;h2 id=&#34;configure-and-scale-your-application:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Configure and Scale your application&lt;/h2&gt;

&lt;p&gt;We can set config parameters for our apps by running &lt;code&gt;deis config&lt;/code&gt;.   The example app we&amp;rsquo;re using has a config paramater &amp;lsquo;POWERED_BY&amp;rsquo; so we can set that by running:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ deis config:set POWERED_BY=&#39;DEIS and Rackspace&#39;
=== exotic-sandwich
POWERED_BY: DEIS and Rackspace
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://lh6.googleusercontent.com/-J5AcNytZLOQ/UwpipEdpeBI/AAAAAAAAN0E/WXWC08rxsBU/w507-h157-no/deis-app-2.png&#34; alt=&#34;deis app2&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;Expecting visitors?  Let&amp;rsquo;s scale your app to 5 nodes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ deis scale web=5
Scaling containers... but first, coffee!
done in 54s

=== exotic-sandwich Containers

--- web: `node server.js`
web.1 up 2014-02-23T20:22:07.241Z (dev-nodes-2)
web.2 up 2014-02-23T20:28:21.778Z (dev-nodes-1)
web.3 up 2014-02-23T20:28:21.788Z (dev-nodes-2)
web.4 up 2014-02-23T20:28:21.799Z (dev-nodes-1)
web.5 up 2014-02-23T20:28:21.810Z (dev-nodes-2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can see what your app is doing by running &lt;code&gt;deis info&lt;/code&gt; and &lt;code&gt;deis logs&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ deis info
=== exotic-sandwich Application
{
  &amp;quot;updated&amp;quot;: &amp;quot;2014-02-23T20:28:21.812Z&amp;quot;, 
  &amp;quot;uuid&amp;quot;: &amp;quot;ef618db6-f5a8-4cab-a7d9-d01e78036e3a&amp;quot;, 
  &amp;quot;created&amp;quot;: &amp;quot;2014-02-23T20:16:51.931Z&amp;quot;, 
  &amp;quot;formation&amp;quot;: &amp;quot;dev&amp;quot;, 
  &amp;quot;owner&amp;quot;: &amp;quot;admin&amp;quot;, 
  &amp;quot;id&amp;quot;: &amp;quot;exotic-sandwich&amp;quot;, 
  &amp;quot;containers&amp;quot;: &amp;quot;{\&amp;quot;web\&amp;quot;: 5}&amp;quot;
}

=== exotic-sandwich Containers

--- web: `node server.js`
web.1 up 2014-02-23T20:22:07.241Z (dev-nodes-2)
web.2 up 2014-02-23T20:28:21.778Z (dev-nodes-1)
web.3 up 2014-02-23T20:28:21.788Z (dev-nodes-2)
web.4 up 2014-02-23T20:28:21.799Z (dev-nodes-1)
web.5 up 2014-02-23T20:28:21.810Z (dev-nodes-2)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;$ deis logs
Feb 23 20:22:57 dev-nodes exotic-sandwich[web.1]: Server listening on port 10001 in development mode
Feb 23 20:25:38 dev-nodes exotic-sandwich[web.1]: Server listening on port 10001 in development mode
Feb 23 20:26:49 dev-nodes exotic-sandwich[web.1]: Server listening on port 10001 in development mode
Feb 23 20:28:28 dev-nodes exotic-sandwich[web.3]: Server listening on port 10003 in development mode
Feb 23 20:28:29 dev-nodes exotic-sandwich[web.5]: Server listening on port 10005 in development mode
Feb 23 20:29:11 dev-nodes exotic-sandwich[web.2]: Server listening on port 10002 in development mode
Feb 23 20:29:12 dev-nodes exotic-sandwich[web.4]: Server listening on port 10004 in development mode
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Congratulations!  You&amp;rsquo;ve successfully built out your own cost effective PAAS and deployed your first application to it.&lt;/p&gt;

&lt;p&gt;Speaking of costs &amp;hellip;  How much would this cost to run per month ?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Cloud Load Balancer - $10.95 / month&lt;/li&gt;
&lt;li&gt;Deis Controller - $57.60 / month&lt;/li&gt;
&lt;li&gt;Deis Nodes (x2) - $115.20 / month&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Total:  $183.75 / month.&lt;/p&gt;

&lt;p&gt;You could run all of this on a single server without a load balancer,  which means it would be just $57.60/month, which with the &lt;a href=&#34;http://developer.rackspace.com/devtrial/&#34;&gt;Rackspace Developer Discount&lt;/a&gt; would reduce down to just $7.60/month.&lt;/p&gt;

&lt;h1 id=&#34;epilogue:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Epilogue&lt;/h1&gt;

&lt;h2 id=&#34;cleanup:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Cleanup&lt;/h2&gt;

&lt;p&gt;Destroy your app:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ deis destroy

 !    WARNING: Potentially Destructive Action
 !    This command will destroy the application: exotic-sandwich
 !    To proceed, type &amp;quot;exotic-sandwich&amp;quot; or re-run this command with --confirm=exotic-sandwich

&amp;gt; exotic-sandwich
Destroying exotic-sandwich... done in 21s
Git remote deis removed
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;list your servers:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec knife rackspace server list
Instance ID                           Name             Public IP       Private IP      Flavor          Image                                 State 
7c43ecb9-1ba3-454c-a5f4-637b56961d68  dev-nodes        23.253.102.184  10.208.135.137  performance1-2  2d59cbce-92fa-412b-8a5e-6eb426ce7dc9  active
f89c4b25-6486-422a-907a-16b3b3223a5e  dev-nodes        23.253.102.158  10.208.137.18   performance1-2  2d59cbce-92fa-412b-8a5e-6eb426ce7dc9  active
bb713170-9322-424a-8837-863a4b396705  deis-controller  23.253.104.13   10.208.132.190  performance1-2  a58c9895-6349-442a-bba7-99611900209d  active
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Delete your servers by running the following command for each:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec knife rackspace server delete 7c43ecb9-1ba3-454c-a5f4-637b56961d68 --purge
Instance ID: 7c43ecb9-1ba3-454c-a5f4-637b56961d68
Host ID: e0da0172f321babe99aec9686c7b99ac7fa5ff8fa1ada934f5fae842
Name: dev-nodes
Flavor: 2 GB Performance
Image: deis-node-image
Public IP Address: 23.253.102.184
Private IP Address: 10.208.135.137

Do you really want to delete this server? (Y/N) y
[WARNING] Error Parsing response json - Yajl::ParseError
WARNING: Deleted server 7c43ecb9-1ba3-454c-a5f4-637b56961d68
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Clean up your chef:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec knife data bag delete deis-apps
$ bundle exec knife data bag delete deis-formations
$ bundle exec knife client delete dev-nodes-1
$ bundle exec knife client delete dev-nodes-2
$ bundle exec knife node delete dev-nodes-1
$ bundle exec knife node delete dev-nodes-2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Delete your glance images:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ nova image-delete deis-base-image
$ nova image-delete deis-node-image
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally delete your Cloud Load Balancer from the &lt;a href=&#34;https://mycloud.rackspace.com/load_balancers&#34;&gt;Rackspace UI&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Managing docker services with this one easy trick</title>
      <link>http://tech.paulcz.net/2013/10/managing-docker-services-with-this-one-easy-trick/</link>
      <pubDate>Sun, 20 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>http://tech.paulcz.net/2013/10/managing-docker-services-with-this-one-easy-trick/</guid>
      <description>

&lt;p&gt;I have been having a lot of internal debate about the idea of running more than one service in a docker container.   A Docker container is built to run a single process in the foreground and to live for only as long as that process is running.  This is great in a utopian world where servers are immutable and sysadmins drink tiki drinks on the beach,  however it doesn&amp;rsquo;t always translate well to the real world.&lt;/p&gt;

&lt;p&gt;Examples where you might want to be able to run multiple servers span from the simple use case of running &lt;code&gt;sshd&lt;/code&gt; as well as your application to running a web app such as &lt;code&gt;wordpress&lt;/code&gt; where you might want both &lt;code&gt;apache&lt;/code&gt; and &lt;code&gt;mysql&lt;/code&gt; running in the same container.&lt;/p&gt;

&lt;p&gt;Wrapping your applications in a supervisor daemon such as &lt;code&gt;runit&lt;/code&gt; seems like a perfect fit for this.  All you need to do is install &lt;code&gt;runit&lt;/code&gt; as part of your &lt;code&gt;dockerfile&lt;/code&gt; and then create appropriate service directories for the apps you want to run in the container.    I was doing some testing of this when I realized a quirk of &lt;code&gt;runit&lt;/code&gt; which I could exploit for evil.&lt;/p&gt;

&lt;p&gt;To start or stop a service with &lt;code&gt;runit&lt;/code&gt; is simply a matter of creating or deleting a symlink in a service directory,   so in theory if you could expose that directory to the server hosting the container you could exploit that to start and stop services from outside of the container.  &lt;code&gt;Docker&lt;/code&gt; volume mapping allows exactly this!&lt;/p&gt;

&lt;p&gt;Below you will find examples of running three services (logstash,elasticsearch,kibana) that make up the &lt;code&gt;logstash&lt;/code&gt; suite.&lt;/p&gt;

&lt;h2 id=&#34;start-by-cloning-the-demo-git-repository-and-run-demo-sh:dab52676160fc987cea1712f60e5a17c&#34;&gt;Start by cloning the demo git repository and run demo.sh&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/paulczar/docker-runit-demo.git
$ cd docker-runit-demo
$ ./demo.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;demo-sh-script:dab52676160fc987cea1712f60e5a17c&#34;&gt;demo.sh script&lt;/h3&gt;

&lt;h4 id=&#34;step-1-build-the-container:dab52676160fc987cea1712f60e5a17c&#34;&gt;Step 1:  Build the container&lt;/h4&gt;

&lt;p&gt;The script uses the below &lt;code&gt;Dockerfile&lt;/code&gt; to build the base container that we&amp;rsquo;ll be running.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Installs runit for service management
#
# Author: Paul Czarkowski
# Date: 10/20/2013

FROM paulczar/jre7
MAINTAINER Paul Czarkowski &amp;quot;paul@paulcz.net&amp;quot;

RUN apt-get update

RUN apt-get -y install curl wget git nginx
RUN apt-get -y install runit || echo

CMD [&amp;quot;/usr/sbin/runsvdir-start&amp;quot;]

&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;step-2-install-the-applications:dab52676160fc987cea1712f60e5a17c&#34;&gt;Step 2: Install the applications&lt;/h4&gt;

&lt;p&gt;This will take a few minutes the first time as it needs to download &lt;code&gt;logstash&lt;/code&gt;, &lt;code&gt;kibana&lt;/code&gt;, and &lt;code&gt;elasticsearch&lt;/code&gt; and stage them in a local &lt;code&gt;./opt&lt;/code&gt;directory.&lt;/p&gt;

&lt;h4 id=&#34;step-3-start-the-docker-container:dab52676160fc987cea1712f60e5a17c&#34;&gt;Step 3: Start the Docker container&lt;/h4&gt;

&lt;p&gt;Starts the &lt;code&gt;Docker&lt;/code&gt; container with the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -d -p 8080:80 -p 5014:514 -p 9200:9200 \
  -v $BASE/opt:/opt \
  -v $BASE/sv:/etc/sv \
  -v $BASE/init:/etc/init \
  -v $BASE/service:/etc/service \
  demo/runit
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The container should be up and running&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker ps
ID                  IMAGE               COMMAND                CREATED             STATUS              PORTS
eb495ad92ba0        demo/runit:latest   /usr/sbin/runsvdir-s   4 seconds ago       Up 3 seconds        5014-&amp;gt;514, 8080-&amp;gt;80, 9200-&amp;gt;9200   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However there aren&amp;rsquo;t any services running!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl localhost:8080
curl: (56) Recv failure: Connection reset by peer
$ curl localhost:9200
curl: (56) Recv failure: Connection reset by peer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can start the services with the following commands&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd service
$ ln -s ../sv/elasticsearch
$ ln -s ../sv/logstash
$ ln -s ../sv/kibana
cd ..
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now see the services are running, test the ports and send some data to logstash.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl localhost:8080      
&amp;lt;!DOCTYPE html&amp;gt;&amp;lt;!--[if IE 8]&amp;gt;&amp;lt;html class=&amp;quot;no-js lt-ie9&amp;quot; lang=&amp;quot;en&amp;quot;&amp;gt;&amp;lt;![endif]--&amp;gt;&amp;lt;!--[if gt IE 8]&amp;gt;&amp;lt;!--&amp;gt;&amp;lt;html class=&amp;quot;no-js&amp;quot; lang=&amp;quot;en&amp;quot;&amp;gt;
...
curl localhost:9200
{
  &amp;quot;ok&amp;quot; : true,
  &amp;quot;status&amp;quot; : 200,
...
$tail -100 /var/log/syslog | nc localhost 5014
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Stop a service ?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ rm service/elasticsearch
$ rm service/logstash
$ rm service/kibana
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;bonus-round-logs:dab52676160fc987cea1712f60e5a17c&#34;&gt;Bonus Round: Logs!&lt;/h2&gt;

&lt;p&gt;The beautify of doing this is that we&amp;rsquo;re actually logging the application output to a mounted volume.   This means we now have access to their logs from the host machine.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ tail opt/logstash/logs/current
$ tail opt/elasticsearch-0.90.5/logs/current
$ tail opt/kibana/logs/access.log
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;cleanup:dab52676160fc987cea1712f60e5a17c&#34;&gt;Cleanup&lt;/h2&gt;

&lt;p&gt;Unfortunately any files created inside the docker instance are owned by root ( an artifact of docker daemon running as root ).   If you&amp;rsquo;re in The following script will clean out any such files after you&amp;rsquo;ve stopped the docker container.&lt;/p&gt;

&lt;p&gt;It will delete any files/dirs inside your current directory that are owned by root.  Obviously it can be very dangerous to run &amp;hellip; so be careful where you run it from!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo find . -uid 0   -exec rm -rfv {} \;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Creating immutable servers with chef and docker.io</title>
      <link>http://tech.paulcz.net/2013/09/creating-immutable-servers-with-chef-and-docker-dot-io/</link>
      <pubDate>Sat, 07 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>http://tech.paulcz.net/2013/09/creating-immutable-servers-with-chef-and-docker-dot-io/</guid>
      <description>

&lt;p&gt;Building applications in a &lt;a href=&#34;http://docker.io&#34;&gt;docker.io&lt;/a&gt; Dockerfile is relatively simple,  but sometimes you want to just install the application exactly as you would normally via already built chef cookbooks.   Turns out this is actually pretty simple.&lt;/p&gt;

&lt;p&gt;The first thing you&amp;rsquo;ll need to do is build a container with chef-client and berkshelf installed.   You can grab the one I&amp;rsquo;ve built by running &lt;code&gt;docker pull paulczar/chef-solo&lt;/code&gt; or build one youself from a &lt;code&gt;Dockerfile&lt;/code&gt; that looks a little something like the following&amp;hellip;&lt;/p&gt;

&lt;h3 id=&#34;creating-a-docker-io-container-with-chef-and-berkshelf:8e5e7d2d9bff571e6328148c619ee8a0&#34;&gt;Creating a docker.io container with chef and berkshelf&lt;/h3&gt;

&lt;p&gt;``` ruby Dockerfile&lt;/p&gt;

&lt;h1 id=&#34;docker-version-0-5-3:8e5e7d2d9bff571e6328148c619ee8a0&#34;&gt;DOCKER-VERSION 0.5.3&lt;/h1&gt;

&lt;p&gt;FROM ubuntu:12.10
MAINTAINER Paul Czarkowski &amp;ldquo;paul@paulcz.net&amp;rdquo;&lt;/p&gt;

&lt;p&gt;RUN apt-get -y update
RUN apt-get -y install curl build-essential libxml2-dev libxslt-dev git
RUN curl -L &lt;a href=&#34;https://www.opscode.com/chef/install.sh&#34;&gt;https://www.opscode.com/chef/install.sh&lt;/a&gt; | bash
RUN echo &amp;ldquo;gem: &amp;ndash;no-ri &amp;ndash;no-rdoc&amp;rdquo; &amp;gt; ~/.gemrc
RUN /opt/chef/embedded/bin/gem install berkshelf&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
_you&#39;ll notice I&#39;m using the embedded chef ruby to install the berkshelf gem,  this is a handy shortcut to avoid messing around with random ruby versions from your distributions packaging._

run `$ docker build -t paulczar/chef-solo .` to build a usable docker container from the above `Dockerfile`.

### Using chef-solo and berkshelf to build an application in a docker.io container ###

My [example application](https://github.com/paulczar/docker-chef-solo) will install `Kibana3` to your docker container.   I&#39;ll step through how it works below.

#### Chef-Solo ####

To run `chef-solo` successfully we require two files.   `solo.rb` to set up file locations, and `solo.json&#39; to set up the json / run list required for your application.

``` ruby chef.rb
root = File.absolute_path(File.dirname(__FILE__))

file_cache_path root
cookbook_path root + &#39;/cookbooks&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;``` json chef.json
{
  &amp;ldquo;kibana&amp;rdquo;: {
    &amp;ldquo;webserver_listen&amp;rdquo;: &amp;ldquo;0.0.0.0&amp;rdquo;
  },
  &amp;ldquo;run_list&amp;rdquo;: [
    &amp;ldquo;recipe[kibana::default]&amp;rdquo;
  ]
}&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
#### Berkshelf ####

To run `berkshelf` we need to build a Berksfile which contains a list of all the chef cookbooks required for the applocation.   Berkshelf will download these cookbooks to a local directory which will be usable by chef-solo.

``` ruby Berksfile
site :opscode

cookbook &#39;build-essential&#39;
cookbook &#39;apache2&#39;
cookbook &#39;git&#39;
cookbook &#39;kibana&#39;, github: &#39;lusis/chef-kibana&#39;
cookbook &#39;nginx&#39; , github: &#39;opscode-cookbooks/nginx&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;You can see some of the cookbooks are being pulled from the opscode repository,  whereas others are being pulled directly from github.&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&#34;dockerfile:8e5e7d2d9bff571e6328148c619ee8a0&#34;&gt;Dockerfile&lt;/h4&gt;

&lt;p&gt;All that&amp;rsquo;s left now is to create a Dockerfile that will bring it all together.&lt;/p&gt;

&lt;p&gt;``` ruby Dockerfile&lt;/p&gt;

&lt;h1 id=&#34;docker-version-0-5-3-1:8e5e7d2d9bff571e6328148c619ee8a0&#34;&gt;DOCKER-VERSION 0.5.3&lt;/h1&gt;

&lt;p&gt;FROM paulczar/chef-client
MAINTAINER Paul Czarkowski &amp;ldquo;paul@paulcz.net&amp;rdquo;&lt;/p&gt;

&lt;p&gt;RUN apt-get -y update
ADD . /chef
RUN cd /chef &amp;amp;&amp;amp; /opt/chef/embedded/bin/berks install &amp;ndash;path /chef/cookbooks
RUN chef-solo -c /chef/solo.rb -j /chef/solo.json
RUN echo &amp;ldquo;daemon off;&amp;rdquo; &amp;gt;&amp;gt; /etc/nginx/nginx.conf&lt;/p&gt;

&lt;p&gt;CMD [&amp;ldquo;nginx&amp;rdquo;]
```&lt;/p&gt;

&lt;p&gt;Run &lt;code&gt;$ docker build -t demo/kibana3 .&lt;/code&gt; to build your application.&lt;/p&gt;

&lt;p&gt;It will add the local files ( &lt;code&gt;solo.rb&lt;/code&gt;, &lt;code&gt;solo.json&lt;/code&gt;, &lt;code&gt;Berksfile&lt;/code&gt; ) to /chef in the server and then call berkshelf to download the cookbooks and chef-solo to install your application.   Finally it will give &lt;code&gt;nginx&lt;/code&gt; a directive to run in the foreground so that we don&amp;rsquo;t have to do any sneaky prcess control to get it to work with the way &lt;code&gt;docker.io&lt;/code&gt; runs processes.&lt;/p&gt;

&lt;p&gt;To run the resultant &lt;code&gt;docker.io&lt;/code&gt; container you simply need to run &lt;code&gt;$ docker run -d -p 80 demo/kibana3&lt;/code&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Logstash &#43; Opscode Omnibus</title>
      <link>http://tech.paulcz.net/2013/05/logstash-plus-opscode-omnibus/</link>
      <pubDate>Mon, 06 May 2013 00:00:00 +0000</pubDate>
      
      <guid>http://tech.paulcz.net/2013/05/logstash-plus-opscode-omnibus/</guid>
      <description>&lt;p&gt;At &lt;a href=&#34;http://devopsdays.org/events/2013-austin/&#34;&gt;DevOps Days Austin&lt;/a&gt; &lt;a href=&#34;http://twitter.com/mattray&#34;&gt;@mattray&lt;/a&gt; did an Openspace session on &lt;a href=&#34;https://github.com/opscode/omnibus-ruby&#34;&gt;Omnibus&lt;/a&gt; which is a toolset based around the concept of installing an app and all of it&amp;rsquo;s prerequisites from source into a directory and then building a package ( either .deb or .rpm ) of that using &lt;a href=&#34;https://github.com/jordansissel/fpm&#34;&gt;fpm&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Having battled many times with OS Packages trying to get newer versions of Ruby, or Redis or other software installed and having to hunt down some random package repo or manually build from source this seems like an excellent idea.&lt;/p&gt;

&lt;p&gt;To learn the basics I decided to build an &lt;a href=&#34;https://github.com/paulczar/omnibus-fpm&#34;&gt;omnibus package for fpm&lt;/a&gt; which helped me work out the kinks and learn the basics.&lt;/p&gt;

&lt;p&gt;From there I moved onto something a little more ambitious&amp;hellip; &lt;a href=&#34;http://logstash.net/&#34;&gt;logstash&lt;/a&gt;, which is an awesome opensource project for log aggregation and searching.&lt;/p&gt;

&lt;p&gt;Using Omnibus I took the Logstash .jar file and bundled in Redis, Kibana, Kibana3(+NodeJS), RabbitMQ, Elasticsearch along with all of their depedencies into a big fat package which installs to /opt/logstash and includes init scripts and default configs for each.&lt;/p&gt;

&lt;p&gt;The Logstash Omnibus project can be found &lt;a href=&#34;https://github.com/paulczar/omnibus-logstash&#34;&gt;here&lt;/a&gt;.  I also uploaded the resultant packages for &lt;a href=&#34;https://s3-us-west-2.amazonaws.com/paulcz-packages/logstash-omnibus-1.1.10_amd64.deb&#34;&gt;Ubuntu 12.04&lt;/a&gt; and &lt;a href=&#34;https://s3-us-west-2.amazonaws.com/paulcz-packages/logstash-omnibus-1.1.10.el6.x86_64.rpm&#34;&gt;RHEL 6&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This gives us a really powerful platform to deploy logstash and all of its prequisites in a completely repeatable manner and not have to worry about the existing versions of Ruby, Java, etc.    It also gives a super simple testing platform where a new user to logstash can install logstash with a single &lt;code&gt;dpkg&lt;/code&gt; or &lt;code&gt;rpm&lt;/code&gt; command and immediately be able to push logs to it via syslog or redis.&lt;/p&gt;

&lt;p&gt;Read more about using and building the &lt;a href=&#34;https://github.com/paulczar/omnibus-logstash/blob/master/README.md&#34;&gt;Logstash Omnibus package here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Vagrant&#43;Puppet&#43;FPM=Amazeballs</title>
      <link>http://tech.paulcz.net/2013/04/vagrant-plus-puppet-plus-fpm-equals-amazeballs/</link>
      <pubDate>Sun, 07 Apr 2013 00:00:00 +0000</pubDate>
      
      <guid>http://tech.paulcz.net/2013/04/vagrant-plus-puppet-plus-fpm-equals-amazeballs/</guid>
      <description>&lt;p&gt;Lately I&amp;rsquo;ve been doing a lot of prototyping with &lt;a href=&#34;http://www.vagrantup.com/&#34;&gt;Vagrant&lt;/a&gt;, specifically for a couple of distinct activities:-&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;building puppet modules using &lt;a href=&#34;https://github.com/elasticdog/puppet-sandbox&#34;&gt;the excellent puppet sandbox&lt;/a&gt; project&lt;/li&gt;
&lt;li&gt;and building RPM packages with &lt;a href=&#34;https://github.com/jordansissel/fpm&#34;&gt;FPM&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I realized I was spending a bunch of time flipping back and forth between Vagrant environments and I had no quick way to utilize RPMs built with FPM inside my puppet modules.&lt;/p&gt;

&lt;p&gt;An idea was born.   I forked off the &lt;a href=&#34;https://github.com/paulczar/puppet-sandbox&#34;&gt;puppet sandbox&lt;/a&gt; project and added a Yum repo module &lt;code&gt;repository&lt;/code&gt; to the standalone puppet provisioner that vagrant uses when it first brings up a box.   It adds a Yum repo on the puppet server called &lt;code&gt;sandbox&lt;/code&gt; and adds a repo file to the client boxes pointing to the repo.   Now I can simply push an RPM to &lt;code&gt;packages/rpm&lt;/code&gt; and run &lt;code&gt;vagrant provision puppet&lt;/code&gt; which reruns puppet and rebuilds the yum repo.&lt;/p&gt;

&lt;p&gt;Given that I often flip back and forth between Ubuntu and CentOS boxes I also created &lt;code&gt;Vagrantfile.centos63&lt;/code&gt; and &lt;code&gt;Vagrantfile.precise64&lt;/code&gt; so I can swiftly destroy the existing environment and bring up another of a different flavour by simply symlinking &lt;code&gt;Vagrantfile&lt;/code&gt; to the appropriate file.&lt;/p&gt;

&lt;p&gt;This worked out pretty well for a while until I realized I was still jumping back and forth between vagrant environments and I realized I had another improvement to make.   So I then went on to create a definition in the puppet sandbox &lt;code&gt;Vagrantfile&lt;/code&gt; file for a &lt;code&gt;FPM server&lt;/code&gt; and a new module in the provisioner to install FPM on it.   Given that this module simply adds a few packages this module works for both CentOS and Ubuntu.&lt;/p&gt;

&lt;p&gt;I also created a couple of sample scripts to download source and build RPMs for both Redis and Elasticsearch which get pushed via the provisioner to &lt;code&gt;/tmp/redis-rpm.sh&lt;/code&gt; and &lt;code&gt;/tmp/elasticsearch-rpm.sh&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Now ( For CentOS boxes at least ) I can very quickly iterate on puppet modules and create RPM packages on the fly and have them instantly available.   The process is very simple and looks a little something like this :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/paulczar/puppet-sandbox
$ cd puppet-sandbox
$ vagrant up puppet fpm client1
$ vagrant ssh fpm
[vagrant@fpm ~]$ sudo /tmp/redis-rpm.sh
  ... 
  ... A bunch of scrolling text while files are downloaded and rpm is built
  ...
[vagrant@fpm ~]$ exit
$ vagrant provision puppet
$ vagrant ssh client1
[vagrant@client1 ~]$ sudo yum clean all
[vagrant@client1 ~]$ sudo yum -y install redis
[vagrant@client1 ~]$ sudo service redis-server start
[vagrant@client1 ~]$ redis-cli ping
PONG
[vagrant@client1 ~]$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If I&amp;rsquo;m building a puppet module that needs redis I can now add the following to it&amp;rsquo;s init.pp ( or more properly create a module for redis and request it from the module I&amp;rsquo;m building )&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  package { &#39;redis&#39;:
    ensure =&amp;gt; &#39;present&#39;;
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Of course Debian/Ubuntu doesn&amp;rsquo;t use Yum/RPM for package management.    I&amp;rsquo;d love to accept a pull request from somebody who wants to extend it to also support a local APT repository.   I left breadcrumbs in the &lt;code&gt;repository&lt;/code&gt; module for some appropriate classes to be spliced in&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Creating a Github Pages Blog With Octopress</title>
      <link>http://tech.paulcz.net/2012/12/creating-a-github-pages-blog-with-octopress/</link>
      <pubDate>Sat, 15 Dec 2012 00:00:00 +0000</pubDate>
      
      <guid>http://tech.paulcz.net/2012/12/creating-a-github-pages-blog-with-octopress/</guid>
      <description>

&lt;p&gt;A lot of tech bloggers will write their blog posts in &lt;a href=&#34;http://daringfireball.net/projects/markdown/&#34;&gt;Markdown&lt;/a&gt;, convert it to HTML and paste that HTML into their blog of choice and then in the blog&amp;rsquo;s editor clean it up to suit their blog.   This is an excellent way to create easy to read portable documents that can easily be published in multiple formats.&lt;/p&gt;

&lt;p&gt;However what if there was a way to skip the second part of that and just create a markdown page, submit it into your source control ( you &lt;em&gt;do&lt;/em&gt; use source control right? ) and your blog would automagically update.&lt;/p&gt;

&lt;p&gt;In comes &lt;a href=&#34;http://octopress.org/&#34;&gt;Octopress&lt;/a&gt;,  it&amp;rsquo;s a framework that wraps around &lt;a href=&#34;https://help.github.com/articles/using-jekyll-with-pages&#34;&gt;Jekyll&lt;/a&gt; which is &lt;a href=&#34;https://github.com/&#34;&gt;Github&amp;rsquo;s&lt;/a&gt; blogging engine that powers &lt;a href=&#34;http://pages.github.com/&#34;&gt;Github Pages&lt;/a&gt;.   Essentially you edit Markdown files and &lt;a href=&#34;http://octopress.org/&#34;&gt;Octopress&lt;/a&gt; will compile it into a static-html &lt;a href=&#34;https://help.github.com/articles/using-jekyll-with-pages&#34;&gt;Jekyll&lt;/a&gt; blog.     This means that your blog will be lightning fast ( no need to run an interpreted language in your web server ) and ultra portable.&lt;/p&gt;

&lt;p&gt;Another side benefit is that you can host it for free on &lt;a href=&#34;https://github.com/&#34;&gt;Github&lt;/a&gt; ( as long as you&amp;rsquo;re okay with sharing your source &amp;hellip; and you should be! ) or for free on &lt;a href=&#34;http://www.heroku.com/&#34;&gt;Heroku&lt;/a&gt; ( don&amp;rsquo;t have to share your source ) or host it on any simple no frills Apache, LightHTTP, nginx, node.js, etc server.&lt;/p&gt;

&lt;p&gt;Here is how I&amp;rsquo;m porting my blogger site to Octopress hosted on Github Pages.   I&amp;rsquo;m not using any of the fancy &lt;a href=&#34;https://github.com/mojombo/jekyll/wiki/blog-migrations&#34;&gt;Jekyll migration tools&lt;/a&gt; as I only have a few posts and it will help me get used to the extended syntax that Octopress uses in Markdown.&lt;/p&gt;

&lt;p&gt;As usual the first step is to install any dependencies.   These instructions are for &lt;a href=&#34;http://www.ubuntu.com/&#34;&gt;Ubuntu 12.10&lt;/a&gt; &amp;hellip; modify to suit your OS of choice.&lt;/p&gt;

&lt;p&gt;Most of these steps are taken directly from the &lt;a href=&#34;http://octopress.org/docs/&#34;&gt;Octopress Documentation&lt;/a&gt;,   I&amp;rsquo;m just condensing them into a single document to suit the exact scenario being described in this post.&lt;/p&gt;

&lt;h2 id=&#34;before-you-begin:221c62aff29cee9dbd022d15fc79996d&#34;&gt;Before You Begin&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Install Git&lt;/li&gt;
&lt;li&gt;Install Ruby 1.9.3 via your OS package management or &lt;a href=&#34;http://octopress.org/docs/setup/rbenv/&#34;&gt;rbenv&lt;/a&gt; or &lt;a href=&#34;http://octopress.org/docs/setup/rvm/&#34;&gt;RVM&lt;/a&gt;.&lt;br /&gt;
&lt;em&gt;If using package management may need to install ruby-dev&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Check your Ruby version is at least 1.9.3 and install bundler:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ruby --version 
sudo gem install bundler
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;initial-setup:221c62aff29cee9dbd022d15fc79996d&#34;&gt;Initial Setup&lt;/h2&gt;

&lt;p&gt;Clone the octopress repository and set it up&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone git://github.com/imathis/octopress.git octopress
cd octopress
bundle install

rake install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;rsquo;re going to use Github pages.   Octopress has some rake tasks to make this easier for you.    Your blog will be hosted at &lt;code&gt;http://username.github.com&lt;/code&gt; and you need to create a &lt;a href=&#34;https://github.com/repositories/new&#34;&gt;new Github repository&lt;/a&gt; called &lt;code&gt;username.github.com&lt;/code&gt; that github pages will use the master branch as the html source for your blog.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rake setup_github_pages
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This rake points our clone to the new repistory we just set up, configures your blog&amp;rsquo;s URL and sets up a master branch in the &lt;code&gt;_deploy&lt;/code&gt; directory for deployment.&lt;/p&gt;

&lt;p&gt;edit &lt;code&gt;_config.yml&lt;/code&gt; and fill in your blog name and other details.   There&amp;rsquo;s also some configs for twitter/G+/etc plugins that are worth configuring.&lt;/p&gt;

&lt;h2 id=&#34;write-some-blog-content:221c62aff29cee9dbd022d15fc79996d&#34;&gt;Write some blog content&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Great time to read &lt;a href=&#34;http://octopress.org/docs/blogging&#34;&gt;Blogging Basics&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Create an &lt;code&gt;About&lt;/code&gt; page and a &lt;code&gt;First Post!&lt;/code&gt; blog post:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rake new_page[&amp;quot;About&amp;quot;]
rake new_post[&amp;quot;First Post!&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Edit the Markdown pages that it creates for you with your preferred &lt;a href=&#34;http://sourceforge.net/p/retext/home/ReText/&#34;&gt;Markdown editor&lt;/a&gt;.   The output of the rake commands should provide appropriate hints as to the location of the created files.&lt;/p&gt;

&lt;p&gt;Generate and preview the blog:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rake generate
rake preview
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will generate the contents of your blog and allow you to preview it at [&lt;a href=&#34;http://localhost:4000&#34;&gt;http://localhost:4000&lt;/a&gt;].&lt;/p&gt;

&lt;p&gt;Once you&amp;rsquo;re happy with the contents we can deploy your blog for the first time.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rake deploy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will copy the generated files into &lt;code&gt;_deploy/&lt;/code&gt;, add them to git, commit and push them up to the master branch. In a few seconds you should get an email from Github telling you that your commit has been received and will be published on your site.   Being your first commit it could take 10 minutes for the blog to be available at [&lt;a href=&#34;http://username.github.com&#34;&gt;http://username.github.com&lt;/a&gt;]&lt;/p&gt;

&lt;p&gt;Don&amp;rsquo;t forget to commit your changes to the source branch:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git add .
git commit -m &#39;Added About page and first post!&#39;
git push origin source
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;want-to-edit-your-blog-from-another-machine-or-edit-an-existing-octopress-blog:221c62aff29cee9dbd022d15fc79996d&#34;&gt;Want to edit your blog from another machine,  or edit an existing octopress blog?&lt;/h2&gt;

&lt;p&gt;This is pretty simple ( assuming you have the prerequisites already install ).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;If you run Dropbox you can do this inside of your dropbox folders to make this instantly avaiable on any system you use.&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone git@github.com:username/username.github.com.git
cd username.github.com
git checkout source
mkdir _deploy
cd _deploy
git init
git remote add origin git@github.com:username/username.github.com.git
git pull origin master
cd ..
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;once this is done you can run &lt;code&gt;rake new_post[&amp;quot;title&amp;quot;]&lt;/code&gt; and all the other rake commands needed to edit/preview/publish your blog.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Running KVM and Openvswitch on Ubuntu 12.10</title>
      <link>http://tech.paulcz.net/2012/12/running-kvm-and-openvswitch-on-ubuntu-12-dot-10/</link>
      <pubDate>Thu, 13 Dec 2012 00:00:00 +0000</pubDate>
      
      <guid>http://tech.paulcz.net/2012/12/running-kvm-and-openvswitch-on-ubuntu-12-dot-10/</guid>
      <description>

&lt;p&gt;I&amp;rsquo;ve got an aging VMWare ESXi 4.0 server that needs to be replaced with something a little more modern and flexing.   Obviously at home I don&amp;rsquo;t need all the cool features that licensed VMWare comes with,  but I do want more than just the basic free version.&lt;/p&gt;

&lt;p&gt;After a few weeks of installing and testing alternatives  ( I&amp;rsquo;d really love to run openstack,  but it&amp;rsquo;s just not worth it at home for a single box ) I&amp;rsquo;ve settled on Ubuntu 12.10 server running KVM and Openvswitch.&lt;/p&gt;

&lt;p&gt;After installing Ubuntu 12.10 I did the following to get KVM up and running&amp;hellip;   I cribbed this mostly from &lt;a href=&#34;http://blog.allanglesit.com/2012/10/linux-kvm-ubuntu-12-10-with-openvswitch/&#34;&gt;blog.allanglesit.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Install updates and Pre-requisites&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;First of all, make sure Ubuntu is fully up to date:
&lt;em&gt;sudo to root as pretty much every command here needs to be run as root&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;{% codeblock lang:bash %}
sudo bash
apt-get update
apt-get upgrade
{% endcodeblock %}&lt;/p&gt;

&lt;p&gt;Now we can go ahead and install the necessary packages:&lt;/p&gt;

&lt;p&gt;{% codeblock lang:bash %}
apt-get -y install aptitude apt-show-versions ntp ntpdate vim kvm \
 libvirt-bin vlan virtinst virt-manager virt-viewer openssh-server \
 iperf pv openvswitch-controller openvswitch-brcompat \
 openvswitch-switch nfs-common
{% endcodeblock %}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kill off the default libvirt bridge and nuke ebtables&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We want to delete the default libvirt interface and we don&amp;rsquo;t need ebtables so we&amp;rsquo;ll get rid of that.&lt;/p&gt;

&lt;p&gt;{% codeblock lang:bash %}
virsh net-destroy default
virsh net-autostart &amp;ndash;disable default
service libvirt-bin stop
service qemu-kvm stop
aptitude purge -y ebtables
service openvswitch-switch restart
service openvswitch-controller restart
{% endcodeblock %}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Configure network interfaces&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll be using just a single interface which will be used for both the bridge and the host itself.    We will also be bridging that network into the the vswitch and then configuring an interface for the host OS.    The network configuration will look something like this:&lt;/p&gt;

&lt;p&gt;{% codeblock /etc/network/interfaces %}&lt;/p&gt;

&lt;h1 id=&#34;the-loopback-network-interface:888f7903415a760a26ea5348c53a988c&#34;&gt;The loopback network interface&lt;/h1&gt;

&lt;p&gt;auto lo
iface lo inet loopback&lt;/p&gt;

&lt;h1 id=&#34;the-primary-network-interface-bridge:888f7903415a760a26ea5348c53a988c&#34;&gt;The primary network interface - bridge!&lt;/h1&gt;

&lt;p&gt;auto eth0
iface eth0 inet manual
up ifconfig $IFACE 0.0.0.0 up
down ifconfig $IFACE down&lt;/p&gt;

&lt;h1 id=&#34;the-host-os-network-interface:888f7903415a760a26ea5348c53a988c&#34;&gt;The host OS network interface&lt;/h1&gt;

&lt;h1 id=&#34;dns-settings-here-12-10-resets-resolv-conf-on-reboot:888f7903415a760a26ea5348c53a988c&#34;&gt;DNS settings here,  12.10 resets resolv.conf on reboot.&lt;/h1&gt;

&lt;p&gt;auto ovsbr0p1
iface ovsbr0p1 inet static
address 192.168.50.10
netmask 255.255.255.0
gateway 192.168.50.1
dns-nameservers 192.168.50.1
dns-search example.com
{% endcodeblock %}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Configure the openvswitch network&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Now we need to configure the network on the openvswitch.     We need to define the bridge, connect it to the uplink interface and create a port for the host OS.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;note: if you&amp;rsquo;re doing this via SSH it will probably break your session&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;{% codeblock lang:bash %}
ovs-vsctl add-br ovsbr0
ovs-vsctl add-port ovsbr0 eth0
ovs-vsctl add-port ovsbr0 ovsbr0p1 &amp;ndash; set interface ovsbr0p1 type=internal
reboot
{% endcodeblock %}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Modify network service sleep times&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;That took forever to boot.    We can fix that by modifying sleeps in /etc/init/failsafe.conf and reboot again to make sure it helped.&lt;/p&gt;

&lt;p&gt;Change :&lt;/p&gt;

&lt;p&gt;{% codeblock %}
$PLYMOUTH message &amp;ndash;text=&amp;ldquo;Waiting for network configuration&amp;hellip;&amp;rdquo; || :
sleep 40
$PLYMOUTH message &amp;ndash;text=&amp;ldquo;Waiting up to 60 more seconds for network configuration&amp;hellip;&amp;rdquo; || :
sleep 59
$PLYMOUTH message &amp;ndash;text=&amp;ldquo;Booting system without full network configuration&amp;hellip;&amp;rdquo; || :
{% endcodeblock %}&lt;/p&gt;

&lt;p&gt;To :&lt;/p&gt;

&lt;p&gt;{% codeblock %}
$PLYMOUTH message &amp;ndash;text=&amp;ldquo;Waiting for network configuration&amp;hellip;&amp;rdquo; || :
sleep 1
$PLYMOUTH message &amp;ndash;text=&amp;ldquo;Waiting up to 60 more seconds for network configuration&amp;hellip;&amp;rdquo; || :
sleep 1
$PLYMOUTH message &amp;ndash;text=&amp;ldquo;Booting system without full network configuration&amp;hellip;&amp;rdquo; || :
{% endcodeblock %}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;LVM configure&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;re going to also use LVM to for the KVM virtual machines to use as storage.    I have a pair of 500g disks in a software raid1 which I&amp;rsquo;ll use for this.&lt;/p&gt;

&lt;p&gt;{% codeblock lang:bash %}
pvcreate /dev/md0
vgcreate data-disk /dev/md0
lvcreate -L 10G -n ISO data-disk
mkfs.ext4 /dev/data-disk/ISO
mkdir -p /data-disk/ISO
echo &amp;ldquo;/dev/data-disk/ISO /data-disk/ISO defaults    ext4    0 0&amp;rdquo; &amp;gt;&amp;gt; /etc/fstab
mount -a
{% endcodeblock %}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Create VM&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Now we can go ahead and create our first VM.   I&amp;rsquo;ve already downloaded the Ubuntu ISO to /data-disk/ISO&lt;/p&gt;

&lt;p&gt;&lt;em&gt;note: virt-install does not support setting a virtualport type of openvswitch yet .. so we have to trick it&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;{% codeblock lang:bash %}
lvcreate -L 8G -n VM-UbuntuTest data-disk
virt-install &amp;ndash;name UbuntuTest &amp;ndash;hvm &amp;ndash;noautoconsole &amp;ndash;ram 1024 \
&amp;ndash;disk path=/dev/data-disk/VM-UbuntuTest &amp;ndash;nonetworks &amp;ndash;vnc \
&amp;ndash;os-type=linux &amp;ndash;os-variant=ubuntuquantal \
&amp;ndash;cdrom /data-disk/ISO/ubuntu-12.10-server-amd64.iso
{% endcodeblock %}&lt;/p&gt;

&lt;p&gt;set up the networking by editing the VM&amp;rsquo;s XML and adding a network interface stanza just before the &amp;lt;/devices&amp;gt;.&lt;/p&gt;

&lt;p&gt;{% codeblock lang:bash %}
virsh edit UbuntuTest
{% endcodeblock %}&lt;/p&gt;

&lt;p&gt;{% codeblock lang:xml %}
&lt;interface type=&#39;bridge&#39;&gt;
 &lt;source bridge=&#39;ovsbr0&#39;/&gt;
 &lt;virtualport type=&#39;openvswitch&#39; /&gt;
 &lt;model type=&#39;virtio&#39;/&gt;
&lt;/interface&gt;
{% endcodeblock %}&lt;/p&gt;

&lt;p&gt;The VM will need to be reset to pick up the network change,  however that will cause it to drop the ISO mount.  We can either continue through with the OS install without networking or reset the VM and then re-attach the ISO as a CD.    I connected to it from my desktop using the VirtualMachineManager GUI to do that but you could use virsh commands if you want to stick to CLI.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>