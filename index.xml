<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Paul Czarkowski</title>
    <link>https://tech.paulcz.net/</link>
    <description>Recent content on Paul Czarkowski</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 13 May 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://tech.paulcz.net/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>How to broadcast to Twitch and Zoom with OBS</title>
      <link>https://tech.paulcz.net/blog/obs-broadcast-to-zoom-and-twitch/</link>
      <pubDate>Wed, 13 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/obs-broadcast-to-zoom-and-twitch/</guid>
      <description>

&lt;p&gt;If you prefer a Video Tutorial, you can watch me screencast it on youtube, otherwise read on below.&lt;/p&gt;

&lt;div class=&#34;embed video-player&#34;&gt;
&lt;iframe class=&#34;youtube-player&#34; type=&#34;text/html&#34; width=&#34;640&#34; height=&#34;385&#34; src=&#34;http://www.youtube.com/embed/0q05_AMWUug&#34; allowfullscreen frameborder=&#34;0&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;


&lt;p&gt;Recent world events (COVID-19) has completely turned the Developer Advocacy role on its head and many of us are scrambling to find new ways to effectively reach audiences remotely.&lt;/p&gt;

&lt;p&gt;Many of us have been reaching for tools like &lt;a href=&#34;https://zoom.us&#34;&gt;Zoom&lt;/a&gt; for smaller groups such as Meetups and streaming platforms like &lt;a href=&#34;https://twitch.tv&#34;&gt;Twitch&lt;/a&gt; / &lt;a href=&#34;https://youtube.com&#34;&gt;YouTube Live&lt;/a&gt; for reaching larger groups.&lt;/p&gt;

&lt;p&gt;Many of us are turning to &lt;a href=&#34;https://obsproject.com/&#34;&gt;Open Broadcaster Studio&lt;/a&gt; (OBS) to turn our PCs into virtual production studios capable of composing multiple artifacts such as windows, audio, and webcams into layered streams. The most basic use case for OBS is to provide the ability to display onscreen code or a terminal session with a webcam image overlayed in the corner&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./obs-basic.png&#34; alt=&#34;Example of OBS overlay scene&#34; /&gt;&lt;/p&gt;

&lt;p&gt;OBS can record to a high definition video file on your local machine as well as broadcast it out to Twitch, YouTube, or any other &lt;a href=&#34;https://en.wikipedia.org/wiki/Real_Time_Streaming_Protocol&#34;&gt;RTSP&lt;/a&gt; capable platform.&lt;/p&gt;

&lt;p&gt;It should come as no surprise that streaming high quality video from multiple sources can be quite taxing and it often makes sense to have &lt;strong&gt;OBS running on a separate computer&lt;/strong&gt; (refurbished Dell desktops are perfect for this) with a video capture device (like an Elgato HD60) capturing video from your primary machine.&lt;/p&gt;

&lt;p&gt;Using OBS its fairly simple to record and broadcast a presentation, or live coding session. However it feels more like a webinar than it does a meetup or conference presentation as the interaction is very one way.&lt;/p&gt;

&lt;p&gt;By hooking up Zoom and OBS bidirectionally you can have the reach of a streaming platform at the same time as the more interative nature of having real live people in the (virtual) room with you. Having even just a few people acting as an audience and being able to talk to them and have them talk back and see their body language and reactions makes all the difference.&lt;/p&gt;

&lt;p&gt;Unfortunately trying to share a screen and audio through multiple systems is complicated and isn&amp;rsquo;t possible straight out of the box and we need to solve a few problems:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Rebroadcast video from Zoom through to your stream&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Rebroadcast video from OBS through to Zoom&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Mix audio from multiple inputs through to multiple outputs (without causing feedback loops)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;1-rebroadcast-video-from-zoom-through-to-your-stream&#34;&gt;1. Rebroadcast video from Zoom through to your stream&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;video-zoom-obs.png&#34; alt=&#34;diagram showing zoom video in obs&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;multiple-monitors&#34;&gt;Multiple Monitors&lt;/h3&gt;

&lt;p&gt;If you have multiple monitors you can dedicate one of them to Zoom and add a &lt;strong&gt;Display Capture&lt;/strong&gt; source in OBS for that whole display and set zoom to fullscreen on that monitor.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;obs-display.png&#34; alt=&#34;OBS sharing a whole display&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: I was on a Zoom between two machines and only one camera, so pretend &lt;code&gt;Not Paul&lt;/code&gt; is a very handsome gentleman.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I&amp;rsquo;m not a fan of the layout of fullscreen zoom calls, and I prefer the flexibility of capturing it as a window, which also makes it possible to do this with a single monitor.&lt;/p&gt;

&lt;h3 id=&#34;single-monitor-window-capture&#34;&gt;Single Monitor / Window Capture&lt;/h3&gt;

&lt;p&gt;In OBS you can add a &lt;strong&gt;Window Capture&lt;/strong&gt; and select your Zoom meeting. This will capture just the Zoom window itself. You can adjust the window size to suit the resolution that you&amp;rsquo;re outputting to.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;obs-window.png&#34; alt=&#34;OBS sharing just the zoom window&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: The window capture settings are OS dependent, on Windows it would display the name and executable file for the window you want to share.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;One Caveat is that Zoom likes to switch to fullscreen when somebody shares their screen. Make sure to deselect the &lt;strong&gt;Enter full screen when a participant shares screen&lt;/strong&gt; in the &lt;strong&gt;Share Screen&lt;/strong&gt; Settings to disable this behavior.&lt;/p&gt;

&lt;p&gt;You might be tempted to try and share your screen to Zoom, however Zoom behaves painfully when you do that and you&amp;rsquo;ll most likely lose your capture of the Zoom window. Instead you should add the OBS virtual camera (see below) which lets you use the contents of your OBS scene as a camera in Zoom.&lt;/p&gt;

&lt;h2 id=&#34;2-rebroadcast-video-from-obs-through-to-zoom&#34;&gt;2. Rebroadcast video from OBS through to Zoom&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;video-zoom-obs-zoom.png&#34; alt=&#34;diagram showing zoom video in obs and back&#34; /&gt;&lt;/p&gt;

&lt;p&gt;OBS has a plugin that lets you create a virtual camera which you can use as your Zoom camera. The people in your Zoom call will see exactly the same video that you&amp;rsquo;re streaming without having to share your screen.&lt;/p&gt;

&lt;p&gt;You can download the virtual camera plugin from &lt;a href=&#34;https://obsproject.com/forum/resources/obs-virtualcam.949/&#34;&gt;here&lt;/a&gt; however it currently only supports Windows. (There&amp;rsquo;s a &lt;a href=&#34;https://github.com/CatxFish/obs-v4l2sink&#34;&gt;video4linux&lt;/a&gt; you can use if you&amp;rsquo;re on Linux) for the same effect.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;obs-camera-settings.png&#34; alt=&#34;OBS Virtual Camera&#34; /&gt;&lt;/p&gt;

&lt;p&gt;With the Virtual Camera plugin installed and enabled you can the select it as a camera in Zoom.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;obs-virtualcam-in-zoom.png&#34; alt=&#34;OBS virtual camera in zoom&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;You can see the remote camera now showing the host&amp;rsquo;s OBS scene of their webcam nested with their screen share.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;3-mix-audio-from-multiple-inputs-through-to-multiple-outputs&#34;&gt;3. Mix audio from multiple inputs through to multiple outputs&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Without causing feedback loops.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Setting up the video was relatively straight forward. Audio is the complicated beast. So far I have only solved this on Windows, but am quite certain you could solve it in Linux using the &lt;a href=&#34;https://jackaudio.org/&#34;&gt;JACK Audio Connection Kit&lt;/a&gt; (JACK).&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re running a Mac you should be able to use &lt;a href=&#34;https://rogueamoeba.com/loopback/&#34;&gt;Rogue Amoeba&amp;rsquo;s Loopback app&lt;/a&gt; to perform similar (and maybe even more advanced) audio filtering. Thanks &lt;a href=&#34;https://twitter.com/mattstratton&#34;&gt;Matty Stratton&lt;/a&gt; for the tip.&lt;/p&gt;

&lt;p&gt;Basically we need to be able to selectively route certain inputs to certain outputs. For instance we want zoom audio to go into OBS, and we want OBS audio to go into Zoom, but we don&amp;rsquo;t want them to loop eachothers audio back and create a feedback loop.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;zoom-obs-audio.png&#34; alt=&#34;diagram showing audio wiring&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In order to solve this, you need a Mixer that can mix three discreet inputs (Zoom, Desktop, Mic) to three discreet outputs (Speakers, Zoom, OBS).&lt;/p&gt;

&lt;p&gt;You also need virtual audio devices that can act as patch cables.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s likely a number of different audio solutions out there to solve this, I&amp;rsquo;ll show you what I found works really well and while not free, is extremely affordable.&lt;/p&gt;

&lt;p&gt;This is where &lt;a href=&#34;https://www.vb-audio.com/&#34;&gt;VB Audio&lt;/a&gt; comes in. It&amp;rsquo;s a website with an interesting assortment of audio software for Windows, most of which is either Free or Donationware (read the licenses carefully before using it in a commercial setting).&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.vb-audio.com/&#34;&gt;VB Audio&lt;/a&gt; offers &lt;a href=&#34;https://www.vb-audio.com/Cable/index.htm&#34;&gt;virtual cables&lt;/a&gt; that we can use to patch the sound correctly. They have up to 5 cables that you can purchase via Donation. In order to wire up our sound we&amp;rsquo;ll use &lt;strong&gt;VB-CABLE A+B&lt;/strong&gt; and &lt;strong&gt;VB-CABLE C+D&lt;/strong&gt; which gives us 4 virtual cables.&lt;/p&gt;

&lt;p&gt;All audio sent to VB-CABLE inputs is sent to the correspnding VB_CABLE output.&lt;/p&gt;

&lt;p&gt;Pay for, download, and install both bundles.&lt;/p&gt;

&lt;p&gt;You will still need a Mixer capable of mixing the inputs correctly. Thankfully &lt;a href=&#34;https://www.vb-audio.com/&#34;&gt;VB Audio&lt;/a&gt; also provides &lt;a href=&#34;https://www.vb-audio.com/Voicemeeter/index.htm&#34;&gt;VoiceMeeter&lt;/a&gt;. It comes in three flavors, &lt;a href=&#34;https://www.vb-audio.com/Voicemeeter/index.htm&#34;&gt;VoiceMeeter&lt;/a&gt;, &lt;a href=&#34;https://www.vb-audio.com/Voicemeeter/banana.htm&#34;&gt;VoiceMeeter Banana&lt;/a&gt;, and &lt;a href=&#34;https://www.vb-audio.com/Voicemeeter/potato.htm&#34;&gt;VoiceMeeter Potato&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The regular VoiceMeeter does not have enough inputs and outputs, so you&amp;rsquo;ll want to use either Banana or Potato. I used Banana as it has exactly the right number of inputs.&lt;/p&gt;

&lt;p&gt;Pay for (Donate), download and install &lt;a href=&#34;https://www.vb-audio.com/Voicemeeter/banana.htm&#34;&gt;VoiceMeeter Banana&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now you have the Mixer and the Virtual Cables we can hook everything up together.&lt;/p&gt;

&lt;h3 id=&#34;configure-voicemeeter-banana&#34;&gt;Configure VoiceMeeter Banana&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;voicemeeter-banana.png&#34; alt=&#34;screenshot of voicemeeter banana&#34; /&gt;&lt;/p&gt;

&lt;p&gt;First open up VoiceMeeter Banana. You&amp;rsquo;ll see there are Three Hardware Inputs and Three Hardware outputs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hardware Inputs:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;You can rename the Inputs by right clicking on their names, and you can select their devices underneath.&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Rename &lt;strong&gt;Hardware input 1&lt;/strong&gt; to &lt;strong&gt;Zoom&lt;/strong&gt; and set it to &lt;strong&gt;WDB: CABLE-B Output&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Rename &lt;strong&gt;Hardware input 2&lt;/strong&gt; to &lt;strong&gt;Mic&lt;/strong&gt; and set it to your microphone device. Mine is &lt;strong&gt;Microphone (Yeti)&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Rename &lt;strong&gt;Hardware input 3&lt;/strong&gt; to &lt;strong&gt;Desktop&lt;/strong&gt;and set it to &lt;strong&gt;WDB: CABLE-D Output&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Hardware Outputs:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;*Set the outputs by clicking the A1, A2, A3 dropdowns next to where it says &lt;strong&gt;HARDWARE OUT&lt;/strong&gt;*&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A1 -&amp;gt; &lt;strong&gt;Speakers / Headphones&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;A2 -&amp;gt; &lt;strong&gt;WDM: Cable-A Input&lt;/strong&gt; (Zoom)&lt;/li&gt;
&lt;li&gt;A3 -&amp;gt; &lt;strong&gt;WDM: Cable-C Input&lt;/strong&gt; (OBS)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Mixing:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Under the Hardware Inputs you can see a Volume selector and a sound activity bar. Beside that you can see options, the outputs to send to as well as Mute.&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Zoom -&amp;gt; A1 (Speakers), A3 (OBS)&lt;/li&gt;
&lt;li&gt;Mic -&amp;gt; A1 (Speakers), A2 (Zoom), A3 (OBS)&lt;/li&gt;
&lt;li&gt;Desktop -&amp;gt; A1 (Speakers), A2 (Zoom), A3 (OBS)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;*I usually set the &lt;strong&gt;Desktop&lt;/strong&gt; audio volume to about &lt;code&gt;-33&lt;/code&gt; so that random noises don&amp;rsquo;t drown out people speaking.&lt;/p&gt;

&lt;h3 id=&#34;configure-desktop-audio&#34;&gt;Configure Desktop Audio&lt;/h3&gt;

&lt;p&gt;In your status bar select the &lt;strong&gt;Speaker&lt;/strong&gt; Icon and configure it to output to &lt;strong&gt;Cable-D Input&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;desktop-audio.png&#34; alt=&#34;screenshot of desktop audio&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;configure-zoom-audio&#34;&gt;Configure Zoom audio&lt;/h3&gt;

&lt;p&gt;In Zoom, go to &lt;strong&gt;Audio Settings&lt;/strong&gt;. Set &lt;strong&gt;Speaker&lt;/strong&gt; to &lt;strong&gt;Cable-B Input&lt;/strong&gt; and set &lt;strong&gt;Microphone&lt;/strong&gt; to &lt;strong&gt;Cable-A Output&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;zoom-audio-settings.png&#34; alt=&#34;screenshot of zoom audio settings&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;configure-obs-settings&#34;&gt;Configure OBS settings&lt;/h3&gt;

&lt;p&gt;in OBS under &lt;strong&gt;Settings&lt;/strong&gt;-&amp;gt;&lt;strong&gt;Audio&lt;/strong&gt; Disable all Audio devices. Then set &lt;strong&gt;Mic/Auxillary Input&lt;/strong&gt; to &lt;strong&gt;CABLE-C Output&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;obs-audio-settings.png&#34; alt=&#34;screenshot of OBS audio settings&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Then in the OBS &lt;strong&gt;Audio Mixer&lt;/strong&gt; rename the &lt;strong&gt;Mic/Auxillary Input&lt;/strong&gt; to &lt;strong&gt;VoiceMeeter&lt;/strong&gt; and mute all other devices such as webcams.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;obs-audio-mixer.png&#34; alt=&#34;screenshot of OBS mixer&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;test-it-out&#34;&gt;Test it out&lt;/h3&gt;

&lt;p&gt;You should now have everything wired up correctly and just need to test it out.&lt;/p&gt;

&lt;p&gt;When you configured the Mic input in VoiceMeeter you chose to send it to all three outputs. This means that you should hear your Mic input through your speakers/headphones.&lt;/p&gt;

&lt;p&gt;If you don&amp;rsquo;t like to listen to yourself you can deselect &lt;strong&gt;A1&lt;/strong&gt; for the &lt;strong&gt;Mic&lt;/strong&gt; input. This is an easy toggle to see if the audio is working.&lt;/p&gt;

&lt;p&gt;You can also use the volume meters in the various apps to confirm they match. For instance when you speak in the Microphone you should see the &lt;strong&gt;Mic&lt;/strong&gt; meter, and the &lt;strong&gt;A1&lt;/strong&gt;, &lt;strong&gt;A2&lt;/strong&gt;, &lt;strong&gt;A3&lt;/strong&gt; meter move at the same time. You should also see activity in the OBS mixer and Zoom.&lt;/p&gt;

&lt;p&gt;You should hit &lt;strong&gt;Start Recording&lt;/strong&gt; in OBS and get on a zoom with a friend (or a second computer) and verify sound is working in both directions, and that all of the audio ends up in the OBS recording.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Assuming you didn&amp;rsquo;t hit any issues following this guide you now have your system configured to Stream to both Twitch (or YouTube) and Zoom at the same time, mixing their audio and video appropriately.&lt;/p&gt;

&lt;p&gt;This gives you a powerful way to not just record content, but to stream it and share it with both intimate groups on Zoom and Larger audiences on Twitch.&lt;/p&gt;

&lt;p&gt;One of the best parts of this setup is how I can easily host someone else on my twitch stream and have them share their desktop and do demos, live coding etc.&lt;/p&gt;

&lt;p&gt;I can also run workshops through zoom and people on my twitch stream can also follow along.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Streaming from Zoom to Twitch (or YouTube)</title>
      <link>https://tech.paulcz.net/blog/streaming-from-zoom-to-twitch/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/streaming-from-zoom-to-twitch/</guid>
      <description>

&lt;p&gt;Zoom meetings are great for a small number of people, and if you&amp;rsquo;re willing to buy a Webinar license you can also use it for a full on webinar (although there are some interesting restrictions to webinars, like you can&amp;rsquo;t have breakout rooms).&lt;/p&gt;

&lt;p&gt;Twitch is a great streaming platform, originally used for streaming video games, its now more commonly being used to stream live coding and another technology related things that involve sharing your screen.&lt;/p&gt;

&lt;p&gt;Traditionally you&amp;rsquo;d use software called &lt;a href=&#34;https://obsproject.com/&#34;&gt;Open Broadcaster Software (OBS)&lt;/a&gt; which you can compose multiple video/audio inputs into a single video that can be streamed to Twitch. This is how folks will compose their xbox, a chat screen and a webcam together in their stream.&lt;/p&gt;

&lt;p&gt;However OBS is quite CPU hungry (I think a GPU might help) and by the time you run OBS on a standard machine its tough to run anything else without starving OBS of resources and causing stuttering on the stream.&lt;/p&gt;

&lt;p&gt;Its made worse if you want to have multiple people on the stream and you end up running something like Zoom which allows you all to chat, and one or more of you to share your screen via zoom, then you just push Zoom itself through OBS.&lt;/p&gt;

&lt;p&gt;But if you&amp;rsquo;re already streaming video via Zoom, wouldn&amp;rsquo;t it be great if you could just use Zoom to stream directly to Twitch and cutout the middle man?  Turns out you can!&lt;/p&gt;

&lt;h2 id=&#34;setting-up-zoom-to-stream-to-twitch&#34;&gt;Setting up Zoom to stream to Twitch&lt;/h2&gt;

&lt;p&gt;Preamble aside, its fairly easy, but completely unintuitive to stream Zoom to Twitch. So lets talk it through. First of all you do need a paid [Pro, Business, Education, or Enterprise] account, hopefully you have that through your workplace.&lt;/p&gt;

&lt;p&gt;Log into your organization&amp;rsquo;s Zoom, and hit the &lt;strong&gt;Settings&lt;/strong&gt; button on the left hand menu. Scroll down to &lt;strong&gt;Advanced settings&lt;/strong&gt; and turn &lt;strong&gt;Allow live streaming meetings&lt;/strong&gt; on. Check either the &lt;strong&gt;YouTube&lt;/strong&gt; or the &lt;strong&gt;Custom Live Streaming Service&lt;/strong&gt;, the latter which we&amp;rsquo;ll configure for Twitch.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;zoom-settings.png&#34; alt=&#34;Zoom Settings Page&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Since I&amp;rsquo;m bad at remembering things, I throw the following in the instructions text box so I remember when setting up the meeting how to do it.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;stream url: rtmp://live.twitch.tv/app
stream key: &amp;lt;your twitch account stream key&amp;gt;
live streaming url: https://twitch.tv/accountname
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then just hit Save. You&amp;rsquo;re now ready to stream. You can either set up the actual stream live in a Zoom meeting, or you can Schedule a meeting and configure it there. Let&amp;rsquo;s take a look at the latter.&lt;/p&gt;

&lt;h2 id=&#34;schedule-a-zoom-meeting-with-twitch-streaming&#34;&gt;Schedule a Zoom meeting with Twitch Streaming&lt;/h2&gt;

&lt;p&gt;In your Zoom account on the zoom website click &lt;strong&gt;Schedule a meeting&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Set a name, the start time, duration etc and click &lt;strong&gt;Save&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;This will drop you to the settings page for the meeting you just scheduled and on the bottom of that page is the &lt;strong&gt;Live Streaming&lt;/strong&gt; configuration setting. Click &lt;strong&gt;configure live stream settings&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;schedule-a-meeting.png&#34; alt=&#34;schedule a meeting&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Here you enter the details following the instructions you conveniently placed in the Stream settings:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;You can find the twitch key at the following URL &lt;a href=&#34;https://dashboard.twitch.tv/u/&#34;&gt;https://dashboard.twitch.tv/u/&lt;/a&gt;&lt;username&gt;/settings/channel&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;configure-live-stream.png&#34; alt=&#34;configure live stream&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Hit Save. Now when you join the Meeting it will automatically configure the Live Stream, but you&amp;rsquo;ll still need to start the Stream by clicking the &lt;strong&gt;&amp;hellip; More&lt;/strong&gt; button and select the streaming option from there.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;If you didn&amp;rsquo;t configure live streaming for the meeting, you can actually configure it here as long as you&amp;rsquo;ve enabled streaming in Settings&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;from-in-zoom-meeting.png&#34; alt=&#34;configure live stream in meeting&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Zoom will redirect you to a browser and configure the stream before redirecting it to your twitch streaming page.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;start-stream.png&#34; alt=&#34;starting stream&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can close this twitch streaming window once its started or leave it open. If you leave it open turn off its sound or you&amp;rsquo;ll create some fun echo loopbacks.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s the Twitch view:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;twitch-streaming.png&#34; alt=&#34;twitch stream&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s the Zoom View:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;zoom-streaming.png&#34; alt=&#34;zoom stream&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Creating Self Signed Certificates on Kubernetes</title>
      <link>https://tech.paulcz.net/blog/creating-self-signed-certs-on-kubernetes/</link>
      <pubDate>Sat, 22 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/creating-self-signed-certs-on-kubernetes/</guid>
      <description>

&lt;p&gt;Welcome to 2020. Creating self signed TLS certificates is still hard. Five (5) years ago I created a project on github called &lt;a href=&#34;https://github.com/paulczar/omgwtfssl&#34;&gt;omgwtfssl&lt;/a&gt; which is a fairly simple bash script wrapping a bunch of &lt;code&gt;openssl&lt;/code&gt; commands to create certificates.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve been using it ever since and kind of forgot about the pain of creating certificates.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Skip the words and jump to the examples &lt;a href=&#34;#creating-self-signed-certificates-with-cert-manager&#34;&gt;Creating self signed certificates with cert-manager&lt;/a&gt;, &lt;a href=&#34;#creating-multiple-certificates-from-the-same-self-signed-ca-with-cert-manager&#34;&gt;Creating multiple certificates from the same self signed CA with cert-manager&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;With the advent of &lt;a href=&#34;https://letsencrypt.org/&#34;&gt;letsencrypt&lt;/a&gt; and later the Kubernetes &lt;a href=&#34;https://cert-manager.io/&#34;&gt;cert-manager&lt;/a&gt; controller we can make real signed certificates with a quick flourish of some &lt;strong&gt;YAML&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve been happily chugging along with this combination of &lt;code&gt;cert-manager&lt;/code&gt; &lt;a href=&#34;https://cert-manager.io/&#34;&gt;cert-manager&lt;/a&gt; for real certificates, and &lt;a href=&#34;https://github.com/paulczar/omgwtfssl&#34;&gt;omgwtfssl&lt;/a&gt; for self signed (despite the fact that the name is &lt;a href=&#34;https://gitlab.com/gitlab-org/charts/gitlab/issues/584&#34;&gt;inappropriate and unprofessional.&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;We should try to find a replacement for omgwtfssl, which is currently used to generate self-signed certificates. The name is inappropriate and unprofessional.&amp;rdquo;  - &lt;a href=&#34;https://gitlab.com/gitlab-org/charts/gitlab/issues/584&#34;&gt;gitlab&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;As amusing as &lt;code&gt;docker run paulczar/omgwtfssl&lt;/code&gt; is to type (I giggle every time), its a bit weird to tell people to create certificates locally then add them to their Kubernetes manifests or Helm charts. So I finally decided to sit down and figure out how to create them sensibly with &lt;a href=&#34;https://cert-manager.io/&#34;&gt;cert-manager&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;create-a-kubernetes-in-docker-cluster&#34;&gt;Create a Kubernetes in Docker Cluster&lt;/h2&gt;

&lt;p&gt;You&amp;rsquo;ll need a Kubernetes cluster, we&amp;rsquo;re not doing anything too resource intensive so a &lt;a href=&#34;https://kind.sigs.k8s.io/docs/user/quick-start/&#34;&gt;kind&lt;/a&gt; cluster should be fine.&lt;/p&gt;

&lt;p&gt;Create &lt;a href=&#34;https://kind.sigs.k8s.io/docs/user/quick-start/&#34;&gt;kind&lt;/a&gt; cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kind create cluster
export KUBECONFIG=&amp;quot;$(kind get kubeconfig-path --name=&amp;quot;kind&amp;quot;)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Test the cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl cluster-info
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;creating-self-signed-certificates-with-cert-manager&#34;&gt;Creating self signed certificates with cert-manager&lt;/h2&gt;

&lt;p&gt;Install &lt;a href=&#34;https://cert-manager.io/&#34;&gt;cert-manager&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl create namespace cert-manager
kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.13.1/cert-manager.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;If you receive a validation error relating to the x-kubernetes-preserve-unknown-fields add &lt;code&gt;--validate&lt;/code&gt; to the above command and run again.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Create a namespace to work in:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl create namespace sandbox
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create an Issuer:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: you can create a ClusterIssuer instead if you want to be able to request certificates from any namespace.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl apply -n sandbox -f &amp;lt;(echo &amp;quot;
apiVersion: cert-manager.io/v1alpha2
kind: Issuer
metadata:
  name: selfsigned-issuer
spec:
  selfSigned: {}
&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a self signed certificate:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This creates a wildcard certificate that could be used for
  any services in the sandbox namespace.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl apply -n sandbox -f &amp;lt;(echo &#39;
apiVersion: cert-manager.io/v1alpha2
kind: Certificate
metadata:
  name: first-tls
spec:
  secretName: first-tls
  dnsNames:
  - &amp;quot;*.sandbox.svc.cluster.local&amp;quot;
  - &amp;quot;*.sandbox&amp;quot;
  issuerRef:
    name: selfsigned-issuer
&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Validate the secret is created&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Check the certificate resource:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl -n sandbox get certificate
  NAME        READY   SECRET      AGE
  first-tls   True    first-tls   9s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check the subsequent secret:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl -n sandbox get secret first-tls
NAME        TYPE                DATA   AGE
first-tls   kubernetes.io/tls   3      73s
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;This secret contains three keys &lt;code&gt;ca.crt&lt;/code&gt;, &lt;code&gt;tls.crt&lt;/code&gt;, &lt;code&gt;tls.key&lt;/code&gt;. You can run &lt;code&gt;kubectl -n sandbox get secret first-tls -o yaml&lt;/code&gt; to see the whole thing.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Test that the certificate is valid:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;openssl x509 -in &amp;lt;(kubectl -n sandbox get secret \
  first-tls -o jsonpath=&#39;{.data.tls\.crt}&#39; | base64 -d) \
  -text -noout
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;If you scan through the output you should find &lt;code&gt;X509v3 Subject Alternative Name: DNS:*.first.svc.cluster.local, DNS:*.first&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Congratulations. You&amp;rsquo;ve just created your first self signed certificate with Kubernetes. While it involves more typing than &lt;code&gt;docker run paulczar/omgwtfssl&lt;/code&gt; it is much more useful for Kubernetes enthusiasts to have the cluster generate them for you.&lt;/p&gt;

&lt;p&gt;However, what if you want to use TLS certificates signed by the same CA for performing client/server authentication? Never fear we can do that too.&lt;/p&gt;

&lt;h2 id=&#34;creating-multiple-certificates-from-the-same-self-signed-ca-with-cert-manager&#34;&gt;Creating multiple certificates from the same self signed CA with cert-manager&lt;/h2&gt;

&lt;p&gt;Install &lt;a href=&#34;https://cert-manager.io/&#34;&gt;cert-manager&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Skip this step if you already installed cert-manager from the first example.&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl create namespace cert-manager
kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.13.1/cert-manager.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;If you receive a validation error relating to the x-kubernetes-preserve-unknown-fields add &lt;code&gt;--validate&lt;/code&gt; to the above command and run again.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Create a namespace to work in:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl create namespace sandbox2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create an Issuer:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: you can create a ClusterIssuer instead if you want to be able to request certificates from any namespace.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl apply -n sandbox2 -f &amp;lt;(echo &amp;quot;
apiVersion: cert-manager.io/v1alpha2
kind: Issuer
metadata:
  name: selfsigned-issuer
spec:
  selfSigned: {}
&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a CA Certificate:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;note &lt;code&gt;isCA&lt;/code&gt; is set to true in the body of the &lt;code&gt;spec&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl apply -n sandbox2 -f &amp;lt;(echo &#39;
apiVersion: cert-manager.io/v1alpha2
kind: Certificate
metadata:
  name: sandbox2-ca
spec:
  secretName: sandbox2-ca-tls
  commonName: sandbox2.svc.cluster.local
  usages:
    - server auth
    - client auth
  isCA: true
  issuerRef:
    name: selfsigned-issuer
&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check the certificate and secret were created:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl -n sandbox2 get certificate sandbox2-ca
NAME          READY   SECRET            AGE
sandbox2-ca   True    sandbox2-ca-tls   15s

$ kubectl -n sandbox2 get secret sandbox2-ca-tls
NAME              TYPE                DATA   AGE
sandbox2-ca-tls   kubernetes.io/tls   3      22s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a second Issuer using the secret name from the &lt;code&gt;sandbox2-ca&lt;/code&gt; secret:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;In order to sign multiple certificates from the same CA we need to create an Issuer resource from secret created by the CA.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl apply -n sandbox2 -f &amp;lt;(echo &#39;
apiVersion: cert-manager.io/v1alpha2
kind: Issuer
metadata:
  name: sandbox2-ca-issuer
spec:
  ca:
    secretName: sandbox2-ca-tls&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a TLS Certificate from the new CA Issuer:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;We can add &lt;code&gt;usages&lt;/code&gt; to the certificate &lt;code&gt;spec&lt;/code&gt; to ensure that the certificates can be used for client/server authentication.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl apply -n sandbox2 -f &amp;lt;(echo &#39;
apiVersion: cert-manager.io/v1alpha2
kind: Certificate
metadata:
  name: sandbox2-server
spec:
  secretName: sandbox2-server-tls
  isCA: false
  usages:
    - server auth
    - client auth
  dnsNames:
  - &amp;quot;server.sandbox2.svc.cluster.local&amp;quot;
  - &amp;quot;server&amp;quot;
  issuerRef:
    name: sandbox2-ca-issuer
&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a second TLS Certificate from the new CA Issuer:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl apply -n sandbox2 -f &amp;lt;(echo &#39;
apiVersion: cert-manager.io/v1alpha2
kind: Certificate
metadata:
  name: sandbox2-client
spec:
  secretName: sandbox2-client-tls
  isCA: false
  usages:
    - server auth
    - client auth
  dnsNames:
  - &amp;quot;client.sandbox2.svc.cluster.local&amp;quot;
  - &amp;quot;client&amp;quot;
  issuerRef:
    name: sandbox2-ca-issuer
&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check that all three certificates are created:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl -n sandbox2 get certificate
NAME              READY   SECRET                AGE
sandbox2-ca       True    sandbox2-ca-tls       7m34s
sandbox2-client   True    sandbox2-client-tls   7s
sandbox2-server   True    sandbox2-server-tls   16s

$ kubectl -n sandbox2 get secret
NAME                  TYPE                                  DATA   AGE
sandbox2-ca-tls       kubernetes.io/tls                     3      8m14s
sandbox2-client-tls   kubernetes.io/tls                     3      48s
sandbox2-server-tls   kubernetes.io/tls                     3      57s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Validate the certificates against the CA:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ openssl verify -CAfile \
&amp;lt;(kubectl -n sandbox2 get secret sandbox2-ca-tls \
  -o jsonpath=&#39;{.data.ca\.crt}&#39; | base64 -d) \
&amp;lt;(kubectl -n sandbox2 get secret sandbox2-server-tls \
  -o jsonpath=&#39;{.data.tls\.crt}&#39; | base64 -d)
/proc/self/fd/18: OK

$ openssl verify -CAfile \
&amp;lt;(kubectl -n sandbox2 get secret sandbox2-ca-tls \
  -o jsonpath=&#39;{.data.ca\.crt}&#39; | base64 -d) \
&amp;lt;(kubectl -n sandbox2 get secret sandbox2-client-tls \
  -o jsonpath=&#39;{.data.tls\.crt}&#39; | base64 -d)
/proc/self/fd/18: OK
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Validate the Client / Server authentication&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Run an &lt;code&gt;openssl&lt;/code&gt; server as a background process:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;touch test.txt

openssl s_server \
  -cert &amp;lt;(kubectl -n sandbox2 get secret sandbox2-server-tls -o jsonpath=&#39;{.data.tls\.crt}&#39; | base64 -d) \
  -key &amp;lt;(kubectl -n sandbox2 get secret sandbox2-server-tls -o jsonpath=&#39;{.data.tls\.key}&#39; | base64 -d) \
  -CAfile &amp;lt;(kubectl -n sandbox2 get secret sandbox2-ca-tls -o jsonpath=&#39;{.data.ca\.crt}&#39; | base64 -d) \
  -WWW -port 12345  \
  -verify_return_error -Verify 1 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Run an &lt;code&gt;openssl&lt;/code&gt; client test:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;look for &lt;code&gt;HTTP/1.0 200 ok&lt;/code&gt; in the client output.&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;echo -e &#39;GET /test.txt HTTP/1.1\r\n\r\n&#39; | \
  openssl s_client \
  -cert &amp;lt;(kubectl -n sandbox2 get secret sandbox2-client-tls -o jsonpath=&#39;{.data.tls\.crt}&#39; | base64 -d) \
  -key &amp;lt;(kubectl -n sandbox2 get secret sandbox2-client-tls -o jsonpath=&#39;{.data.tls\.key}&#39; | base64 -d) \
  -CAfile &amp;lt;(kubectl -n sandbox2 get secret sandbox2-client-tls -o jsonpath=&#39;{.data.ca\.crt}&#39; | base64 -d) \
  -connect localhost:12345 -quiet
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;stop the background process:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kill %1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Congratulations, you&amp;rsquo;ve now created a pair of certificates signed by the same CA that can be used for client/server authentication.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Creating self signed certificates is now officially easy. You can use &lt;a href=&#34;https://github.com/paulczar/omgwtfssl&#34;&gt;omgwtfssl&lt;/a&gt; locally, or &lt;a href=&#34;https://cert-manager.io/&#34;&gt;cert-manager&lt;/a&gt; in your Kubernetes cluster. Either way you get cheap and easy self signed certificates for testing. Obviously you should use real certificates in production, in which case you would still be able to use &lt;a href=&#34;https://cert-manager.io/&#34;&gt;cert-manager&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to GitHub Actions</title>
      <link>https://tech.paulcz.net/blog/intro-to-github-actions/</link>
      <pubDate>Thu, 29 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/intro-to-github-actions/</guid>
      <description>

&lt;p&gt;After what seems like an &lt;a href=&#34;https://github.blog/2018-10-17-action-demos/&#34;&gt;eternity&lt;/a&gt; I finally got added to the &lt;a href=&#34;https://github.com/features/actions/&#34;&gt;GitHub Actions&lt;/a&gt; beta a few days ago.&lt;/p&gt;

&lt;p&gt;I recently created a &lt;a href=&#34;https://gohugo.io&#34;&gt;Hugo&lt;/a&gt; website for learning Kubernetes called &lt;a href=&#34;https://k8s.camp&#34;&gt;k8s.camp&lt;/a&gt; which is hosted in GitHub Pages and I figured that switching it from &lt;a href=&#34;https://circleci.com&#34;&gt;CircleCI&lt;/a&gt; to GitHub Actions would be a great way to learn Actions.&lt;/p&gt;

&lt;p&gt;Before I get started I do want to mention that my experience as a new user trying to learn how actions work was sub-optimal.&lt;/p&gt;

&lt;p&gt;My previous automation for &lt;a href=&#34;https://k8s.camp&#34;&gt;k8s.camp&lt;/a&gt; was using CircleCI which was very intuitive, I can&amp;rsquo;t say the same for GitHub Actions.&lt;/p&gt;

&lt;h2 id=&#34;no-good-plan-survives-first-contact&#34;&gt;No good plan survives first contact&lt;/h2&gt;

&lt;p&gt;Obviously the first thing I did was google &amp;ldquo;github actions documentation&amp;rdquo; and the first hit was &lt;a href=&#34;https://developer.github.com/actions/&#34;&gt;developer.github.com/actions&lt;/a&gt; which I hastily clicked on to be greeted with the following:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./doc-move.png&#34; alt=&#34;doc move deprecate&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Which suggested I was off to a rocky start. I clicked through to the &lt;a href=&#34;https://help.github.com/en/categories/automating-your-workflow-with-github-actions&#34;&gt;&amp;ldquo;Automating your workflow with GitHub Actions&amp;rdquo;&lt;/a&gt; page.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Pretty much any time I googled a GitHub Actions question I was sent to the deprecated documentation with the &lt;code&gt;HCL&lt;/code&gt; syntax rather than &lt;code&gt;yaml&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Most of the existing examples are still in &lt;code&gt;HCL&lt;/code&gt;, both in the documentation and out in the wild.&lt;/p&gt;

&lt;p&gt;On top of this the examples in the documentation are quite trite and don&amp;rsquo;t really help do anything useful. It&amp;rsquo;s not even really clear how to structure the workflows and actions in the repo, so I had to fumble my way through it.&lt;/p&gt;

&lt;p&gt;After a bunch of reading documentation and finding incomplete examples and blog posts I managed to slowly grok my way through it.&lt;/p&gt;

&lt;p&gt;Hopefully this blog post will help others have a better first experience.&lt;/p&gt;

&lt;h2 id=&#34;workflows-and-actions&#34;&gt;Workflows and Actions&lt;/h2&gt;

&lt;p&gt;My understanding through trial and error is there are two things you need for GitHub Actions, Workflows and Actions.&lt;/p&gt;

&lt;h3 id=&#34;workflows&#34;&gt;Workflows&lt;/h3&gt;

&lt;p&gt;Workflows are pretty much what you&amp;rsquo;d expect them to be. Files that describe the steps to be taken when an event is triggered.&lt;/p&gt;

&lt;p&gt;These are stored in &lt;code&gt;.github/workflows&lt;/code&gt; in your git repository and look something like this (in fact below is the &lt;a href=&#34;https://github.com/paulczar/k8s-camp/blob/master/.github/workflows/build-and-deploy.yml&#34;&gt;current workflow&lt;/a&gt; for k8s.camp):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;name: Build and Deploy to GitHub Pages
on:
  push:
    branches: [master]
jobs:
  build:
    name: build-and-deploy
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v1
        with:
          fetch-depth: 1
      - uses: ./.github/actions/spell-check
      - uses: ./.github/actions/publish-gh-pages
        env:
          PUSH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: GitHub Actions act like GitHub Apps which means it gets it&amp;rsquo;s own &lt;code&gt;GITHUB_TOKEN&lt;/code&gt; secret which can be pulled into &lt;em&gt;environment variables&lt;/em&gt; for the Actions. In fact there&amp;rsquo;s a ton of default &lt;em&gt;environment variables&lt;/em&gt; passed into the action such as the repository name and git commit sha.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You can see you define the type of activities to trigger the workflow (in this case a push to the &lt;code&gt;master&lt;/code&gt; branch) and a set of steps to run. These steps call out to actions.&lt;/p&gt;

&lt;p&gt;You can have multiple workflows based on different GitHub activities and even chain them together to create even more complex workflows.&lt;/p&gt;

&lt;h2 id=&#34;actions&#34;&gt;Actions&lt;/h2&gt;

&lt;p&gt;Actions are also pretty self explanatory, they&amp;rsquo;re the discrete units of work required to fulfill each step in the workflow.&lt;/p&gt;

&lt;p&gt;Actions can be local to the repo, or they can be called from another repository.&lt;/p&gt;

&lt;p&gt;For example the above workflow calls a remote checkout action &lt;code&gt;uses: actions/checkout@v1&lt;/code&gt; which literally looks out to the &lt;code&gt;v1&lt;/code&gt; release of the github repo &lt;code&gt;actions/checkout&lt;/code&gt; &lt;a href=&#34;https://github.com/actions/checkout/releases/tag/v1.0.0&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Actions are run inside Docker containers, and thus an Action is usually a &lt;code&gt;Dockerfile&lt;/code&gt; combined with a script to run inside the container. Remote actions should contain an &lt;code&gt;action.yaml&lt;/code&gt; to define the action and its interactions.&lt;/p&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;The first step in learning how to use github actions is to learn how to build and run Docker images the wrong way.&lt;/p&gt;&amp;mdash; Czarknado 🦈🌪️ (@pczarkowski) &lt;a href=&#34;https://twitter.com/pczarkowski/status/1167437284410626054?ref_src=twsrc%5Etfw&#34;&gt;August 30, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;You can vendor your actions into your repo (which I prefer to do, especially compared to pulling in some other random person&amp;rsquo;s action blindly) in which case I prefer to place them in the &lt;code&gt;.github/actions&lt;/code&gt; directory to keep things clean.&lt;/p&gt;

&lt;h2 id=&#34;the-three-steps&#34;&gt;The three steps&lt;/h2&gt;

&lt;p&gt;There are three steps defined in my workflow file above. These will be run in order by the action, if one fails the entire action will halt.&lt;/p&gt;

&lt;h3 id=&#34;step-1-checkout&#34;&gt;Step 1 - checkout&lt;/h3&gt;

&lt;p&gt;The first step is using a github provided action to check out the source at the correct commit.&lt;/p&gt;

&lt;h3 id=&#34;step-2-spellcheck&#34;&gt;Step 2 - spellcheck&lt;/h3&gt;

&lt;p&gt;The second step is running a spellcheck across all of my markdown files.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;As I mentioned an action runs in a Docker container, so an Action is generally defined by a &lt;code&gt;Dockerfile&lt;/code&gt; to build the image, and an &lt;code&gt;ENTRYPOINT&lt;/code&gt; script to run the action inside the image.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;in the k8s.camp repo is the following local action &lt;code&gt;./.github/actions/spell-check&lt;/code&gt; which consists of:&lt;/p&gt;

&lt;h4 id=&#34;dockerfile&#34;&gt;Dockerfile&lt;/h4&gt;

&lt;p&gt;This is a fairly basic Dockerfile that simply starts from a small &lt;code&gt;nodejs&lt;/code&gt; image and adds in my entrypoint script.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: the &lt;code&gt;LABEL&lt;/code&gt;s are important to github actions and are documented &lt;a href=&#34;https://developer.github.com/actions/creating-github-actions/creating-a-docker-container/#label&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-dockerfile&#34;&gt;FROM node:lts-alpine

LABEL &amp;quot;name&amp;quot;=&amp;quot;Markdown Spell Checker&amp;quot;
LABEL &amp;quot;maintainer&amp;quot;=&amp;quot;Paul Czarkowski&amp;quot;
LABEL &amp;quot;version&amp;quot;=&amp;quot;0.0.1&amp;quot;

LABEL &amp;quot;com.github.actions.name&amp;quot;=&amp;quot;Markdown Spell Checker&amp;quot;
LABEL &amp;quot;com.github.actions.description&amp;quot;=&amp;quot;Markdown Spell Checker&amp;quot;
LABEL &amp;quot;com.github.actions.icon&amp;quot;=&amp;quot;package&amp;quot;
LABEL &amp;quot;com.github.actions.color&amp;quot;=&amp;quot;green&amp;quot;

COPY entrypoint.sh /entrypoint.sh

ENTRYPOINT [&amp;quot;/entrypoint.sh&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;entrypoint-sh&#34;&gt;entrypoint.sh&lt;/h4&gt;

&lt;p&gt;The script is really simple and uses &lt;code&gt;npm&lt;/code&gt; to install &lt;code&gt;markdown-spellcheck&lt;/code&gt; locally and then runs it.&lt;/p&gt;

&lt;p&gt;The exit code of &lt;code&gt;markdown-spellcheck&lt;/code&gt; will be used to determine if the step passed or failed.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/sh

cd $GITHUB_WORKSPACE

npm install markdown-spellcheck

./node_modules/markdown-spellcheck/bin/mdspell -r -n -a --en-us &#39;content/**/*.md&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;step-3-publish-to-gh-pages&#34;&gt;Step 3 - Publish to gh-pages&lt;/h3&gt;

&lt;p&gt;The third step will run &lt;code&gt;hugo&lt;/code&gt; to generate the &lt;code&gt;html&lt;/code&gt; for the website and then pushing the results to the &lt;code&gt;gh-pages&lt;/code&gt; branch.&lt;/p&gt;

&lt;h4 id=&#34;dockerfile-1&#34;&gt;Dockerfile&lt;/h4&gt;

&lt;p&gt;Here we have a slightly more complicated &lt;code&gt;Dockerfile&lt;/code&gt; that starts with Ubuntu and installs &lt;code&gt;curl&lt;/code&gt;, &lt;code&gt;git&lt;/code&gt;, and &lt;code&gt;hugo&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-dockerfile&#34;&gt;FROM ubuntu:bionic

LABEL &amp;quot;name&amp;quot;=&amp;quot;Publish k8s.camp&amp;quot;
LABEL &amp;quot;maintainer&amp;quot;=&amp;quot;Paul Czarkowski&amp;quot;
LABEL &amp;quot;version&amp;quot;=&amp;quot;0.0.1&amp;quot;

LABEL &amp;quot;com.github.actions.name&amp;quot;=&amp;quot;Publish k8s.camp&amp;quot;
LABEL &amp;quot;com.github.actions.description&amp;quot;=&amp;quot;Publish k8s.camp via gh-pages&amp;quot;
LABEL &amp;quot;com.github.actions.icon&amp;quot;=&amp;quot;package&amp;quot;
LABEL &amp;quot;com.github.actions.color&amp;quot;=&amp;quot;green&amp;quot;

RUN apt-get update &amp;gt; /dev/null &amp;amp;&amp;amp; apt-get -yqq install curl git &amp;gt; /dev/null

RUN curl -sSL \
    https://github.com/gohugoio/hugo/releases/download/v0.56.3/hugo_0.56.3_Linux-64bit.tar.gz | \
    tar xzf - hugo &amp;amp;&amp;amp; \
    chmod +x /hugo &amp;amp;&amp;amp; \
    /hugo version

COPY entrypoint.sh /entrypoint.sh

ENTRYPOINT [&amp;quot;/entrypoint.sh&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;entrypoint-sh-1&#34;&gt;entrypoint.sh&lt;/h4&gt;

&lt;p&gt;The entrypoint script configures git, checks out the &lt;code&gt;gh-pages&lt;/code&gt; branch, and generates the html into that branch before pushing it back up to git using the &lt;code&gt;GITHUB_TOKEN&lt;/code&gt; which is passed to it as the environment variable &lt;code&gt;PUSH_TOKEN&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash

set -o pipefail

echo &amp;quot;--&amp;gt; Configure git client&amp;quot;

git config --global user.email &amp;quot;username.taken@gmail.com&amp;quot;
git config --global user.name &amp;quot;Hugo Publisher&amp;quot;

echo &amp;quot;--&amp;gt; check out gh-pages&amp;quot;
git worktree add -B gh-pages public origin/gh-pages

echo &amp;quot;--&amp;gt; hugo publish&amp;quot;
cd $GITHUB_WORKSPACE
/hugo

echo &amp;quot;--&amp;gt; push gh-pages&amp;quot;
if [[ -z &amp;quot;$PUSH_TOKEN&amp;quot; ]]; then
  echo &amp;quot;No push token provided, skipping publish&amp;quot;
else
  cd public
  git add --all &amp;amp;&amp;amp; \
  git commit -m &amp;quot;Github Action Build ${GITHUB_SHA} `date +&#39;%Y-%m-%d %H:%M:%S&#39;`&amp;quot; --allow-empty &amp;amp;&amp;amp; \
  git remote set-url origin https://${GITHUB_ACTOR}:${PUSH_TOKEN}@github.com/${GITHUB_REPOSITORY}
  git push origin gh-pages:gh-pages
fi
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: I am using &lt;code&gt;git remote set-url&lt;/code&gt; to update the git repo URL to include the &lt;code&gt;PUSH_TOKEN&lt;/code&gt; which gives the action &lt;code&gt;push&lt;/code&gt; access to the repository.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;end-result&#34;&gt;End Result&lt;/h2&gt;

&lt;p&gt;Pushing this new &lt;code&gt;.github&lt;/code&gt; directory structure defining a workflow and a set of steps up to my git repo is all I needed to set up the Actions, and the workflow triggered from this very first push.&lt;/p&gt;

&lt;p&gt;Like all &lt;code&gt;ci&lt;/code&gt; tools I had a dozen or so commits to stabilize the actual test scripts and workflow, but now all I have to do to publish a new version of the &lt;a href=&#34;https://k8s.camp&#34;&gt;k8s.camp&lt;/a&gt; website is to merge a commit into the &lt;code&gt;master&lt;/code&gt; branch of the repo.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./actions2.png&#34; alt=&#34;screenshot of github actions&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Once you get through the initial hump of learning how GitHub actions work it becomes fairly straightforward to configure and use actions.&lt;/p&gt;

&lt;p&gt;For the sake of sanity I have kept the workflow for &lt;a href=&#34;https://k8s.camp&#34;&gt;k8s.camp&lt;/a&gt; very simple, but I can see how you could quickly build out a fairly complex set of workflows to perform your CI/CD tasks.&lt;/p&gt;

&lt;p&gt;While I kept the Actions in the local repository, I can see a very interesting versatility in having remote actions. I can see myself creating a central Actions github repo with all of my actions which I can then link back to from my various projects.&lt;/p&gt;

&lt;p&gt;If you have a basic hugo setup publishing to &lt;code&gt;gh-pages&lt;/code&gt; feel free to clone down the &lt;a href=&#34;https://github.com/paulczar/k8s-camp&#34;&gt;paulczar/k8s-camp&lt;/a&gt; repo and re-use them in your own.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Creating a Helm Chart Repository - Part 1</title>
      <link>https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-1/</link>
      <pubDate>Wed, 03 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-1/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;Note: If this topic has peaked your interest, you can join me for a Webinar on August 15 where I&amp;rsquo;ll dive deep into &lt;a href=&#34;https://content.pivotal.io/webinars/aug-15-cloud-native-operations-with-kubernetes-and-ci-cd-webinar?utm_campaign=cno-k8s-ci-cd-q319&amp;amp;utm_source=blog&amp;amp;utm_medium=website&#34;&gt;Cloud Native Operations with Kubernetes and CI/CD Pipelines&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Welcome to a three part blog series on Creating a &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Chart Repository. In &lt;strong&gt;part 1&lt;/strong&gt; I will demonstrate creating a &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; chart repository using &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; and &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Pages. In &lt;a href=&#34;https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-2&#34;&gt;part 2&lt;/a&gt; I will add Automation to automatically update the repository, and in &lt;a href=&#34;https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-3&#34;&gt;part 3&lt;/a&gt; I will add testing for changes to the charts themselves.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If you&amp;rsquo;re into Videos, I walked JJ through starting with Helm from scratch all the way to creating a Helm Repo and CI/CD.  &lt;div class=&#34;embed video-player&#34;&gt;
&lt;iframe class=&#34;youtube-player&#34; type=&#34;text/html&#34; width=&#34;640&#34; height=&#34;385&#34; src=&#34;http://www.youtube.com/embed/xn63krHJNKI&#34; allowfullscreen frameborder=&#34;0&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Helm is the defacto tool for packaging, sharing, and running Kubernetes Manifests. I&amp;rsquo;m going to assume you know the basics of &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; and have used it before. There&amp;rsquo;s plenty of great introductory topics around.&lt;/p&gt;

&lt;p&gt;You can host and share &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Charts (packages) via a &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Repository which is effectively a static website with an &lt;code&gt;index.yaml&lt;/code&gt; providing metadata and links to the &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Packages.&lt;/p&gt;

&lt;p&gt;This makes hosting a repository perfectly suited to running in &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Pages, s3, google cloud storage, etc. I like to use &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Pages as it allows your source code and repo to live effectively in the same place.&lt;/p&gt;

&lt;p&gt;I will walk you through creating a new &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Project hosting multiple &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; charts and demonstrate how to set up Continuous Integration with CircleCI to automatically test and publish new changes to your &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Charts.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: While I would usually use &lt;a href=&#34;https://concourse-ci.org/&#34;&gt;Concourse CI&lt;/a&gt; for my CI workflows, I wanted to &lt;em&gt;only&lt;/em&gt; use managed services and I chose Circle as that is already commonly used in the Helm community.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;create-a-new-github-https-github-com-repository&#34;&gt;Create a new &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Repository&lt;/h2&gt;

&lt;p&gt;Log into &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; and &lt;a href=&#34;https://github.com/new&#34;&gt;create a new repository&lt;/a&gt; called &lt;code&gt;my-helm-charts&lt;/code&gt;. I chose to have &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; create it as with an Apache2 License.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./github-new-repo.png&#34; alt=&#34;Creating new repo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can now clone down this repository and get to work:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone git@github.com:paulczar/my-helm-charts.git
Cloning into &#39;my-helm-charts&#39;...
remote: Enumerating objects: 4, done.
remote: Counting objects: 100% (4/4), done.
remote: Compressing objects: 100% (3/3), done.
remote: Total 4 (delta 0), reused 0 (delta 0), pack-reused 0
Receiving objects: 100% (4/4), 4.52 KiB | 4.52 MiB/s, done.

$ cd my-helm-charts

$ tree
.
├── LICENSE
└── README.md

0 directories, 2 files
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can see we have a default LICENSE file and a default README.md, we can leave them alone for now. Your next step is to create a couple of &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; charts. Since this is purely for demonstration purposes they don&amp;rsquo;t have to be overly functional charts which means we can just use the default boilerplate created by &lt;code&gt;helm create&lt;/code&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;You do have &lt;a href=&#34;https://github.com/helm/helm#install&#34;&gt;helm&lt;/a&gt; installed right?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ mkdir charts

$ [Helm](https://helm.sh) create charts/app1
Creating charts/app1

$ [Helm](https://helm.sh) create charts/app2
Creating charts/app2

$ tree
.
├── charts
│   ├── app1
│   │   ├── charts
│   │   ├── Chart.yaml
│   │   ├── templates
│   │   │   ├── deployment.yaml
│   │   │   ├── _helpers.tpl
│   │   │   ├── ingress.yaml
│   │   │   ├── NOTES.txt
│   │   │   ├── service.yaml
│   │   │   └── tests
│   │   │       └── test-connection.yaml
│   │   └── values.yaml
│   └── app2
│       ├── charts
│       ├── Chart.yaml
│       ├── templates
│       │   ├── deployment.yaml
│       │   ├── _helpers.tpl
│       │   ├── ingress.yaml
│       │   ├── NOTES.txt
│       │   ├── service.yaml
│       │   └── tests
│       │       └── test-connection.yaml
│       └── values.yaml
├── LICENSE
└── README.md

9 directories, 18 files
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Push these changes to git:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;echo &amp;quot;.deploy&amp;quot; &amp;gt;&amp;gt; .gitignore

$ git add .

$ git commit -m &#39;Initial Commit&#39;
[master 2172141] Initial Commit
 18 files changed, 524 insertions(+)
...
...

$ git push origin master
Enumerating objects: 27, done.
Counting objects: 100% (27/27), done.
Delta compression using up to 4 threads
Compressing objects: 100% (24/24), done.
Writing objects: 100% (26/26), 4.72 KiB | 536.00 KiB/s, done.
Total 26 (delta 8), reused 0 (delta 0)
remote: Resolving deltas: 100% (8/8), done.
To github.com:paulczar/my-helm-charts.git
   abdcced..2172141  master -&amp;gt; master
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;publish-your-helm-https-helm-sh-repository&#34;&gt;Publish your &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Repository&lt;/h2&gt;

&lt;h3 id=&#34;prepare-github-https-github-com-pages&#34;&gt;Prepare &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Pages&lt;/h3&gt;

&lt;p&gt;We&amp;rsquo;re going to use a combination of &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Pages and releases to host our &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Repository. Therefore we need to ensure we have &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Pages enabled on the git repo and to create an empty &lt;code&gt;gh-pages&lt;/code&gt; branch.&lt;/p&gt;

&lt;p&gt;You can create an empty &lt;code&gt;gh-pages&lt;/code&gt; branch by creating an orphan branch like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git checkout --orphan gh-pages
Switched to a new branch &#39;gh-pages&#39;

$ rm -rf charts

$ git add . --all

$ git commit -m &#39;initial gh-pages&#39;
git commit -m &#39;initial gh-pages&#39;
[gh-pages a9ce382] initial gh-pages
 18 files changed, 524 deletions(-)
...
...

$ git push origin gh-pages
Enumerating objects: 3, done.
...
...
To github.com:paulczar/my-helm-charts.git
 * [new branch]      gh-pages -&amp;gt; gh-pages
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next check that &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Pages is enabled by clicking on your git repo settings in GitHub:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./github-pages.png&#34; alt=&#34;github repo settings&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: I have a custom domain set up, your URL will probably be username.github.io/my-helm-charts.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;After a few minutes you should have a default rendering on your README.md at the provided URL:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./default-gh-pages.png&#34; alt=&#34;default [GitHub](https://github.com) ages&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;introducing-chart-releaser&#34;&gt;Introducing chart-releaser&lt;/h3&gt;

&lt;p&gt;You could use a combination of &lt;code&gt;helm package&lt;/code&gt; and &lt;code&gt;helm repo&lt;/code&gt; commands to construct your &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; repository by hand, or you can simplify your life by using &lt;code&gt;chart-releaser&lt;/code&gt; which will not only create your packages, but will upload them as binaries into an appropriately versioned &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Release.&lt;/p&gt;

&lt;p&gt;Download chart-releaser for your architecture [here].&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: I&amp;rsquo;m doing this on a linux machine, so you may need to update the commands below for Mac OS.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In a new terminal download and unpackage it, moving it to an executable path:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd /tmp
$ curl -sSL https://github.com/helm/chart-releaser/releases/download/v0.2.1/chart-releaser_0.2.1_linux_amd64.tar.gz | tar xzf -

$ mv cr ~/bin/cr

$ cr help
Create [Helm](https://helm.sh) chart repositories on [GitHub](https://github.com) Pages by uploading Chart packages
and Chart metadata to [GitHub](https://github.com) Releases and creating a suitable index file

Usage:
  cr [command]

Available Commands:
  help        Help about any command
  index       Update Helm repo index.yaml for the given GitHub repo
  upload      Upload Helm chart packages to GitHub Releases
  version     Print version information

Flags:
      --config string   Config file (default is $HOME/.cr.yaml)
  -h, --help            help for cr

Use &amp;quot;cr [command] --help&amp;quot; for more information about a command.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are two commands we care about &lt;code&gt;cr index&lt;/code&gt; and &lt;code&gt;cr upload&lt;/code&gt;, the first will create an appropriate &lt;code&gt;index.yaml&lt;/code&gt; and the second will upload the packages to &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Releases. In order to do the latter you&amp;rsquo;ll need to pass it in a &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Token so that it can use the &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; APIs.&lt;/p&gt;

&lt;p&gt;In your browser go to your &lt;a href=&#34;https://github.com/settings/tokens&#34;&gt;github developer settings&lt;/a&gt; and create a new personal access token.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./personal-access-token.png&#34; alt=&#34;create personal access token&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Create an environment variable (or a &lt;code&gt;~/.cr.yaml&lt;/code&gt; config file) containing the access token:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Pro-tip: put an additional space in your command right before &lt;code&gt;export&lt;/code&gt; and it won&amp;rsquo;t be saved to your command history.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$  export CH_TOKEN=c4a4ed6ab91a246572b0c46c19c630ccadc1049
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;create-and-upload-helm-https-helm-sh-packages&#34;&gt;Create and Upload &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Packages&lt;/h3&gt;

&lt;p&gt;Your next step is to create and upload the packages:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ Helm package charts/{app1,app2} --destination .deploy
Successfully packaged chart and saved it to: .deploy/app1-0.1.0.tgz
Successfully packaged chart and saved it to: .deploy/app2-0.1.0.tgz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Run &lt;code&gt;cr upload&lt;/code&gt; to create releases and upload the packages, note if it runs correctly there&amp;rsquo;s no output.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cr upload -o paulczar -r my-helm-charts -p .deploy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check your &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; repository now has a releases page with two releases:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./github-releases&#34; alt=&#34;github releases page&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;create-and-upload-index-file-to-github-https-github-com-pages&#34;&gt;Create and upload index file to &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Pages&lt;/h3&gt;

&lt;p&gt;Checkout your &lt;code&gt;gh-pages&lt;/code&gt; branch and run &lt;code&gt;cr index&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git checkout gh-pages

$ cr index -i ./index.yaml -p .deploy --owner paulczar --repo my-helm-charts
====&amp;gt; UpdateIndexFile new index at ./index.yaml
====&amp;gt; Found app1-0.1.0.tgz
====&amp;gt; Extracting chart metadata from .deploy/app1-0.1.0.tgz
====&amp;gt; Calculating Hash for .deploy/app1-0.1.0.tgz
====&amp;gt; Found app2-0.1.0.tgz
====&amp;gt; Extracting chart metadata from .deploy/app2-0.1.0.tgz
====&amp;gt; Calculating Hash for .deploy/app2-0.1.0.tgz
--&amp;gt; Updating index ./index.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There should now be a &lt;code&gt;index.yaml&lt;/code&gt; file containing the details of your &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; packages and the path to their archive:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cat index.yaml
apiVersion: v1
entries:
  app1:
  - apiVersion: v1
    appVersion: &amp;quot;1.0&amp;quot;
    created: &amp;quot;2019-07-03T13:03:06.139332963-05:00&amp;quot;
    description: A Helm chart for Kubernetes
    digest: 48cf831b72febeac2860a0be372094250aea68a9c76147c028085c8802dd48ec
    name: app1
    urls:
    - https://github.com/paulczar/my-helm-charts/releases/download/app1-0.1.0/app1-0.1.0.tgz
    version: 0.1.0
  app2:
  - apiVersion: v1
    appVersion: &amp;quot;1.0&amp;quot;
    created: &amp;quot;2019-07-03T13:03:07.301308677-05:00&amp;quot;
    description: A Helm chart for Kubernetes
    digest: 64b00fc4804aba524201f64e78ee22ad8e61d0923424f8e24e8b70befed88141
    name: app2
    urls:
    - https://github.com/paulczar/my-helm-charts/releases/download/app2-0.1.0/app2-0.1.0.tgz
    version: 0.1.0
generated: &amp;quot;2019-07-03T13:03:05.685803874-05:00&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Commit this to git and then wait a few minutes and check that it exists in your &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Pages url:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git add index.yaml

$ git commit -m &#39;release 0.1.0&#39;
[gh-pages 696df18] release 0.1.0
 1 file changed, 23 insertions(+)
 create mode 100644 index.yaml

$ git push origin gh-pages
...
To github.com:paulczar/my-helm-charts.git
   75f1fe8..696df18  gh-pages -&amp;gt; gh-pages
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check it exists in &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Pages:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;github-pages-index-yaml.png&#34; alt=&#34;github pages index.yaml&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;test-your-new-helm-https-helm-sh-repostiory&#34;&gt;Test your new &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Repostiory&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: To do this you&amp;rsquo;ll need a Kubernetes cluster with Helm&amp;rsquo;s tiller installed, but you already know how to do that right?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;$ helm repo add my-helm-charts https://tech.paulcz.net/my-helm-charts
&amp;quot;my-helm-charts&amp;quot; has been added to your repositories

$ helm install --name test --namespace test my-helm-charts/app1
NAME:   test
LAST DEPLOYED: Wed Jul  3 13:17:32 2019
NAMESPACE: test
STATUS: DEPLOYED

RESOURCES:
==&amp;gt; v1/Deployment
NAME       READY  UP-TO-DATE  AVAILABLE  AGE
test-app1  0/1    1           0          0s

==&amp;gt; v1/Pod(related)
NAME                        READY  STATUS             RESTARTS  AGE
test-app1-7b575d95f6-zhlh2  0/1    ContainerCreating  0         0s

==&amp;gt; v1/Service
NAME       TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)  AGE
test-app1  ClusterIP  10.100.200.213  &amp;lt;none&amp;gt;       80/TCP   0s


NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace test -l &amp;quot;app.kubernetes.io/name=app1,app.kubernetes.io/instance=test&amp;quot; -o jsonpath=&amp;quot;{.items[0].metadata.name}&amp;quot;)
  echo &amp;quot;Visit http://127.0.0.1:8080 to use your application&amp;quot;
  kubectl port-forward $POD_NAME 8080:80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check that it deployed ok:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl -n test get all
NAME                             READY   STATUS    RESTARTS   AGE
pod/test-app1-7b575d95f6-zhlh2   1/1     Running   0          42m

NAME                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
service/test-app1   ClusterIP   10.100.200.213   &amp;lt;none&amp;gt;        80/TCP    42m

NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/test-app1   1/1     1            1           42m

NAME                                   DESIRED   CURRENT   READY   AGE
replicaset.apps/test-app1-7b575d95f6   1         1         1       42m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Clean up:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm delete --purge test
release &amp;quot;test&amp;quot; deleted
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;update-the-readme-md-with-instructions-on-using&#34;&gt;Update the README.md with instructions on using&lt;/h3&gt;

&lt;p&gt;switch back to your &lt;code&gt;master&lt;/code&gt; brach:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git checkout master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Edit your README.md to provide details on how to use charts from your repository:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;# My [Helm](https://helm.sh) Charts

This repository contains [Helm](https://helm.sh) charts for various projects

* [Application 1](charts/app1/)
* [Application 2](charts/app2/)

## Installing Charts from this Repository

Add the Repository to Helm:

    helm repo add my-helm-charts https://tech.paulcz.net/my-helm-charts

Install Application 1:

    helm install my-helm-charts/app1

Install Application 2:

    helm install my-helm-charts/app2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Commit the change up to GitHub:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git add README.md
$ git commit -m &#39;update readme with instructions&#39;
$ git push origin master
...
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;That&amp;rsquo;s the end of &lt;strong&gt;Part 1&lt;/strong&gt; of this three part series. In future posts I will demonstrate adding &lt;a href=&#34;https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-2&#34;&gt;automation&lt;/a&gt; and &lt;a href=&#34;https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-3&#34;&gt;testing&lt;/a&gt; to this &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; chart repository.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Creating a Helm Chart Repository - Part 2</title>
      <link>https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-2/</link>
      <pubDate>Wed, 03 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-2/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Welcome to a three part blog series on Creating a &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Chart Repository. In &lt;a href=&#34;https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-1&#34;&gt;part 1&lt;/a&gt; of this series I demonstrated creating a &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; chart repository using &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; and &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Pages. In this &lt;strong&gt;part 2&lt;/strong&gt; I will add Automation to automatically update the repository, and in &lt;a href=&#34;https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-3&#34;&gt;part 2&lt;/a&gt; I will add testing for changes to the charts themselves.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If you&amp;rsquo;re into Videos, I walked JJ through starting with Helm from scratch all the way to creating a Helm Repo and CI/CD.  &lt;div class=&#34;embed video-player&#34;&gt;
&lt;iframe class=&#34;youtube-player&#34; type=&#34;text/html&#34; width=&#34;640&#34; height=&#34;385&#34; src=&#34;http://www.youtube.com/embed/xn63krHJNKI&#34; allowfullscreen frameborder=&#34;0&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;use-circle-ci-to-automate-helm-https-helm-sh-chart-updates&#34;&gt;Use Circle CI to automate &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Chart Updates&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: While I would usually use &lt;a href=&#34;https://concourse-ci.org/&#34;&gt;Concourse CI&lt;/a&gt; for my CI workflows, I wanted to &lt;em&gt;only&lt;/em&gt; use managed services and I chose Circle as that is already commonly used in the Helm community. It would be trivial to whip up a Concourse Pipeline to do the same thing.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now that we&amp;rsquo;ve successfully created a &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Chart Repostiory using &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; and &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; pages we can move on to adding some Automation so that our Chart Repository is updated any time we push changes up to our master branch.&lt;/p&gt;

&lt;p&gt;Its pretty easy to create a new Circle CI account. You simply go to their website and hit &lt;a href=&#34;https://circleci.com/signup/&#34;&gt;sign-up&lt;/a&gt;, it will ask you to log using &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt;Oauth2 and once you&amp;rsquo;ve given it access to your repositories you are good to go.&lt;/p&gt;

&lt;p&gt;Once logged in you need to hit the &lt;strong&gt;ADD Projects&lt;/strong&gt; menu item and hit the &lt;strong&gt;set up project&lt;/strong&gt; button next to &lt;strong&gt;my-helm-charts&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;circle-add-project.png&#34; alt=&#34;add project&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can leave the defaults and just go down and click the &lt;strong&gt;Start Building&lt;/strong&gt; button.&lt;/p&gt;

&lt;p&gt;It will attempt to run and fail because you don&amp;rsquo;t have a &lt;code&gt;.circleci/config.yml&lt;/code&gt; file in your repo yet. We&amp;rsquo;ll create that soon.&lt;/p&gt;

&lt;p&gt;Before we do that though we need to create a private key for Circle CI with write access to our project. Hit the &lt;strong&gt;settings&lt;/strong&gt; button on the top right of the &lt;strong&gt;Workflows -&amp;gt; username -&amp;gt; my-helm-charts&lt;/strong&gt; screen tat looks like a little cog.&lt;/p&gt;

&lt;p&gt;From here you want to hit SSH permissions and hit &lt;strong&gt;Checkout SSH Keys&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;There should be a &lt;strong&gt;Add user key&lt;/strong&gt; section with a button that says &lt;strong&gt;Authorize with GitHub&lt;/strong&gt;, hit that button. To be extra certain it loads the same page and you need to click the &lt;strong&gt;Create and add [username] user key&lt;/strong&gt; which will create a key and pass the public key off to github.&lt;/p&gt;

&lt;p&gt;On that same &lt;strong&gt;settings&lt;/strong&gt; page you need to add some environment variables:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./circle-env-vars.png&#34; alt=&#34;circle env vars&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now its time to set up our Automation.&lt;/p&gt;

&lt;h3 id=&#34;create-circle-ci-config-for-uploading-new-packages&#34;&gt;Create Circle CI config for uploading new packages&lt;/h3&gt;

&lt;p&gt;Create a new directory &lt;code&gt;.circleci&lt;/code&gt; and a file inside that called &lt;code&gt;config.yml&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ mkdir .circleci
$ touch .circleci/config.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Write out the &lt;code&gt;config.yml&lt;/code&gt; file like so:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: this CircleCI config file creates two jobs, One to lint the shell scripts we&amp;rsquo;re about to create, the other to release charts and copy documentation into our &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; repo website. These tasks will run when code is pushed or merged into the &lt;code&gt;master&lt;/code&gt; branch.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;circleci-config-yml-https-github-com-paulczar-my-helm-charts-blob-part-2-circleci-config-yml&#34;&gt;&lt;a href=&#34;https://github.com/paulczar/my-helm-charts/blob/part-2/.circleci/config.yml&#34;&gt;.circleci/config.yml&lt;/a&gt;&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;version: 2
jobs:
  lint-scripts:
    docker:
      - image: koalaman/shellcheck-alpine
    steps:
      - checkout
      - run:
          name: lint
          command: shellcheck -x .circleci/*.sh
  release-charts:
    machine: true
    steps:
      - checkout
      - run:
          command: |
            echo &amp;quot;export GIT_REPOSITORY_URL=$CIRCLE_REPOSITORY_URL&amp;quot; &amp;gt;&amp;gt; $BASH_ENV
            echo &amp;quot;export GIT_USERNAME=$CIRCLE_PROJECT_USERNAME&amp;quot; &amp;gt;&amp;gt; $BASH_ENV
            echo &amp;quot;export GIT_REPOSITORY_NAME=$CIRCLE_PROJECT_REPONAME&amp;quot; &amp;gt;&amp;gt; $BASH_ENV
            .circleci/install_tools.sh
            .circleci/release.sh

workflows:
  version: 2
  release:
    jobs:
      - lint-scripts
      - release-charts:
          filters:
            tags:
              ignore: /.*/
            branches:
              only: master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We referenced two scripts in the &lt;code&gt;config.yml&lt;/code&gt; file, so we better create those. These scripts are a mix of ones that I have written, and have borrowed from &lt;a href=&#34;https://twitter.com/unguiculus&#34;&gt;Reinhard Nägele&lt;/a&gt; one of the main contributors to awesome tooling in the Helm Community as found &lt;a href=&#34;https://github.com/codecentric/helm-charts/blob/master/.circleci/release.sh&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;It&amp;rsquo;s no surprise that these scripts came from &lt;a href=&#34;https://twitter.com/unguiculus&#34;&gt;Reinhard Nägele&lt;/a&gt;as he is a primary maintainer of both &lt;a href=&#34;https://github.com/helm/chart-testing&#34;&gt;chart-testing&lt;/a&gt; and &lt;a href=&#34;https://github.com/helm/chart-releaser&#34;&gt;chart-releaser&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;circleci-install-tools-sh-https-github-com-paulczar-my-helm-charts-blob-part-2-circleci-install-tools-sh&#34;&gt;&lt;a href=&#34;https://github.com/paulczar/my-helm-charts/blob/part-2/.circleci/install_tools.sh&#34;&gt;.circleci/install_tools.sh&lt;/a&gt;&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/usr/bin/env bash

set -o errexit

readonly HELM_VERSION=2.13.1
readonly CHART_RELEASER_VERSION=0.1.4

echo &amp;quot;Installing Helm...&amp;quot;
curl -LO &amp;quot;https://kubernetes-helm.storage.googleapis.com/helm-v$HELM_VERSION-linux-amd64.tar.gz&amp;quot;
sudo mkdir -p &amp;quot;/usr/local/helm-v$HELM_VERSION&amp;quot;
sudo tar -xzf &amp;quot;helm-v$HELM_VERSION-linux-amd64.tar.gz&amp;quot; -C &amp;quot;/usr/local/helm-v$HELM_VERSION&amp;quot;
sudo ln -s &amp;quot;/usr/local/helm-v$HELM_VERSION/linux-amd64/helm&amp;quot; /usr/local/bin/helm
rm -f &amp;quot;helm-v$HELM_VERSION-linux-amd64.tar.gz&amp;quot;
helm init --client-only

echo &amp;quot;Installing chart-releaser...&amp;quot;
curl -LO &amp;quot;https://github.com/helm/chart-releaser/releases/download/v${CHART_RELEASER_VERSION}/chart-releaser_${CHART_RELEASER_VERSION}_Linux_x86_64.tar.gz&amp;quot;
sudo mkdir -p &amp;quot;/usr/local/chart-releaser-v$CHART_RELEASER_VERSION&amp;quot;
sudo tar -xzf &amp;quot;chart-releaser_${CHART_RELEASER_VERSION}_Linux_x86_64.tar.gz&amp;quot; -C &amp;quot;/usr/local/chart-releaser-v$CHART_RELEASER_VERSION&amp;quot;
sudo ln -s &amp;quot;/usr/local/chart-releaser-v$CHART_RELEASER_VERSION/chart-releaser&amp;quot; /usr/local/bin/chart-releaser
rm -f &amp;quot;chart-releaser_${CHART_RELEASER_VERSION}_Linux_x86_64.tar.gz&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;circleci-release-sh-https-github-com-paulczar-my-helm-charts-blob-part-2-circleci-release-sh&#34;&gt;&lt;a href=&#34;https://github.com/paulczar/my-helm-charts/blob/part-2/.circleci/release.sh&#34;&gt;.circleci/release.sh&lt;/a&gt;&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/usr/bin/env bash

set -o errexit
set -o nounset
set -o pipefail

: &amp;quot;${CH_TOKEN:?Environment variable CH_TOKEN must be set}&amp;quot;
: &amp;quot;${GIT_REPOSITORY_URL:?Environment variable GIT_REPO_URL must be set}&amp;quot;
: &amp;quot;${GIT_USERNAME:?Environment variable GIT_USERNAME must be set}&amp;quot;
: &amp;quot;${GIT_EMAIL:?Environment variable GIT_EMAIL must be set}&amp;quot;
: &amp;quot;${GIT_REPOSITORY_NAME:?Environment variable GIT_REPOSITORY_NAME must be set}&amp;quot;

readonly REPO_ROOT=&amp;quot;${REPO_ROOT:-$(git rev-parse --show-toplevel)}&amp;quot;

main() {
    pushd &amp;quot;$REPO_ROOT&amp;quot; &amp;gt; /dev/null

    echo &amp;quot;Fetching tags...&amp;quot;
    git fetch --tags

    local latest_tag
    latest_tag=$(find_latest_tag)

    local latest_tag_rev
    latest_tag_rev=$(git rev-parse --verify &amp;quot;$latest_tag&amp;quot;)
    echo &amp;quot;$latest_tag_rev $latest_tag (latest tag)&amp;quot;

    local head_rev
    head_rev=$(git rev-parse --verify HEAD)
    echo &amp;quot;$head_rev HEAD&amp;quot;

    if [[ &amp;quot;$latest_tag_rev&amp;quot; == &amp;quot;$head_rev&amp;quot; ]]; then
        echo &amp;quot;No code changes. Nothing to release.&amp;quot;
        exit
    fi

    rm -rf .deploy
    mkdir -p .deploy

    echo &amp;quot;Identifying changed charts since tag &#39;$latest_tag&#39;...&amp;quot;

    local changed_charts=()
    readarray -t changed_charts &amp;lt;&amp;lt;&amp;lt; &amp;quot;$(git diff --find-renames --name-only &amp;quot;$latest_tag_rev&amp;quot; -- charts | cut -d &#39;/&#39; -f 2 | uniq)&amp;quot;

    if [[ -n &amp;quot;${changed_charts[*]}&amp;quot; ]]; then
        for chart in &amp;quot;${changed_charts[@]}&amp;quot;; do
            echo &amp;quot;Packaging chart &#39;$chart&#39;...&amp;quot;
            package_chart &amp;quot;charts/$chart&amp;quot;
        done

        release_charts
        sleep 5
        update_index
    else
        echo &amp;quot;Nothing to do. No chart changes detected.&amp;quot;
    fi

    popd &amp;gt; /dev/null
}

find_latest_tag() {
    if ! git describe --tags --abbrev=0 2&amp;gt; /dev/null; then
        git rev-list --max-parents=0 --first-parent HEAD
    fi
}

package_chart() {
    local chart=&amp;quot;$1&amp;quot;
    Helm dependency build &amp;quot;$chart&amp;quot;
    Helm package &amp;quot;$chart&amp;quot; --destination .deploy
}

release_charts() {
    chart-releaser upload -o &amp;quot;$GIT_USERNAME&amp;quot; -r &amp;quot;$GIT_REPOSITORY_NAME&amp;quot; -p .deploy
}

update_index() {
    chart-releaser index -o &amp;quot;$GIT_USERNAME&amp;quot; -r &amp;quot;$GIT_REPOSITORY_NAME&amp;quot; -p .deploy/index.yaml

    git config user.email &amp;quot;$GIT_EMAIL&amp;quot;
    git config user.name &amp;quot;$GIT_USERNAME&amp;quot;

    for file in charts/*/*.md; do
        if [[ -e $file ]]; then
            mkdir -p &amp;quot;.deploy/docs/$(dirname &amp;quot;$file&amp;quot;)&amp;quot;
            cp --force &amp;quot;$file&amp;quot; &amp;quot;.deploy/docs/$(dirname &amp;quot;$file&amp;quot;)&amp;quot;
        fi
    done

    git checkout gh-pages
    cp --force .deploy/index.yaml index.yaml

    if [[ -e &amp;quot;.deploy/docs/charts&amp;quot; ]]; then
        mkdir -p charts
        cp --force --recursive .deploy/docs/charts/* charts/
    fi

    git checkout master -- README.md

    if ! git diff --quiet; then
        git add .
        git commit --message=&amp;quot;Update index.yaml&amp;quot; --signoff
        git push &amp;quot;$GIT_REPOSITORY_URL&amp;quot; gh-pages
    fi
}

main
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add these new files to git and push them up to the &lt;code&gt;master&lt;/code&gt; branch:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git add .

$ git status
On branch master
Your branch is up to date with &#39;origin/master&#39;.

Changes to be committed:
  (use &amp;quot;git reset HEAD &amp;lt;file&amp;gt;...&amp;quot; to unstage)

  new file:   .circleci/config.yml
  new file:   .circleci/install_tools.sh
  new file:   .circleci/release.sh

$ git commit -m &#39;add circle ci scripts&#39;

$ git push origin master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This push should kick off a Circle CI job which will hopefully pass (I usually get it wrong the first few times).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./circle-first-job.png&#34; alt=&#34;circle first job&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You&amp;rsquo;ll notice there&amp;rsquo;s a failed job, that&amp;rsquo;s because when circleci sees the releases being updated it tries to run a job for the &lt;code&gt;gh-pages&lt;/code&gt; branch that doesn&amp;rsquo;t have a circle-ci config. We can use this sweet git trick to grab the one from the master branch:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git checkout gh-pages
$ git pull origin gh-pages
$ mkdir .circleci
$ git checkout master -- .circleci/config.yml
$ git add .circleci/config.yml
$ git commit -m &#39;add circleci config&#39;
$ git push origin gh-pages
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;validate-the-release-of-new-charts&#34;&gt;Validate the release of new charts&lt;/h3&gt;

&lt;p&gt;So far we haven&amp;rsquo;t actually changed our &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Charts, so the automation hasn&amp;rsquo;t created a new release. We can change this by bumping the chart version of one of them.  Edit &lt;code&gt;./charts/app1/Chart.yaml&lt;/code&gt; and bump the version like so:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
appVersion: &amp;quot;1.0&amp;quot;
description: A Helm chart for Kubernetes
name: app1
version: 0.1.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Push this change up:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git add .
$ git commit -m &#39;update app1 chart&#39;
$ git push origin master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should see the new job show up in Circle and complete fairly quickly.&lt;/p&gt;

&lt;p&gt;Once the job has completed successfully you can check you now have a &lt;code&gt;myapp-0.1.1&lt;/code&gt; release in your &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt;repo and your &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; repository now has &lt;code&gt;myapp-0.1.1&lt;/code&gt; in its &lt;code&gt;index.yaml&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ curl http://tech.paulcz.net/my-helm-charts/index.yaml
apiVersion: v1
entries:
  app1:
  - apiVersion: v1
    appVersion: &amp;quot;1.0&amp;quot;
    created: &amp;quot;2019-07-03T23:16:21.087774995Z&amp;quot;
    description: A Helm chart for Kubernetes
    digest: 9fbf6f9d10fba82aa3b749875e137b283890136a7379efba2bbff0b645cb1c35
    name: app1
    urls:
    - https://github.com/paulczar/my-helm-charts/releases/download/app1-0.1.1/app1-0.1.1.tgz
    version: 0.1.1
  - apiVersion: v1
    appVersion: &amp;quot;1.0&amp;quot;
    created: &amp;quot;2019-07-03T23:16:21.376254864Z&amp;quot;
    description: A Helm chart for Kubernetes
    digest: 48cf831b72febeac2860a0be372094250aea68a9c76147c028085c8802dd48ec
    name: app1
    urls:
    - https://github.com/paulczar/my-helm-charts/releases/download/app1-0.1.0/app1-0.1.0.tgz
    version: 0.1.0
  app2:
  - apiVersion: v1
    appVersion: &amp;quot;1.0&amp;quot;
    created: &amp;quot;2019-07-03T23:16:21.22793015Z&amp;quot;
    description: Helm chart for Kubernetes
    digest: 64b00fc4804aba524201f64e78ee22ad8e61d0923424f8e24e8b70befed88141
    name: app2
    urls:
    - https://github.com/paulczar/my-helm-charts/releases/download/app2-0.1.0/app2-0.1.0.tgz
    version: 0.1.0
generated: &amp;quot;2019-07-03T23:16:20.624914794Z&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In &lt;a href=&#34;https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-1&#34;&gt;Part 1&lt;/a&gt; we created set of &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Charts managed in source control (GitHub) and in Part 2 we just added automation via CircleCI to automate building and deploying Chart packages to a &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Chart Repository hosted in &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; pages and &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt;releases.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&#34;https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-3&#34;&gt;Part 3&lt;/a&gt; we will add further automation to test for changes in those &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; charts and to pass them through rigorous testing before allowing them to be merged into the &lt;code&gt;master&lt;/code&gt; branch.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Creating a Helm Chart Repository - Part 3</title>
      <link>https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-3/</link>
      <pubDate>Wed, 03 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-3/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Welcome to a three part blog series on Creating a &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Chart Repository. In &lt;a href=&#34;https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-1&#34;&gt;part 1&lt;/a&gt; of this series I demonstrated creating a &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; chart repository using &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; and &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Pages. In &lt;a href=&#34;https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-2&#34;&gt;part 2&lt;/a&gt; I will add Automation to automatically update the repository, and in &lt;strong&gt;part 3&lt;/strong&gt; I will add testing for changes to the charts themselves.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If you&amp;rsquo;re into Videos, I walked JJ through starting with Helm from scratch all the way to creating a Helm Repo and CI/CD.  &lt;div class=&#34;embed video-player&#34;&gt;
&lt;iframe class=&#34;youtube-player&#34; type=&#34;text/html&#34; width=&#34;640&#34; height=&#34;385&#34; src=&#34;http://www.youtube.com/embed/xn63krHJNKI&#34; allowfullscreen frameborder=&#34;0&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;use-circle-ci-to-test-helm-https-helm-sh-charts&#34;&gt;Use Circle CI to test &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Charts&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Note - You could use any other CI system here, I chose Circle as it is easy to integrate with &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; and has a free tier. If you do use a different CI system the scripts should still work, but you&amp;rsquo;ll need to rewrite a config file suitable for your CI choice.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;introducing-chart-testing&#34;&gt;Introducing Chart Testing&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; community has built a tool very imaginitively named [Chart Testing]((&lt;a href=&#34;https://github.com/helm/chart-testing&#34;&gt;https://github.com/helm/chart-testing&lt;/a&gt;) specifically for testing &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; charts. Not only is it capable of linting and performing test installs of a &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; chart, but its also designed to work within a monorepo and only test those charts that have changed.&lt;/p&gt;

&lt;p&gt;You can download and use &lt;a href=&#34;https://github.com/helm/chart-testing/releases&#34;&gt;Chart Testing&lt;/a&gt; locally, but really the power of it is using it in CI, so lets go straight to that.&lt;/p&gt;

&lt;h3 id=&#34;creat-a-chart-testing-script-and-update-circle-ci-config&#34;&gt;Creat a Chart Testing script and update Circle CI config&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: While I would usually use &lt;a href=&#34;https://concourse-ci.org/&#34;&gt;Concourse CI&lt;/a&gt; for my CI workflows, I wanted to &lt;em&gt;only&lt;/em&gt; use managed services and I chose Circle as that is already commonly used in the Helm community.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We need to add a new script, a chart-testing config file, and update the Circle CI config file.&lt;/p&gt;

&lt;h4 id=&#34;circleci-config-yaml-https-github-com-paulczar-my-helm-charts-blob-part-2-circleci-config-yaml&#34;&gt;&lt;a href=&#34;https://github.com/paulczar/my-helm-charts/blob/part-2/.circleci/config.yaml&#34;&gt;./circleci/config.yaml&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;Create two new jobs:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;These scripts and configs were heavily borrowed from &lt;a href=&#34;https://twitter.com/unguiculus&#34;&gt;Reinhard Nägele&lt;/a&gt; who is a primary maintainer of both &lt;a href=&#34;https://github.com/helm/chart-testing&#34;&gt;chart-testing&lt;/a&gt; and &lt;a href=&#34;https://github.com/helm/chart-releaser&#34;&gt;chart-releaser&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The first job tells Chart Testing to lint the charts according to the &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Community &lt;a href=&#34;https://helm.sh/docs/chart_best_practices/&#34;&gt;Best Practices Guide&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The second job tells Chart Testing to actually install and test the charts using KIND (Kubernetes IN Docker).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;  lint-charts:
    docker:
      - image: gcr.io/kubernetes-charts-ci/test-image:v3.3.2
    steps:
      - checkout
      - run:
          name: lint
          command: |
            git remote add upstream https://github.com/paulczar/percona-helm-charts
            git fetch upstream master
            ct lint --config .circleci/ct.yaml

  install-charts:
    machine: true
    steps:
      - checkout
      - run:
          no_output_timeout: 12m
          command: .circleci/install_charts.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add a new workflow telling Circle to lint and test any changes.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: it excludes the &lt;code&gt;master&lt;/code&gt; branch as we don&amp;rsquo;t want to try to retest charts as they&amp;rsquo;re merged in after successfully testing the new commit.:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;  lint-and-install:
    jobs:
      - lint-scripts
      - lint-charts:
          filters:
            branches:
              ignore: master
            tags:
              ignore: /.*/
      - install-charts:
          requires:
            - lint-charts
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;circleci-ct-yaml-https-github-com-paulczar-my-helm-charts-blob-part-3-circleci-ct-yaml&#34;&gt;&lt;a href=&#34;https://github.com/paulczar/my-helm-charts/blob/part-3/.circleci/ct.yaml&#34;&gt;./circleci/ct.yaml&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;This file provides configuration for Chart Testing. For now all we need is to tell it to provide &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; with a longer timeout:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;helm-extra-args: --timeout 600
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;circleci-kind-config-yaml-https-github-com-paulczar-my-helm-charts-blob-part-3-circleci-kind-config-yaml&#34;&gt;&lt;a href=&#34;https://github.com/paulczar/my-helm-charts/blob/part-3/.circleci/kind-config.yaml&#34;&gt;./circleci/kind-config.yaml&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;This file provides a configuration for KIND to use:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;kind: Cluster
apiVersion: kind.sigs.k8s.io/v1alpha3
nodes:
  - role: control-plane
  - role: worker
  - role: worker
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;circleci-install-charts-sh-https-github-com-paulczar-my-helm-charts-blob-part-3-circleci-install-charts-sh&#34;&gt;&lt;a href=&#34;https://github.com/paulczar/my-helm-charts/blob/part-3/.circleci/install_charts.sh&#34;&gt;./circleci/install_charts.sh&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;Finally this script will install KIND and will perform test installations for any changed &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Charts:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/usr/bin/env bash

set -o errexit
set -o nounset
set -o pipefail

readonly CT_VERSION=v2.3.3
readonly KIND_VERSION=0.2.1
readonly CLUSTER_NAME=chart-testing
readonly K8S_VERSION=v1.14.0

run_ct_container() {
    echo &#39;Running ct container...&#39;
    docker run --rm --interactive --detach --network host --name ct \
        --volume &amp;quot;$(pwd)/.circleci/ct.yaml:/etc/ct/ct.yaml&amp;quot; \
        --volume &amp;quot;$(pwd):/workdir&amp;quot; \
        --workdir /workdir \
        &amp;quot;quay.io/helmpack/chart-testing:$CT_VERSION&amp;quot; \
        cat
    echo
}

cleanup() {
    echo &#39;Removing ct container...&#39;
    docker kill ct &amp;gt; /dev/null 2&amp;gt;&amp;amp;1

    echo &#39;Done!&#39;
}

docker_exec() {
    docker exec --interactive ct &amp;quot;$@&amp;quot;
}

create_kind_cluster() {
    echo &#39;Installing kind...&#39;

    curl -sSLo kind &amp;quot;https://github.com/kubernetes-sigs/kind/releases/download/$KIND_VERSION/kind-linux-amd64&amp;quot;
    chmod +x kind
    sudo mv kind /usr/local/bin/kind

    kind create cluster --name &amp;quot;$CLUSTER_NAME&amp;quot; --config .circleci/kind-config.yaml --image &amp;quot;kindest/node:$K8S_VERSION&amp;quot; --wait 60s

    docker_exec mkdir -p /root/.kube

    echo &#39;Copying kubeconfig to container...&#39;
    local kubeconfig
    kubeconfig=&amp;quot;$(kind get kubeconfig-path --name &amp;quot;$CLUSTER_NAME&amp;quot;)&amp;quot;
    docker cp &amp;quot;$kubeconfig&amp;quot; ct:/root/.kube/config

    docker_exec kubectl cluster-info
    echo

    docker_exec kubectl get nodes
    echo
}

install_local_path_provisioner() {
    docker_exec kubectl delete storageclass standard
    docker_exec kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml
}

install_tiller() {
    echo &#39;Installing Tiller...&#39;
    docker_exec kubectl --namespace kube-system create sa tiller
    docker_exec kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller
    docker_exec helm init --service-account tiller --upgrade --wait
    echo
}

install_charts() {
    docker_exec ct install
    echo
}

main() {
    run_ct_container
    trap cleanup EXIT

    changed=$(docker_exec ct list-changed)
    if [[ -z &amp;quot;$changed&amp;quot; ]]; then
        echo &#39;No chart changes detected.&#39;
        return
    fi

    echo &#39;Chart changes detected.&#39;
    create_kind_cluster
    install_local_path_provisioner
    install_tiller
    install_charts
}

main
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;commit-the-changes&#34;&gt;Commit the changes&lt;/h3&gt;

&lt;p&gt;Next up commit these new changes to your master branch:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git add .
$ git commit -m &#39;add chart testing on PRs&#39;
$ git push origin master
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;test-the-new-automation&#34;&gt;Test the new Automation&lt;/h2&gt;

&lt;p&gt;Create a new branch:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git checkout -b update-app2-chart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Modify the app2 &lt;code&gt;Chart.yaml&lt;/code&gt; to be a new version number:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
appVersion: &amp;quot;1.0&amp;quot;
description: A Helm chart for Kubernetes
name: app2
version: 0.1.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Commit and Push the changes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git add charts/app2/Chart.yaml
$ git commit -m &#39;bump chart2 version&#39;
$ git push origin update-app2-chart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Circle CI should run tests and should fail:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;circle-test-fail.png&#34; alt=&#34;circle fail test&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This failure is because when &lt;code&gt;helm create&lt;/code&gt; creates your chart, it doesn&amp;rsquo;t implement all of our best practices. If you check in the Circle CI job log you&amp;rsquo;ll see:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;Error validating data /root/project/charts/app2/Chart.yaml with schema /etc/ct/chart_schema.yaml
  home: Required field missing
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The error is quite clear, we should have a field &lt;code&gt;home&lt;/code&gt; in our &lt;code&gt;Chart.yaml&lt;/code&gt;. In fact there should also be a &lt;code&gt;maintainers&lt;/code&gt; field. Let&amp;rsquo;s add those into both chart&amp;rsquo;s &lt;code&gt;Chart.yaml&lt;/code&gt; files:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;home: http://github.com/paulczar/my-helm-charts
maintainers:
  - name: paulczar
    email: username.taken@gmail.com
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: Since you&amp;rsquo;re also changing App1, you should bump its version a patch level to &lt;code&gt;0.1.2&lt;/code&gt;, all changes to a Chart, even non functional one should bump the chart version.&lt;/p&gt;

&lt;p&gt;Note: Ensure you leave a blank line at the end of the &lt;code&gt;Chart.yaml&lt;/code&gt; file. I forgot to and had to resubmit.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Push these new changes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git add .
$ git commit -m &#39;comply to chart best practices&#39;
$ git push origin update-app2-chart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After a few seconds you should see the new jobs start in CircleCI and this time all three tasks should complete successfully:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;circle-lint-and-install.png&#34; alt=&#34;circle ci lint and install&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It took about 6 minutes to run, because it did a full install of both Charts (as we changed them both) to a disposable KIND cluster.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: Since this was a branch, the charts were not released to the Chart Repository as that job is only triggered on the &lt;code&gt;master branch&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Next you&amp;rsquo;ll want to create a pull request for this change, you can do that via the &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; web ui:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;github-pull-request.png&#34; alt=&#34;github pull request&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: Since Circle CI has already tested the commits in this PR (Pull Request) it shows handy little test pass/fail marks against the commits.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Since the PR is showing as passing tests, you can go ahead and Merge it by clicking that green &lt;code&gt;Merge&lt;/code&gt; button (although I like to use &lt;code&gt;Squash and Merge&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;This Merge into the &lt;code&gt;master&lt;/code&gt; branch will kick off the &lt;code&gt;release-charts&lt;/code&gt; workflow and after a few seconds we&amp;rsquo;ll have an updated &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Repository:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ curl http://tech.paulcz.net/my-helm-charts/index.yaml
apiVersion: v1
entries:
...
...
  app1:
    name: app1
    version: 0.1.2
...
...
  app2:
    name: app2
    version: 0.1.1
...
...
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;testing-pull-requests&#34;&gt;Testing Pull Requests&lt;/h2&gt;

&lt;p&gt;In the advanced settings of Circle CI you can tell it to automatically test Pull Requests that come from forks of your &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; repository. Adding this is a great feature if you want others to work on your code with you. However you do need to protect your secrets.&lt;/p&gt;

&lt;p&gt;For example a bad actor could add &amp;ldquo;echo $CH_TOKEN&amp;rdquo; to one of the scripts and steal my &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; token which they could then use to mess with my Repositories.&lt;/p&gt;

&lt;p&gt;For that reason I&amp;rsquo;ve opted not to include that in this example.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In &lt;a href=&#34;https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-1&#34;&gt;Part 1&lt;/a&gt; we created set of &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Charts managed in source control (GitHub).&lt;/p&gt;

&lt;p&gt;In &lt;a href=&#34;https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-2&#34;&gt;Part 2&lt;/a&gt; we added automation via CircleCI to automate building and deploying Chart packages to a &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Chart Repository hosted in &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Pages and &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Releases.&lt;/p&gt;

&lt;p&gt;In &lt;strong&gt;Part 3&lt;/strong&gt; we added further automation to test changes in those &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; charts and to pass them through rigorous testing before allowing them to be merged into the &lt;code&gt;master&lt;/code&gt; branch.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Spring into Kubernetes - Using Kubernetes as a Config Server</title>
      <link>https://tech.paulcz.net/blog/spring-into-kubernetes-config-server/</link>
      <pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/spring-into-kubernetes-config-server/</guid>
      <description>

&lt;p&gt;In previous installments of Spring into Kubernetes I&amp;rsquo;ve shown you how to &lt;a href=&#34;https://tech.paulcz.net/blog/building-spring-docker-images/&#34;&gt;build images&lt;/a&gt;, &lt;a href=&#34;https://tech.paulcz.net/blog/spring-into-kubernetes-part-1/&#34;&gt;deploy applications&lt;/a&gt; and write a &lt;a href=&#34;https://tech.paulcz.net/blog/spring-into-kubernetes-part-2/&#34;&gt;Helm Chart&lt;/a&gt; for Spring applications. In this installment we&amp;rsquo;ll look at &lt;a href=&#34;https://github.com/spring-cloud/spring-cloud-kubernetes&#34;&gt;Spring Cloud Kubernetes&lt;/a&gt; integrations, specifically using Kubernetes &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#create-configmaps-from-files&#34;&gt;Config Maps&lt;/a&gt; as a Config Server.&lt;/p&gt;

&lt;p&gt;Usually I would use a &lt;a href=&#34;https://pivotal.io/platform/pivotal-container-service&#34;&gt;Pivotal Container Service&lt;/a&gt; cluster to demonstrate, but in this demonstration I&amp;rsquo;ll use a local &lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-minikube/&#34;&gt;minikube&lt;/a&gt; cluster.&lt;/p&gt;

&lt;h2 id=&#34;spring-cloud-kubernetes&#34;&gt;Spring Cloud Kubernetes&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/spring-cloud/spring-cloud-kubernetes&#34;&gt;Spring Cloud Kubernetes&lt;/a&gt; brings in a ton of integrations with Kubernetes. This demonstration will focus just on the ability to integrate Kubernetes as a &lt;a href=&#34;https://github.com/spring-cloud/spring-cloud-kubernetes#kubernetes-propertysource-implementations&#34;&gt;configuration server&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Minimal changes are needed to your applications, you need to simply add the following classes to your &lt;code&gt;pom.xml&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;spring-boot-starter-actuator&amp;lt;/artifactId&amp;gt;
&amp;lt;/dependency&amp;gt;
&amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;org.springframework.cloud&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;spring-cloud-starter-kubernetes-config&amp;lt;/artifactId&amp;gt;
&amp;lt;/dependency&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You also need to enable it in your &lt;code&gt;bootstrap.yaml&lt;/code&gt; (or &lt;code&gt;.properties&lt;/code&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;spring.cloud.kubernetes.config.enabled: true
spring.cloud.kubernetes.reload.enabled: true
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;getting-started&#34;&gt;Getting Started&lt;/h2&gt;

&lt;p&gt;You&amp;rsquo;ll need to &lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-minikube/&#34;&gt;install minikube&lt;/a&gt; by following the instructions provided for your Operating System. You&amp;rsquo;ll also need &lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-kubectl/&#34;&gt;kubectl&lt;/a&gt; so that you can give instructions to Kubernetes.&lt;/p&gt;

&lt;p&gt;Start Minikube:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ minikube start
😄  minikube v0.34.1 on linux (amd64)
💡  Tip: Use &#39;minikube start -p &amp;lt;name&amp;gt;&#39; to create a new cluster, or &#39;minikube delete&#39; to delete this one.
🔄  Restarting existing virtualbox VM for &amp;quot;minikube&amp;quot; ...
⌛  Waiting for SSH access ...
📶  &amp;quot;minikube&amp;quot; IP address is 192.168.99.103
🐳  Configuring Docker as the container runtime ...
✨  Preparing Kubernetes environment ...
🚜  Pulling images required by Kubernetes v1.13.3 ...
🔄  Relaunching Kubernetes v1.13.3 using kubeadm ...
⌛  Waiting for kube-proxy to come back up ...
🤔  Verifying component health ......
💗  kubectl is now configured to use &amp;quot;minikube&amp;quot;
🏄  Done! Thank you for using minikube!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ensure that you can communicate with minikube:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl get nodes
NAME       STATUS   ROLES    AGE   VERSION
minikube   Ready    master   36h   v1.13.3
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;deploy-spring-hello-world&#34;&gt;Deploy Spring Hello World&lt;/h2&gt;

&lt;p&gt;The source for this demo can be found at &lt;a href=&#34;https://github.com/paulczar/spring-helloworld&#34;&gt;paulczar/spring-hello&lt;/a&gt; on github. Of importance it will respond to a web request with the contents of the application property &lt;code&gt;message&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Deploy the example Hello World application and expose it via a service:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl run hello --image=paulczar/spring-hello:k8s001 --port=8080
deployment.apps/hello created

$ kubectl expose deployment hello --type=LoadBalancer --port 80 --target-port 8080
service/hello exposed
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can use &lt;code&gt;minikube service list&lt;/code&gt; to get a list of services and the URLs for those services. This helps make up for the lack of LoadBalancer support in minikube:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ minikube service list
|-------------|------------|-----------------------------|
|  NAMESPACE  |    NAME    |             URL             |
|-------------|------------|-----------------------------|
| default     | hello      | http://192.168.99.103:30871 |
| default     | kubernetes | No node port                |
| kube-system | kube-dns   | No node port                |
|-------------|------------|-----------------------------|
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Use the URL provided for the &lt;code&gt;hello&lt;/code&gt; service:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl http://192.168.99.103:30871
hello development
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our application is running and responding with &lt;code&gt;hello development&lt;/code&gt;. This is the default value for &lt;code&gt;message&lt;/code&gt; in the &lt;code&gt;development&lt;/code&gt; spring profile.&lt;/p&gt;

&lt;h2 id=&#34;configure-kubernetes-support-for-spring-hello-world&#34;&gt;Configure Kubernetes support for Spring Hello World&lt;/h2&gt;

&lt;p&gt;If you have &lt;code&gt;rbac&lt;/code&gt; enabled in your cluster (which you should, we&amp;rsquo;re not animals) the service account your application is running and will be unable to view kubernetes resources. You can see these errors in the pod&amp;rsquo;s logs:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;k logs deployment/hello
2019-03-01 16:13:02.629  WARN 1 --- [           main] o.s.cloud.kubernetes.StandardPodUtils    : Failed to get pod with name:[hello-bb9cf575d-rqt6n]. You should look into this if things aren&#39;t working as you expect. Are you missing serviceaccount permissions?

io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: GET at: https://10.96.0.1/api/v1/namespaces/default/pods/hello-bb9cf575d-rqt6n. Message: Forbidden!Configured service account doesn&#39;t have access. Service account may have been revoked. pods &amp;quot;hello-bb9cf575d-rqt6n&amp;quot; is forbidden: User &amp;quot;system:serviceaccount:default:default&amp;quot; cannot get resource &amp;quot;pods&amp;quot; in API group &amp;quot;&amp;quot; in the namespace &amp;quot;default&amp;quot;.
...
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You could create a new service user and give it the appropriate permissions, or you could give permissions to the default service account. Do the latter using a kubernetes manifest found &lt;a href=&#34;https://raw.githubusercontent.com/paulczar/spring-helloworld/master/deploy/rbac.yaml&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Update the &lt;code&gt;rolebinding&lt;/code&gt; for the default user:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl apply -f https://raw.githubusercontent.com/paulczar/spring-helloworld/master/deploy/rbac.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Delete the current pod to have the kubernetes deployment start a new one which should now have permissions:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl get pods
NAME                    READY   STATUS    RESTARTS   AGE
hello-bb9cf575d-rqt6n   1/1     Running   0          12m

$ kubectl delete pod hello-bb9cf575d-rqt6n
pod &amp;quot;hello-bb9cf575d-rqt6n&amp;quot; deleted

$ kubectl get pods
NAME                    READY   STATUS    RESTARTS   AGE
hello-bb9cf575d-8mwxq   1/1     Running   0          21s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you look at the logs you&amp;rsquo;ll see the error has disappeared and you can see it is looking for a &lt;code&gt;configmap&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl logs deployment/hello
2019-03-01 16:26:33.080 DEBUG 1 --- [           main] o.s.cloud.kubernetes.config.ConfigUtils  : Config Map name has not been set, taking it from property/env spring.application.name (default=application)
2019-03-01 16:26:33.080 DEBUG 1 --- [           main] o.s.cloud.kubernetes.config.ConfigUtils  : Config Map namespace has not been set, taking it from client (ns=default)
2019-03-01 16:26:33.188  INFO 1 --- [           main] b.c.PropertySourceBootstrapConfiguration : Located property source: CompositePropertySource {name=&#39;composite-configmap&#39;, propertySources=[ConfigMapPropertySource@872306601 {name=&#39;configmap.hello.default&#39;, properties={}}]}

Since you haven&#39;t yet created a `configmap` the response from the application should still be the default:

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;console
$ curl &lt;a href=&#34;http://192.168.99.103:30871&#34;&gt;http://192.168.99.103:30871&lt;/a&gt;
hello development&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
Next create a `configmap` that the application will use, by default it will look for a `configmap` with the same name as the application:

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;console
$ kubectl create configmap hello &amp;ndash;from-literal=message=&amp;ldquo;HELLO KUBERNETES&amp;rdquo;
configmap/hello created&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
Run the `curl` command again and you should see the new response:

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;console
$ curl &lt;a href=&#34;http://192.168.99.103:30871&#34;&gt;http://192.168.99.103:30871&lt;/a&gt;
HELLO KUBERNETES
```&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;While this was a fairly simple demonstration of the Spring Cloud Kubernetes integrations you can see how useful it can be. By integrating directly into Kubernetes you can avoid running a &lt;a href=&#34;https://spring.io/projects/spring-cloud-config&#34;&gt;Spring Cloud Config&lt;/a&gt; service to get dynamic configuration of your application.&lt;/p&gt;

&lt;p&gt;You can also load up an entire application properties file (either &lt;code&gt;.properties&lt;/code&gt; or &lt;code&gt;.yaml&lt;/code&gt;) inside a &lt;code&gt;configmap&lt;/code&gt;, you can also store passwords and keys in a Kubernetes &lt;code&gt;secret&lt;/code&gt; and dynamically load those.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Spring into Kubernetes - Deploying with Helm</title>
      <link>https://tech.paulcz.net/blog/spring-into-kubernetes-part-2/</link>
      <pubDate>Thu, 07 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/spring-into-kubernetes-part-2/</guid>
      <description>

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;In this installment of Spring into Kubernetes we&amp;rsquo;ll be looking at using &lt;a href=&#34;https://helm.sh/&#34;&gt;Helm&lt;/a&gt; to install our Spring application to Kubernetes.&lt;/p&gt;

&lt;h1 id=&#34;about-helm&#34;&gt;About Helm&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://helm.sh/&#34;&gt;Helm&lt;/a&gt; is the package manager for Kubernetes. It provides tooling to create, template, package, and &lt;a href=&#34;https://hub.helm.sh/&#34;&gt;share&lt;/a&gt; Kubernetes manifests. A helm chart is effectively a signed tarball that contains a set of templated Kubernetes manifests, a metadata file and a set of default values.&lt;/p&gt;

&lt;p&gt;Helm has two major components, the Helm client and the Tiller server. The Helm client is a CLI tool that you use to create, package and deploy helm charts. The Tiller server is installed into your Kubernetes cluster and is responsible for managing the lifecycle of your applications as instructed by the Helm client.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: The Tiller server was created before Kubernetes extensions existed and will be removed in Helm 3 in favor of utilizing Kubernetes extensions. You can also use &amp;ldquo;tillerless&amp;rdquo; Helm but that&amp;rsquo;s out of scope for this blog post.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Helm charts can be shared via a Helm Repository which gives you an easy way to upload, share, and download packages from a central location. The public &lt;a href=&#34;https://hub.helm.sh/&#34;&gt;Helm Hub&lt;/a&gt; has prebuilt Helm packages for most common open source applications.&lt;/p&gt;

&lt;h1 id=&#34;step-1-install-helm&#34;&gt;Step 1 - Install Helm&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.helm.sh/using_helm/#installing-helm&#34;&gt;Installing Helm&lt;/a&gt; is quite simple, if you&amp;rsquo;re on a Mac you can install Helm via Homebrew, otherwise check the &lt;a href=&#34;https://docs.helm.sh/using_helm/#installing-helm&#34;&gt;Helm install documentation&lt;/a&gt; for platform specific instructions:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;brew install kubernetes-helm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With the Helm client installed you can install the Tiller server to your Kubernetes cluster.&lt;/p&gt;

&lt;p&gt;If you are using minikube or a single tenant Kubernetes cluster without Role Based Authentication Control (RBAC) enabled you can deploy Tiller by simply running &lt;code&gt;helm init&lt;/code&gt;. On most clusters you should create a service account and role binding first like so:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;kubectl -n kube-system create serviceaccount tiller
kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller
helm init --service-account=tiller
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After a minute or so you can check it has finished installing by running &lt;code&gt;helm version&lt;/code&gt; which will give you the version of both the client and the server. If the server doesn&amp;rsquo;t respond just want a few more moments and try again:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm version
Client: &amp;amp;version.Version{SemVer:&amp;quot;v2.12.2&amp;quot;, GitCommit:&amp;quot;7d2b0c73d734f6586ed222a567c5d103fed435be&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;}
Server: &amp;amp;version.Version{SemVer:&amp;quot;v2.12.2&amp;quot;, GitCommit:&amp;quot;7d2b0c73d734f6586ed222a567c5d103fed435be&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;step-2-create-and-deploy-a-new-helm-chart&#34;&gt;Step 2 - Create and Deploy a new Helm Chart&lt;/h1&gt;

&lt;p&gt;Use the Helm client to create a new Helm chart:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm create petclinic
Creating petclinic

$ cd petclinic

$ tree
├── charts
├── Chart.yaml
├── templates
│   ├── deployment.yaml
│   ├── _helpers.tpl
│   ├── ingress.yaml
│   ├── NOTES.txt
│   ├── service.yaml
│   └── tests
│       └── test-connection.yaml
└── values.yaml

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can see that the Helm chart sets up a boilerplate Helm chart to get you started. This is actually a fully functioning chart so you can go ahead and deploy it right away to ensure that everything is working correctly. You could simply run &lt;code&gt;helm install .&lt;/code&gt; but generally you&amp;rsquo;ll want to specify a namespace and release name by using the &lt;code&gt;--namespace&lt;/code&gt; and &lt;code&gt;--name&lt;/code&gt; for better management.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm install --namespace test --name test .
NAME:   test
LAST DEPLOYED: Tue Feb  5 06:26:19 2019
NAMESPACE: test
STATUS: DEPLOYED
...
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can use &lt;code&gt;kubectl port-forward&lt;/code&gt; to test that everything worked right:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl --namespace test port-forward deployment/test-petclinic 8080:80
Visit http://127.0.0.1:8080 to use your application
Forwarding from [::1]:8080 -&amp;gt; 80
Forwarding from 127.0.0.1:8080 -&amp;gt; 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Point your browser at &lt;a href=&#34;http://localhost:8080&#34;&gt;http://localhost:8080&lt;/a&gt; and you should see the following:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./welcome-nginx.png&#34; alt=&#34;Nginx Welcome Screen&#34; /&gt;&lt;/p&gt;

&lt;p&gt;But this isn&amp;rsquo;t Petclinic it&amp;rsquo;s an empty nginx container. Since our petclinic app is pretty simple we can use this boilerplate chart to deploy Petclinic with just a few minor changes.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The nginx app inside the container listens on port 80, petclinic listens on port 8080 so you&amp;rsquo;ll need to edit the file &lt;code&gt;templates/deployment.yaml&lt;/code&gt; and find the YAML &lt;code&gt;containerPort: 80&lt;/code&gt; under the container spec and change the value to &lt;code&gt;8080&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Also in the &lt;code&gt;templates/deployment.yaml&lt;/code&gt; file you want to increase the initial timeout for the &lt;code&gt;livenessCheck&lt;/code&gt; check as Java takes longer to be ready than nginx. Find the YAML key &lt;code&gt;livenessProbe:&lt;/code&gt; and add the keypair &lt;code&gt;initialDelaySeconds: 60&lt;/code&gt; to it.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The updated section of the &lt;code&gt;templates/deployment.yaml&lt;/code&gt; should look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;    spec:
      containers:
        - name: {{ .Chart.Name }}
          image: &amp;quot;{{ .Values.image.repository }}:{{ .Values.image.tag }}&amp;quot;
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          livenessProbe:
            initialDelaySeconds: 60
            httpGet:
              path: /
              port: http

&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: the double curly brace &lt;code&gt;{{ }}&lt;/code&gt; signifies for the golang templating engine to process some code, usually print out some values, but can also be used for if/then statements and loops.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You also need to set the image and tag to petclinic. You could edit &lt;code&gt;values.yaml&lt;/code&gt; to change the defaults, but you can also override default values with &lt;code&gt;--set&lt;/code&gt;. Since we&amp;rsquo;ve already deployed the helm chart we can &lt;em&gt;upgrade&lt;/em&gt; it by running a &lt;code&gt;helm upgrade&lt;/code&gt; and providing the appropriate &lt;code&gt;--set&lt;/code&gt; flag like so:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: &lt;code&gt;helm upgrade&lt;/code&gt; helps you to manage the release lifecycle of your application, Tiller keeps track of the releases that you&amp;rsquo;ve deployed and helps you both upgrade and rollback deployments.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm upgrade test \
  --set image.repository=paulczar/spring-petclinic \
  --set image.tag=latest .
Release &amp;quot;test&amp;quot; has been upgraded. Happy Helming!
LAST DEPLOYED: Tue Feb  5 06:54:07 2019
...
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Forward port 8080 through to the deployment again and check its working via your web browser:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: the command shown in the &lt;code&gt;helm upgrade&lt;/code&gt; output is wrong because we&amp;rsquo;re using port 8080 now, you could fix the output by editing the file &lt;code&gt;templates/NOTES.txt&lt;/code&gt;, but for now just run the command below.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl --namespace test port-forward deployment/test-petclinic 8080
Forwarding from [::1]:8080 -&amp;gt; 8080
Forwarding from 127.0.0.1:8080 -&amp;gt; 8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./pc-localhost.png&#34; alt=&#34;petclinic&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;step-3-cleanup&#34;&gt;Step 3 - Cleanup&lt;/h1&gt;

&lt;p&gt;To uninstall the helm chart simple run the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm delete --purge test
release &amp;quot;test&amp;quot; deleted
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;step-4-optional-package-your-helm-chart&#34;&gt;Step 4 (optional) - Package your Helm Chart&lt;/h1&gt;

&lt;p&gt;Helm charts are designed to be packaged and shared via a Helm repository. Simply running &lt;code&gt;helm package .&lt;/code&gt; will create the Helm package like so:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ mkdir pkg
$ helm package -d pkg .
Successfully packaged chart and saved it to: /tmp/petclinic/petclinic-0.1.0.tgz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These packages combined with an index file can be hosted on any static website (github pages is very common) to create a Helm Repository. You can generate the index file with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm repo index pkg
$ cat pkg/index.yaml
apiVersion: v1
entries:
  petclinic:
  - apiVersion: v1
    appVersion: &amp;quot;1.0&amp;quot;
    created: 2019-02-06T02:45:10.99990157-06:00
    description: A Helm chart for Kubernetes
    digest: 86f3740e6bc325ea330428e42af091d6613ca9b92678b3aecdf680f0302b4685
    name: petclinic
    urls:
    - petclinic-0.1.0.tgz
    version: 0.1.0
generated: 2019-02-06T02:45:10.99955354-06:00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can use any basic web server as a Helm repository, many folks are using &lt;a href=&#34;https://github.com/int128/helm-github-pages&#34;&gt;github pages&lt;/a&gt; which is quite a clever way to host your Helm Repository right next to the actual code for your Helm chart.&lt;/p&gt;

&lt;h1 id=&#34;step-5-use-a-helm-repository&#34;&gt;Step 5 - Use a Helm Repository&lt;/h1&gt;

&lt;p&gt;I have created a more featureful Helm Chart designed specifically to generically run most Spring applications which can be found on my github repo &lt;a href=&#34;https://github.com/paulczar/helm-chart-spring&#34;&gt;helm-chart-spring&lt;/a&gt;. It installs petclinic by default and has options to be able to automatically set up LoadBalancers and Ingress as well as more advanced kubernetes resources such as &lt;code&gt;podDisruptionBudgets&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;You can install it straight from my Helm Repository like so:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm repo add paulczar http://tech.paulcz.net/helm-chart-spring/repo
&amp;quot;paulczar&amp;quot; has been added to your repositories

$ helm install --namespace test --name test \
    paulczar/spring --set service.type=LoadBalancer \
    --version 0.0.6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After a few moments you should be able to access the petclinic app via the LoadBalancer created by the above command.  Run &lt;code&gt;kubectl get svc&lt;/code&gt; to find the IP:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ k get svc
NAME          TYPE           CLUSTER-IP       EXTERNAL-IP      PORT(S)        AGE
test-spring   LoadBalancer   10.100.200.137   35.238.37.241   80:30509/TCP   89s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Point your browser at the &lt;code&gt;EXTERNAL-IP&lt;/code&gt; value:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./kube-lb-pc.png&#34; alt=&#34;petclinic&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Uninstall the Helm chart:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm delete --purge test
release &amp;quot;test&amp;quot; deleted
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Helm has made it incredibly easy to get started creating and customizing charts to install Applications on Kubernetes. By templating and packaging your Kubernetes manifests you get an easy to share package for your application and are able to enable a lot of deployment flexibility to deploy multiple releases of your application to one or many Kubernetes clusters and customize things like the service type giving you tremendous control over how people access it.&lt;/p&gt;

&lt;p&gt;Better yet, if you are looking to install an open source application you should first look to the &lt;a href=&#34;https://hub.helm.sh/&#34;&gt;Helm Hub&lt;/a&gt; which is a public repository of hundreds of pre-packaged helm charts that install with just a few keypresses.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cloud Native Operations - Kubernetes Controllers</title>
      <link>https://tech.paulcz.net/blog/cloud-native-operations-k8s-controllers/</link>
      <pubDate>Tue, 29 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/cloud-native-operations-k8s-controllers/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;Note: If this topic has peaked your interest, you can join me for a Webinar on August 15 where I&amp;rsquo;ll dive deep into &lt;a href=&#34;https://content.pivotal.io/webinars/aug-15-cloud-native-operations-with-kubernetes-and-ci-cd-webinar?utm_campaign=cno-k8s-ci-cd-q319&amp;amp;utm_source=blog&amp;amp;utm_medium=website&#34;&gt;Cloud Native Operations with Kubernetes and CI/CD Pipelines&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;lulz-what-cloud-native-operations&#34;&gt;Lulz what? Cloud Native Operations ?!?!?!&lt;/h1&gt;

&lt;p&gt;Historically Operations practices have lagged behind development. During the 90s a number of lightweight software development practices evolved such as Scrum and Extreme Programming. During the early 2000&amp;rsquo;s it became pretty common to practice (or at least claim to) some form of Agile in software development.&lt;/p&gt;

&lt;p&gt;It wasn&amp;rsquo;t until the last year of that decade that we started to see an uptick in Operations folks wanting to adopt Agile type methodologies and as the devops (and later SRE) movements took off we started to borrow heavily from Lean principals such as Kanban and Value Stream Mapping.&lt;/p&gt;

&lt;p&gt;Cloud Computing has brought about another shift in software development, going from large monolithic applications to collections of microservices that work together, and even further into being event based via messages and streams which now falls under the umbrella of &amp;ldquo;cloud native&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;With the rise of Kubernetes and similar platforms as well as companies like Hashicorp and stalwarts of Agile Operations such as Google and Pivotal we&amp;rsquo;re starting to see that same shift in Operations as we start to talk about &lt;strong&gt;Platform as Product&lt;/strong&gt; and turning engineering [operations] teams into product teams.&lt;/p&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;this means we all need to learn to get better at product engineering, kittens.  and turning infra engineering teams into infra product teams. &lt;a href=&#34;https://t.co/t1Vj6RKbdd&#34;&gt;pic.twitter.com/t1Vj6RKbdd&lt;/a&gt;&lt;/p&gt;&amp;mdash; Charity Majors (@mipsytipsy) &lt;a href=&#34;https://twitter.com/mipsytipsy/status/1088673759291011073?ref_src=twsrc%5Etfw&#34;&gt;January 25, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;h1 id=&#34;kubernetes-controllers&#34;&gt;Kubernetes Controllers&lt;/h1&gt;

&lt;p&gt;There&amp;rsquo;s a lot more to be said about Cloud Native Operations and Platform as Product (the two go hand-in-hand) but for now I want to focus on a fundamental aspect of Kubernetes that will be a force multiplier for making the composable building blocks of Cloud Native Operations.&lt;/p&gt;

&lt;p&gt;Most resources in Kubernetes are managed by a Controller. A Kubernetes Controller is to microservices what a Chef recipe is to a Monolith.&lt;/p&gt;

&lt;p&gt;Each resource is controlled by its own control loop. This is a step forward from previous systems like Chef or Puppet which both have control loops but at the server level, not the resource.&lt;/p&gt;

&lt;p&gt;A Controller is a fairly simple piece of code that creates a control loop over a single resource to ensure that resource is behaving correctly. These Control loops cam stack together to create complex functionality with simple interfaces.&lt;/p&gt;

&lt;p&gt;The canonical example of this in action is in how we manage pods in Kubernetes. A Pod is [effectively] a running copy of your application that a specific worker node is asked to run. If that application crashes the kubelet running on that node will start it again.&lt;/p&gt;

&lt;p&gt;However if that node crashes the Pod is not recovered as the control loop (via the kubelet process) responsible for the resource no longer exists. To make applications more resiliant Kubernetes has the ReplicaSet controller.&lt;/p&gt;

&lt;p&gt;Kubernetes has a process running on the masters called a &lt;code&gt;controller-manager&lt;/code&gt; that run the controllers for these more advanced resources. This is where the ReplicaSet controller runs, and it is responsible for ensuring that a set number of copies of your application are always running.&lt;/p&gt;

&lt;p&gt;To do this the ReplicaSet controller requests that the provided number of Pods are created and then it routinely checks that the correct number of Pods are still running and will request more pods, or destroy existing pods to do so.&lt;/p&gt;

&lt;p&gt;By requesting a ReplicaSet from Kubernetes you get a self-healing deployment of your application. You can further add lifecycle management to your workload by requesting a Deployment which is a controller that manages ReplicaSets.&lt;/p&gt;

&lt;p&gt;These Controllers are great for managing Kubernetes resources, but are also fantastic for managing resources outside of Kubernetes. You can extend Kubernetes by writing a Controller that watches for events and annotations and performs extra work, or by writing a Custom Resource Definition.&lt;/p&gt;

&lt;h1 id=&#34;example-external-dns-controller&#34;&gt;Example - External DNS Controller&lt;/h1&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/kubernetes-incubator/external-dns&#34;&gt;external-dns&lt;/a&gt; controller is a perfect example of a watcher. You configure it with your DNS provider and it will watch resources such as Services and Ingresses. When one of those resources changes it will inspect them for annotations which will tell it if it needs to perform an action.&lt;/p&gt;

&lt;p&gt;With the &lt;code&gt;external-dns&lt;/code&gt; controller running in your cluster you can simply add the following annotation to a service and it will go out and create a matching DNS A record for that resource:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;kubectl annotate service nginx \
    &amp;quot;external-dns.alpha.kubernetes.io/hostname=nginx.example.org.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can change other characteristics such as the TTL value of the DNS record:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;kubectl annotate service nginx \
    &amp;quot;external-dns.alpha.kubernetes.io/ttl=10&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Just like that you now have automatic DNS management for your applications and services in Kubernetes that reacts to any changes in your cluster to ensure your DNS is correct.&lt;/p&gt;

&lt;h1 id=&#34;example-certificate-manager-operator&#34;&gt;Example - Certificate Manager Operator&lt;/h1&gt;

&lt;p&gt;Like the &lt;code&gt;external-dns&lt;/code&gt; controller the &lt;a href=&#34;http://docs.cert-manager.io/en/latest/&#34;&gt;cert-manager&lt;/a&gt; will react to changes in resources, but also comes with a Custom Resource Definition that will allow you to request certificates as a resource in of themselves, not just a byproduct of an annotation.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;cert-manager&lt;/code&gt; works with &lt;a href=&#34;https://letsencrypt.org/&#34;&gt;Lets Encrypt&lt;/a&gt; and other sources of Certificates to request valid signed TLS certificates. You can even use it in combination with &lt;code&gt;external-dns&lt;/code&gt; like the following which will register &lt;code&gt;web.example.com&lt;/code&gt; and retrieve a TLS certificate from Lets Encrypt and store that in a Secret.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    certmanager.k8s.io/acme-http01-edit-in-place: &amp;quot;true&amp;quot;
    certmanager.k8s.io/cluster-issuer: letsencrypt-prod
    kubernetes.io/tls-acme: &amp;quot;true&amp;quot;
  name: example
spec:
  rules:
  - host: web.example.com
    http:
      paths:
      - backend:
          serviceName: example
          servicePort: 80
        path: /*
  tls:
  - hosts:
    - web.example.com
    secretName: example-tls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can also request a certificate directly from the &lt;code&gt;cert-manager&lt;/code&gt; CRD like so which like above will result in a certificate keypair being stored in a Kubernetes secret:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: certmanager.k8s.io/v1alpha1
kind: Certificate
metadata:
  name: example-com
  namespace: default
spec:
  secretName: example-com-tls
  issuerRef:
    name: letsencrypt-staging
  commonName: example.com
  dnsNames:
  - www.example.com
  acme:
    config:
    - http01:
        ingressClass: nginx
      domains:
      - example.com
    - http01:
        ingress: my-ingress
      domains:
      - www.example.com
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;This was just a quick look at one of the ways that Kubernetes is helping enable a new wave of changes to how we operate software. This is a favorite topic of mine, so look forward to hearing more.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Building Spring Docker Images</title>
      <link>https://tech.paulcz.net/blog/building-spring-docker-images/</link>
      <pubDate>Fri, 25 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/building-spring-docker-images/</guid>
      <description>

&lt;p&gt;While investigating running &lt;a href=&#34;https://spring.io&#34;&gt;Spring&lt;/a&gt; applications on Kubernetes I discovered that a lot of the existing example Spring applications do not have a &lt;code&gt;Dockerfile&lt;/code&gt; in their git repository. I thought this odd at first (and frankly still do).&lt;/p&gt;

&lt;p&gt;What I discovered though, is there&amp;rsquo;s quite a number of ways to build &lt;a href=&#34;https://spring.io/guides/gs/spring-boot-docker/&#34;&gt;Spring (and Java in general) container images&lt;/a&gt; that don&amp;rsquo;t necessarily rely on writing a Dockerfile.&lt;/p&gt;

&lt;p&gt;Full disclosure, I am a firm believe that any opensource project of consequence (where feasible) should ship a Dockerfile in their git repo, and ideally have images up on the Docker hub (or other public container registry) as it allows for newcomers to experience your application or project in just a few seconds with no need to play detective to try and figure out how to get it running.&lt;/p&gt;

&lt;p&gt;I will demonstrate building the &lt;a href=&#34;https://github.com/spring-projects/spring-petclinic&#34;&gt;Spring Pet Clinic example application&lt;/a&gt; into container images.&lt;/p&gt;

&lt;p&gt;If you want to follow along at home start by cloning down the repo to your local machine:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;git clone https://github.com/spring-projects/spring-petclinic.git

cd spring-petclinic
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;option-1-dockerfile&#34;&gt;Option 1 - Dockerfile&lt;/h1&gt;

&lt;p&gt;The Pet Clinic app uses Maven to build a .jar file, so we have a few options here.&lt;/p&gt;

&lt;h2 id=&#34;build-jar-and-then-copy-it-into-a-java-image&#34;&gt;Build .jar and then copy it into a Java Image&lt;/h2&gt;

&lt;p&gt;This assumes that you have a suitable version of Java and Maven on your system.&lt;/p&gt;

&lt;p&gt;Start by building the project into a .jar file with Maven:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ mvn install -DskipTests
[INFO] Installing /home/pczarkowski/development/demo/spring-into-kubernetes-1/spring-petclinic/target/spring-petclinic-2.1.0.BUILD-SNAPSHOT.jar to /home/pczarkowski/.m2/repository/org/springframework/samples/spring-petclinic/2.1.0.BUILD-SNAPSHOT/spring-petclinic-2.1.0.BUILD-SNAPSHOT.jar
[INFO] Installing /home/pczarkowski/development/demo/spring-into-kubernetes-1/spring-petclinic/pom.xml to /home/pczarkowski/.m2/repository/org/springframework/samples/spring-petclinic/2.1.0.BUILD-SNAPSHOT/spring-petclinic-2.1.0.BUILD-SNAPSHOT.pom
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 26.984 s
[INFO] Finished at: 2019-01-25T09:23:11-06:00
[INFO] ------------------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see this resulted in a Java file &lt;code&gt;spring-petclinic-2.1.0.BUILD-SNAPSHOT.jar&lt;/code&gt;.  We can create a Dockerfile to ingest this called &lt;code&gt;Dockerfile.cp&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;FROM openjdk:11.0.1-jre-slim-stretch
EXPOSE 8080
ARG JAR=spring-petclinic-2.1.0.BUILD-SNAPSHOT.jar
COPY target/$JAR /app.jar
ENTRYPOINT [&amp;quot;java&amp;quot;,&amp;quot;-jar&amp;quot;,&amp;quot;/app.jar&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: because we already built the Jar we only need a slim JRE image to run it in. We can also use an ARG for the file name in case we need to change it on build with &lt;code&gt;--build-arg JAR=...&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A simple &lt;code&gt;docker build&lt;/code&gt; command should create us an image we can run:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;docker build -f ./Dockerfile.cp -t spring/petclinic .
Sending build context to Docker daemon  98.22MB
Step 1/5 : FROM openjdk:11.0.1-jre-slim-stretch
11.0.1-jre-slim-stretch: Pulling from library/openjdk
5e6ec7f28fb7: Pull complete
1cf4e4a3f534: Pull complete
5d9d21aca480: Pull complete
0a126fb8ec28: Pull complete
1904df324545: Pull complete
e6d9d96381c8: Pull complete
Digest: sha256:965a07951bee0c3b1f8aff4818619ace3e675d91cfb746895e8fb84e3e6b13ca
Status: Downloaded newer image for openjdk:11.0.1-jre-slim-stretch
 ---&amp;gt; 49b31a72a85a
Step 2/5 : EXPOSE 8080
 ---&amp;gt; Running in 1aeaae727a80
Removing intermediate container 1aeaae727a80
 ---&amp;gt; a1a1850f8e8f
Step 3/5 : ARG JAR=spring-petclinic-2.1.0.BUILD-SNAPSHOT.jar
 ---&amp;gt; Running in b6faa7c0faa3
Removing intermediate container b6faa7c0faa3
 ---&amp;gt; 2b55681ac9df
Step 4/5 : COPY target/$JAR /app.jar
 ---&amp;gt; dec4f0d56c9d
Step 5/5 : ENTRYPOINT [&amp;quot;java&amp;quot;,&amp;quot;-jar&amp;quot;,&amp;quot;/app.jar&amp;quot;]
 ---&amp;gt; Running in f492e1668fff
Removing intermediate container f492e1668fff
 ---&amp;gt; f669afd61b8d
Successfully built f669afd61b8d
Successfully tagged spring/petclinic:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Start the new container, wait a minute or so (you can watch the logs with &lt;code&gt;docker logs -f petclinic&lt;/code&gt; if you want), and then test it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d --name petclinic -p 8080:8080 spring/petclinic
a1d51b6f9a47501dfe90f24866e7fb6c82e436323fa4adc09074e8ac7447a1a7

$ curl -s localhost:8080 | head
&amp;lt;!DOCTYPE html&amp;gt;

&amp;lt;html&amp;gt;

  &amp;lt;head&amp;gt;

    &amp;lt;meta http-equiv=&amp;quot;Content-Type&amp;quot; content=&amp;quot;text/html; charset=UTF-8&amp;quot;/&amp;gt;
    &amp;lt;meta charset=&amp;quot;utf-8&amp;quot;&amp;gt;
    &amp;lt;meta http-equiv=&amp;quot;X-UA-Compatible&amp;quot; content=&amp;quot;IE=edge&amp;quot;&amp;gt;
    &amp;lt;meta name=&amp;quot;viewport&amp;quot; content=&amp;quot;width=device-width, initial-scale=1&amp;quot;&amp;gt;

$ docker rm -f petclinic
petclinic
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can look at the resultant image size using &lt;code&gt;docker images&lt;/code&gt;, if you want to dive deeper you can also use &lt;code&gt;docker inspect&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ docker images spring/petclinic
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
spring/petclinic    latest              f669afd61b8d        41 minutes ago      318MB
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;use-a-multi-stage-dockerfile&#34;&gt;Use a multi-stage Dockerfile&lt;/h2&gt;

&lt;p&gt;If you don&amp;rsquo;t have Java and Maven on your system, or you want to delegate the whole thing to Docker you can utilize a multi-stage Dockerfile to build the .jar file and then copy it into a slim image.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-dockerfile&#34;&gt;FROM maven:3.6-jdk-11-slim as BUILD
COPY . /src
WORKDIR /src
RUN mvn install -DskipTests

FROM openjdk:11.0.1-jre-slim-stretch
EXPOSE 8080
WORKDIR /app
ARG JAR=spring-petclinic-2.1.0.BUILD-SNAPSHOT.jar

COPY --from=BUILD /src/target/$JAR /app.jar
ENTRYPOINT [&amp;quot;java&amp;quot;,&amp;quot;-jar&amp;quot;,&amp;quot;/app.jar&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Like before we can use &lt;code&gt;docker build&lt;/code&gt; to build this image, but unlike before we don&amp;rsquo;t need Java or Maven installed locally:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ docker build -f ./Dockerfile.multi -t spring/petclinic .
...
...
Successfully built ee062471d65c
Successfully tagged spring/petclinic:latest

REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
spring/petclinic    latest              ee062471d65c        18 minutes ago      318MB
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you&amp;rsquo;d expect the Docker Image size is the same as the previous build given we effectively did the same thing, build the Jar and then Copy it into a slim image.&lt;/p&gt;

&lt;h1 id=&#34;option-2-google-jib&#34;&gt;Option 2 - Google JIB&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/GoogleContainerTools/jib&#34;&gt;Jib&lt;/a&gt; builds optimized Docker and OCI images for your Java applications without a Docker daemon - and without deep mastery of Docker best-practices. It is available as plugins for Maven and Gradle and as a Java library.&lt;/p&gt;

&lt;p&gt;Normally you&amp;rsquo;d add JIB to your maven build via the pom.xml [as shown here].(&lt;a href=&#34;https://github.com/GoogleContainerTools/jib/tree/master/jib-maven-plugin#setup&#34;&gt;https://github.com/GoogleContainerTools/jib/tree/master/jib-maven-plugin#setup&lt;/a&gt;), To kick the tires we can just pass some extra arguments to maven and get the same result.&lt;/p&gt;

&lt;p&gt;You can build your image with JIB (you don&amp;rsquo;t even need Docker running!) and ship it straight up to the docker registry by running the following:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: In this example I am using my docker registry username in the image name so that it is uploaded correctly, you&amp;rsquo;ll want to swap out &lt;code&gt;paulczar&lt;/code&gt; for your own username.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ mvn compile com.google.cloud.tools:jib-maven-plugin:1.0.0:build -Dimage=paulczar/petclinic:jib -DskipTests
[INFO] Containerizing application to paulczar/petclinic:jib...
[WARNING] Base image &#39;gcr.io/distroless/java&#39; does not use a specific image digest - build may not be reproducible
[INFO]
[INFO] Container entrypoint set to [java, -cp, /app/resources:/app/classes:/app/libs/*, org.springframework.samples.petclinic.PetClinicApplication]
[INFO]
[INFO] Built and pushed image as paulczar/petclinic:jib
[INFO] Executing tasks:
[INFO] [==============================] 100.0% complete
[INFO]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 57.478 s
[INFO] Finished at: 2019-01-25T11:03:25-06:00
[INFO] ------------------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: This provides a warning &lt;code&gt;build may not be reproducible&lt;/code&gt;. You can pass an argument to use your own base Java image to make it more deterministic by adding &lt;code&gt;-Djib.from.image=openjdk:11.0.1-jre-slim-stretch&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If you want JIB to build against your local docker install and not push the image to the registry you can run the following:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ mvn compile com.google.cloud.tools:jib-maven-plugin:1.0.0:dockerBuild
...
...
[INFO] Built image to Docker daemon as spring-petclinic:2.1.0.BUILD-SNAPSHOT
[INFO] Executing tasks:
[INFO] [==============================] 100.0% complete
[INFO]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 10.485 s
[INFO] Finished at: 2019-01-25T11:14:21-06:00
[INFO] ------------------------------------------------------------------------

docker images spring-petclinic:2.1.0.BUILD-SNAPSHOT
REPOSITORY          TAG                    IMAGE ID            CREATED             SIZE
spring-petclinic    2.1.0.BUILD-SNAPSHOT   79d677deeedb        49 years ago        164MB
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: This image is much smaller than the rest, this is because by default JIB creates a distroless Java image. This might seem like a good idea for the size, but will like the warning from the previous build give you an image that may not be reproducable. I recommend always using the &lt;code&gt;-Djib.from.image=openjdk:11.0.1-jre-slim-stretch&lt;/code&gt; argument to choose your upstream Java image which will again give you a 318Mb image like the previous builds.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;others&#34;&gt;Others&lt;/h1&gt;

&lt;p&gt;I&amp;rsquo;ve shown you what I believe are the best methods for building a Docker image for your Spring application. There are some other maven plugins that do the same thing:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/spotify/dockerfile-maven&#34;&gt;Spotify/dockerfile-maven&lt;/a&gt; builds a Jar and then uses a user provided Dockerfile to copy it in.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/spotify/docker-maven-plugin&#34;&gt;spotify/docker-maven&lt;/a&gt; builds the whole image for you much like JIB.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/fabric8io/docker-maven-plugin&#34;&gt;fabricate/docker-maven&lt;/a&gt; also builds the whole image like JIB.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Hopefully after reading this you have a better idea how to build Docker images for your Spring (or general Java) Application. Personally I prefer the multi-stage Dockerfile as your Dockerfile becomes the contract on how your image is built, however I do really like the way i can use JIB to build an image without needing Docker as this simplifies my build environment and means I can very easily use tools like &lt;a href=&#34;https://travis-ci.org&#34;&gt;Travis CI&lt;/a&gt; or &lt;a href=&#34;https://drone.io&#34;&gt;Drone&lt;/a&gt; to build my images for me.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Spring into Kubernetes - Part 1</title>
      <link>https://tech.paulcz.net/blog/spring-into-kubernetes-part-1/</link>
      <pubDate>Thu, 24 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/spring-into-kubernetes-part-1/</guid>
      <description>

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Welcome to the first part in a series of blog posts looking at running Spring applications on Kubernetes. To kick the series off we&amp;rsquo;re going to take a look at the &lt;a href=&#34;https://github.com/spring-projects/spring-petclinic&#34;&gt;Spring Pet Clinic&lt;/a&gt; example application and demonstrate how we can quickly and easily get it running on Kubernetes.&lt;/p&gt;

&lt;h1 id=&#34;step-1-install-a-kubernetes&#34;&gt;Step 1.  Install a Kubernetes&lt;/h1&gt;

&lt;p&gt;Before we get started I would encourage you to read (or at least skim) through Kelsey Hightower&amp;rsquo;s seminal &lt;a href=&#34;https://github.com/kelseyhightower/kubernetes-the-hard-way&#34;&gt;Kubernetes The Hard Way&lt;/a&gt; to get an idea for the complexity of installing and operating a Kubernetes cluster.&lt;/p&gt;

&lt;p&gt;Once you&amp;rsquo;ve done that you should be thoroughly discouraged from installing Kubernetes. That&amp;rsquo;s a good thing! Most people should have no need to run their own production grade Kubernetes cluster. Every major public cloud has a Kubernetes service, and Pivotal provides the &lt;a href=&#34;https://pivotal.io/platform/pivotal-container-service&#34;&gt;Pivotal Container Service&lt;/a&gt; to ease the burden of installing and managing Kubernetes in your own Datacenter.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ll be using &lt;a href=&#34;https://pivotal.io/platform/pivotal-container-service&#34;&gt;Pivotal Container Service&lt;/a&gt;. You should be able to use any flavor of Kubernetes to follow along with this series of blog posts. The simplest way to do so is to probably install and run &lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-minikube/&#34;&gt;Minikube&lt;/a&gt; which will give you a single node Kubernetes cluster running in a VM on your laptop.&lt;/p&gt;

&lt;h1 id=&#34;step-2-validate-your-kubernetes-cluster-is-ready&#34;&gt;Step 2. Validate your Kubernetes cluster is ready&lt;/h1&gt;

&lt;p&gt;However you got your Kubernetes cluster is between you and your deity of choice and I won&amp;rsquo;t ask any questions. You should have received [or had one automatically created] a Kubernetes config file and you should have downloaded the &lt;code&gt;kubectl&lt;/code&gt; command line tool.&lt;/p&gt;

&lt;p&gt;Run the following commands to ensure that your Kubernetes config and cluster are working correctly:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl version
kubClient Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;13&amp;quot;, GitVersion:&amp;quot;v1.13.2&amp;quot;, GitCommit:&amp;quot;cff46ab41ff0bb44d8584413b598ad8360ec1def&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, BuildDate:&amp;quot;2019-01-10T23:35:51Z&amp;quot;, GoVersion:&amp;quot;go1.11.4&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;linux/amd64&amp;quot;}
Server Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;12&amp;quot;, GitVersion:&amp;quot;v1.12.4&amp;quot;, GitCommit:&amp;quot;f49fa022dbe63faafd0da106ef7e05a29721d3f1&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, BuildDate:&amp;quot;2018-12-14T06:59:37Z&amp;quot;, GoVersion:&amp;quot;go1.10.4&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;linux/amd64&amp;quot;}

$ kubectl get nodes
NAME                                      STATUS   ROLES    AGE     VERSION
vm-16ff7fc4-88fa-406d-5f6a-7ccc92286c08   Ready    &amp;lt;none&amp;gt;   2d11h   v1.12.4
vm-5bfb8f6e-d715-45ba-77df-047447995ac1   Ready    &amp;lt;none&amp;gt;   2d11h   v1.12.4
vm-c2c16e34-1a73-4a78-50b4-bdec12d3de77   Ready    &amp;lt;none&amp;gt;   2d11h   v1.12.4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;looking great! Create a namespace to work in that you can delete when finished to easily clean up after the demo.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl create namespace spring-into-kubernetes-1
namespace/spring-into-kubernetes-1 created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Update your Kubernetes config to use this new namespace by default:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl config set-context $(kubectl config current-context) --namespace=spring-into-kubernetes-1
Context &amp;quot;cluster1&amp;quot; modified.
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;step-3-build-a-docker-image-for-spring-pet-clinic&#34;&gt;Step 3. Build a Docker image for Spring Pet Clinic&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: If you are using minikube you can use the &lt;a href=&#34;https://github.com/kubernetes/minikube/blob/master/docs/reusing_the_docker_daemon.md&#34;&gt;minikube docker socket&lt;/a&gt; and skip pushing the image up to a registry.&lt;/p&gt;

&lt;p&gt;Note: If you don&amp;rsquo;t want to build your own image you can use the one that I&amp;rsquo;ve already built &lt;code&gt;paulczar/petclinic:spring-k8s-1&lt;/code&gt; and skip straight to running it.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Use &lt;code&gt;git&lt;/code&gt; to clone down the Spring Pet Clinic git repo locally:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ git clone https://github.com/spring-projects/spring-petclinic.git
Cloning into &#39;spring-petclinic&#39;...
remote: Enumerating objects: 7860, done.
remote: Total 7860 (delta 0), reused 0 (delta 0), pack-reused 7860
Receiving objects: 100% (7860/7860), 6.99 MiB | 14.57 MiB/s, done.
Resolving deltas: 100% (2908/2908), done.

$ cd spring-petclinic
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ordinarily you&amp;rsquo;d use &lt;code&gt;docker build&lt;/code&gt; to create a docker image, however this repo does not have a &lt;code&gt;Dockerfile&lt;/code&gt;, so thankfully we have some less obvious ways through maven to do this using tools like the &lt;a href=&#34;https://github.com/GoogleContainerTools/jib&#34;&gt;Google JIB project&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: For more ways to build Docker images for Spring Applications see the official &lt;a href=&#34;https://spring.io/guides/gs/spring-boot-docker/&#34;&gt;Spring documentation&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ mvn compile -Dimage=spring/petclinic:spring-k8s-1 \
    com.google.cloud.tools:jib-maven-plugin:1.0.0:dockerBuild
...
...
[INFO] Built image to Docker daemon as spring-petclinic:2.1.0.BUILD-SNAPSHOT
[INFO] Executing tasks:
[INFO] [==============================] 100.0% complete
[INFO]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 20.561 s
[INFO] Finished at: 2019-01-24T08:56:30-06:00
[INFO] ------------------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If like me you don&amp;rsquo;t have &lt;code&gt;java&lt;/code&gt; installed on your local desktop you can cheat your way through by mapping your docker socket through to a maven container like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ docker run -ti --rm --workdir /src -v $(pwd):/src \
    -v /var/run/docker.sock:/var/run/docker.sock \
    maven:3.6-jdk-11 mvn compile -Dimage=spring/petclinic:spring-k8s-1 \
    com.google.cloud.tools:jib-maven-plugin:1.0.0:dockerBuild
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will have built an image called &lt;code&gt;spring/petclinic:spring-k8s-1&lt;/code&gt;. Verify it:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ docker images spring/petclinic:spring-k8s-1
REPOSITORY          TAG                    IMAGE ID            CREATED             SIZE
spring/petclinic    spring-k8s-1   6afb36ee7749        15 seconds ago        164MB
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tag this image for your Docker registry username (anywhere you see my registry username you should swap it for yours) and push it up:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ docker tag spring/petclinic:spring-k8s-1 paulczar/petclinic:spring-k8s-1
$ docker push paulczar/petclinic:spring-k8s-1
The push refers to repository [docker.io/paulczar/petclinic]
1f5a2dd7582f: Layer already exists
6e01ddadb469: Pushed
185699264bb4: Layer already exists
5b0bbc8b30cc: Pushed
6189abe095d5: Pushed
c5204564c844: Pushed
spring-k8s-1: digest: sha256:0a71768b0a3b199b6ec78dff983f26c4103e6089ed50c17985b58c120c3aaf72 size: 1581

&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;step-4-optional-validate-the-docker-image-works-locally&#34;&gt;Step 4. (optional) Validate the Docker Image works locally&lt;/h1&gt;

&lt;p&gt;It&amp;rsquo;s usually a good idea to validate things locally in Docker before moving ahead, so go ahead and do that by running Spring Petclinic via your local docker daemon:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: We&amp;rsquo;re running docker with the &lt;code&gt;-ti&lt;/code&gt; flag to keep it in the foreground, &lt;code&gt;--rm&lt;/code&gt; to delete the container when done and &lt;code&gt;-p&lt;/code&gt; to map a port from localhost.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ docker run -ti --rm -p 8080:8080 paulczar/petclinic:spring-k8s-1
...
...
2019-01-24 15:07:35.928  INFO 1 --- [  restartedMain] o.s.b.a.e.web.EndpointLinksResolver      : Exposing 15 endpoint(s) beneath base path &#39;/manage&#39;
2019-01-24 15:07:36.057  INFO 1 --- [  restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 8080 (http) with context path &#39;&#39;
2019-01-24 15:07:36.061  INFO 1 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 7.151 seconds (JVM running for 7.522)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Validate it works by pointint your web browser at localhost port 8080:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./pc-localhost.png&#34; alt=&#34;Spring Pet Clinic via Docker&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Stop and delete your running container by hitting &lt;code&gt;control c&lt;/code&gt; in the terminal running the docker image.&lt;/p&gt;

&lt;h1 id=&#34;step-5-run-pet-clinic-in-kubernetes&#34;&gt;Step 5. Run Pet Clinic in Kubernetes&lt;/h1&gt;

&lt;p&gt;There are two main ways of interacting with Kubernetes. &lt;strong&gt;declarative&lt;/strong&gt; and &lt;strong&gt;imperative&lt;/strong&gt;. With declarative you run &lt;code&gt;kubectl apply&lt;/code&gt; with a local copy of the Kubernetes manifest and let Kubernetes determine how to ensure the running resources matches. When you use Kubernetes imperatively you give it more precise commands like &lt;code&gt;kubectl create&lt;/code&gt; and &lt;code&gt;kubectl run&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Most advanced users of Kubernetes use the declarative methods, but the imperative are perfect for when you&amp;rsquo;re just getting started or want to do something quickly.&lt;/p&gt;

&lt;p&gt;Create a Kubernetes deployment by running the &lt;code&gt;kubectl create&lt;/code&gt; command and then validate it with &lt;code&gt;kubectl get all&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl create deployment petclinic --image=paulczar/petclinic:spring-k8s-1
deployment.apps/petclinic created

$ kubectl get all
NAME                             READY   STATUS    RESTARTS   AGE
pod/petclinic-5ffccf75c4-snhdd   1/1     Running   0          36s

NAME                        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/petclinic   1         1         1            1           36s

NAME                                   DESIRED   CURRENT   READY   AGE
replicaset.apps/petclinic-5ffccf75c4   1         1         1       36s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can see that Kubernetes has created several resources. It created a &lt;strong&gt;Deployment&lt;/strong&gt; which is reconciled by a &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/&#34;&gt;Kubernetes controller&lt;/a&gt; for managing the lifecycle of &lt;strong&gt;ReplicaSets&lt;/strong&gt;. The &lt;strong&gt;Deployment&lt;/strong&gt; created the &lt;strong&gt;ReplicaSet&lt;/strong&gt; which has a controller for ensuring a set number of replicas of your application is running. The &lt;strong&gt;ReplicaSet&lt;/strong&gt; created a &lt;strong&gt;Pod&lt;/strong&gt; which is your running application.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/&#34;&gt;Controllers&lt;/a&gt; are a major part of what makes Kubernetes so good. They create tight control loops around resources to add more complex functionality as described above.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You might notice that the Kubernetes output above does not give you an IP address or any hints as to how to access your application. This is because we need to expose the application using a &lt;strong&gt;Service&lt;/strong&gt; resource.&lt;/p&gt;

&lt;h1 id=&#34;step-6-expose-your-application-to-the-internet&#34;&gt;Step 6. Expose your application to the Internet&lt;/h1&gt;

&lt;p&gt;If you don&amp;rsquo;t care about making your application available to the internet and just want to validate that it works you can run &lt;code&gt;kubectl port-forward&lt;/code&gt; and access your application via a &lt;code&gt;localhost&lt;/code&gt; port forward like so:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;kubectl port-forward deployment/petclinic 8080:8080
Forwarding from 127.0.0.1:8080 -&amp;gt; 8080
Forwarding from [::1]:8080 -&amp;gt; 8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Access the petclinic app just like we did earlier by pointing your browser at localhost:8080:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./pc-localhost.png&#34; alt=&#34;Spring Pet Clinic via Docker&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Stop the port forward with a &lt;code&gt;control-C&lt;/code&gt; on in the terminal.&lt;/p&gt;

&lt;p&gt;Use the &lt;code&gt;kubectl expose&lt;/code&gt; command to create a LoadBalancer service in Kubernetes and then validate it with &lt;code&gt;kubectl get services&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl expose deployment petclinic --type=LoadBalancer --port 80 --target-port 8080
service/petclinic exposed

$ kubectl get service
NAME        TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
petclinic   LoadBalancer   10.100.200.110   &amp;lt;pending&amp;gt;     80:31148/TCP   15s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After a few minutes the &lt;code&gt;EXTERNAL-IP&lt;/code&gt; field should go from &lt;code&gt;&amp;lt;pending&amp;gt;&lt;/code&gt; to having an IP address:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl get service
NAME        TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
petclinic   LoadBalancer   10.100.200.110   35.238.37.241    80:31148/TCP   15s
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: If you are running minikube you won&amp;rsquo;t get a real loadbalancer and can run &lt;code&gt;minikube service example-service --url&lt;/code&gt; to get an IP/Port combo that should work.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Point your web browser at that IP address on port 80:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./kube-lb-pc.png&#34; alt=&#34;Spring Pet Clinic via Docker&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;step-7-cleanup&#34;&gt;Step 7. Cleanup&lt;/h1&gt;

&lt;p&gt;Delete the service and deployment by running the &lt;code&gt;kubectl delete&lt;/code&gt; command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ k8s-platform-ops kubectl delete deployment petclinic
deployment.extensions &amp;quot;petclinic&amp;quot; deleted
$ k8s-platform-ops kubectl delete service petclinic
$ k8s-platform-ops kubectl get all
No resources found.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Delete your namespace and switch back to the &lt;code&gt;default&lt;/code&gt; namespace:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl delete namespace spring-into-kubernetes-1
namespace &amp;quot;spring-into-kubernetes-1&amp;quot; deleted

$ kubectl config set-context $(kubectl config current-context) --namespace=default
Context &amp;quot;cluster1&amp;quot; modified.
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Congratulations! You&amp;rsquo;ve successfully deployed a basic Spring application to Kubernetes and it wasn&amp;rsquo;t even all that hard.&lt;/p&gt;

&lt;p&gt;Using Kubernetes imperatively like this is a great way to get started and easily demonstrate running an application on Kubernetes. Of course there&amp;rsquo;s many more things to take into consideration if you want to run an application in production on Kubernetes and we&amp;rsquo;ll explore some of those things in future installments of Spring Into Kubernetes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The future of Kubernetes is Virtual Machines</title>
      <link>https://tech.paulcz.net/blog/future-of-kubernetes-is-virtual-machines/</link>
      <pubDate>Tue, 25 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/future-of-kubernetes-is-virtual-machines/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;Note: If this topic has peaked your interest, you can join me for a Webinar on August 15 where I&amp;rsquo;ll dive deep into &lt;a href=&#34;https://content.pivotal.io/webinars/aug-15-cloud-native-operations-with-kubernetes-and-ci-cd-webinar?utm_campaign=cno-k8s-ci-cd-q319&amp;amp;utm_source=blog&amp;amp;utm_medium=website&#34;&gt;Cloud Native Operations with Kubernetes and CI/CD Pipelines&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;peering-into-the-crystal-ball&#34;&gt;Peering into the crystal ball&lt;/h2&gt;

&lt;p&gt;Kubernetes as a technology has been very important to my career this year, and will be even more so for next year too. As 2018 comes to a close its time to drag out the hubris and make a bold prediction. The future of Kubernetes is Virtual Machines, not Containers.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;The future of Kubernetes is Virtual Machines, not Containers.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The Chinese Zodiac calls 2018 the year of the Dog, in tech it was the year of the Kubernetes. There are plenty of people only now learning about Kubernetes. CIOs everywhere are hard at work trying to develop a &amp;ldquo;Kubernetes Strategy&amp;rdquo; [1]. Some organizations are already running large production workloads on Kubernetes.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;[1] If you&amp;rsquo;re trying to develop a Kubernetes strategy you&amp;rsquo;ve already failed, but that&amp;rsquo;s a different blog post.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In other words there are plenty of people at each stage of the Gartner Hype Cycle for Kubernetes. Many are stuck in the trough of disillusionment or have drowned in the pit of despair.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./hype-cycle.png&#34; alt=&#34;Gartner Hype Cycle&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Jeremykemp at &lt;a href=&#34;https://commons.wikimedia.org/wiki/File:Gartner_Hype_Cycle.svg&#34;&gt;Wikipedia&lt;/a&gt; &lt;a href=&#34;https://creativecommons.org/licenses/by-sa/3.0&#34;&gt;Creative Commons CC BY-SA 3.0&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I am a big fan of containers and I&amp;rsquo;m not going to try to suggest that &lt;a href=&#34;https://chrisshort.net/docker-inc-is-dead/&#34;&gt;containers are dead&lt;/a&gt;. What Docker brought us in 2013 was a wrapper around Linux Containers. They showed us an amazing new way to build, package, share, and deploy applications.  This came at exactly the right time as we had started to get serious about Continuous Delivery.  Their model was perfect for the modern delivery pipeline and the emergence of PaaS and later CaaS platforms.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;cloud-native-ops-pipeline.png&#34; alt=&#34;Cloud Native Operations Pipeline&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Engineers working at Google saw the technology community was finally ready for containers. Google has been using (and more or less invented) containers for a very long time already. They started to build Kubernetes which as we all know by now is a re-imagining of Google&amp;rsquo;s own Borg platform built in the open as a community effort.&lt;/p&gt;

&lt;p&gt;It didn&amp;rsquo;t take long for the big public clouds to provide a Kubernetes based platform (GKE, AKS, EKS).  The on premise folks were also quick to build platforms based on Kubernetes as well (Pivotal Container Service, Openshift, etc).&lt;/p&gt;

&lt;h2 id=&#34;soft-and-squishy-multi-tenancy&#34;&gt;Soft and Squishy Multi-Tenancy&lt;/h2&gt;

&lt;p&gt;There&amp;rsquo;s one sticky problem that&amp;rsquo;s left to solve, and this I believe will prove to be the downfall of the container &amp;hellip;  multi-tenancy.&lt;/p&gt;

&lt;p&gt;Linux containers were not built to be secure isolated sandboxes (like Solaris Zones or FreeBSD Jails). Instead they&amp;rsquo;re built upon a shared kernel model that utilizes kernel features to provide basic process isolation. As &lt;a href=&#34;https://blog.jessfraz.com/post/containers-zones-jails-vms/&#34;&gt;Jessie Frazelle&lt;/a&gt; would say &amp;ldquo;Containers aren&amp;rsquo;t a real thing&amp;rdquo;.&lt;/p&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;containers aren&amp;#39;t things&lt;/p&gt;&amp;mdash; Jessie Frazelle (@jessfraz) &lt;a href=&#34;https://twitter.com/jessfraz/status/1015407561187655680?ref_src=twsrc%5Etfw&#34;&gt;July 7, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;Compounding this is the fact that most Kubernetes components are not Tenant aware. Sure you have &lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/&#34;&gt;Namespaces&lt;/a&gt; and &lt;a href=&#34;https://kubernetes.io/docs/concepts/policy/pod-security-policy/&#34;&gt;Pod Security Policies&lt;/a&gt; but the API itself is not. Nor are the internal components like the &lt;code&gt;kubelet&lt;/code&gt; or &lt;code&gt;kube-proxy&lt;/code&gt;. This leads to Kubernetes having a &amp;ldquo;Soft Tenancy&amp;rdquo; model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./k8s-arch.png&#34; alt=&#34;Kubernetes Architecture&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Abstractions leak. A platform built on top of containers will inherit many of the soft tenancy aspects of containers. Platforms built on top of hard multi-tenancy Virtual Machines all inherit that hard tenancy (VMware, Amazon Web Services, OpenStack, etc).&lt;/p&gt;

&lt;h2 id=&#34;kubesprawl-rules-everything-around-me&#34;&gt;Kubesprawl Rules Everything Around Me&lt;/h2&gt;

&lt;p&gt;Kubernetes&amp;rsquo; soft tenancy model leaves service providers and distributions in a weird place. The Kubernetes cluster itself becomes the line of &amp;ldquo;Hard Tenanacy&amp;rdquo;. There are many reasons (even inside the same organisation) to require hard tenancy between users (or appplications). Since the public clouds provide fully managed Kubernetes as a Service offerings its easy enough for each Tenant to get their own cluster and use the cluster boundary as the isolation point.&lt;/p&gt;

&lt;p&gt;Some Kubernetes distributions like &lt;a href=&#34;https://pivotal.io/platform/pivotal-container-service&#34;&gt;Pivotal Container Service (PKS)&lt;/a&gt; are very aware of this &lt;a href=&#34;https://content.pivotal.io/youtube-uberflip/kubernetes-one-cluster-or-many-3&#34;&gt;tenancy issue&lt;/a&gt; and have taken a similar model by providing that same Kubernetes as a Service experience you&amp;rsquo;d get from a public cloud but in your own datacenter.&lt;/p&gt;

&lt;p&gt;This leads to the emerging pattern of &amp;ldquo;many clusters&amp;rdquo; rather than &amp;ldquo;one big shared&amp;rdquo; cluster. Its not uncommon to see customers of Google&amp;rsquo;s GKE Service have dozens of Kubernetes clusters deployed for multiple teams. Often each developer gets their own cluster. This kind of behavior leads to a shocking amount of Kubesprawl.&lt;/p&gt;

&lt;p&gt;Alternatively Kubernetes operators running self Kubernetes clusters (either upstream or distribution based) in their own datacenters are left to take on the extra work of managing lots of clusters on their own, or to accept the soft tenancy on just one or two larger clusters.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;This kind of behavior leads to a shocking amount of Kubesprawl&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Generally the smallest cluster you get is 4 machines (or VMs). One (or 3 for HA) for the Kubernetes Master, three for the Kubernetes Workers. This ties up a ton of money in systems that are for the most part just sitting there idle.&lt;/p&gt;

&lt;p&gt;So we need to somehow move Kubernetes to a hard-tenancy model. The Kubernetes community is very aware of this need and has a &lt;a href=&#34;https://groups.google.com/forum/#!forum/kubernetes-wg-multitenancy&#34;&gt;multi-tenancy working group&lt;/a&gt;. This group has working hard on that problem and they have several suggested models and proposals on how to solve each model.&lt;/p&gt;

&lt;p&gt;Jessie Frazelle wrote a &lt;a href=&#34;https://blog.jessfraz.com/post/hard-multi-tenancy-in-kubernetes/&#34;&gt;blog post&lt;/a&gt;  about this exact topic which is great because she&amp;rsquo;s much smarter than me so I can link you to her and save myself about ten years of hard study trying to catch up to her. If you have not read it, stop reading here and go read it first.&lt;/p&gt;

&lt;h2 id=&#34;just-make-really-small-vms-optimized-for-speed&#34;&gt;Just make really small VMs optimized for speed&amp;hellip;&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Kata Containers is an open source project and community working to build a standard implementation of lightweight Virtual Machines (VMs) that feel and perform like containers, but provide the workload isolation and security advantages of VMs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Jessie suggests using a VM Container technology such as &lt;a href=&#34;https://katacontainers.io/&#34;&gt;Kata Containers&lt;/a&gt;. Kata Containers combine Virtual Machine level isolation that act and perform like Containers. This allows Kubernetes to give each Tenant (we&amp;rsquo;ll assume a tenant per namespace) its own set of Kubernetes system services running in nested VM Containers (A VM Container running inside a VM provided by the underlying IaaS).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./jessie-tenancy1.png&#34; alt=&#34;Hard Tenancy Kubernetes&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;image from &lt;a href=&#34;https://blog.jessfraz.com/post/hard-multi-tenancy-in-kubernetes/&#34;&gt;Jessie Frazelle - Hard Multi-Tenancy in Kubernetes&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is an elegant solutions to Kubernetes multi-tenancy. Her suggestion goes even further to suggest that Kubernetes use nested Container VMs for the workloads (Pods) running on Kubernetes which provides a significant increase of resource utilization.&lt;/p&gt;

&lt;p&gt;We have at least one more optimization to make here. Build out a suitable Hypervisor for the underlying IaaS or cloud provider. If a VM Container is a first level abstraction provided by the IaaS then we increase our resource utilization even further. The minimal number of VMs required to run a Kubernetes cluster goes down to one (or three for HA) to host the Kubernetes control plane exposed to the &amp;ldquo;Superuser&amp;rdquo;.&lt;/p&gt;

&lt;h2 id=&#34;resource-cost-optimized-multi-tenancy&#34;&gt;Resource (cost) Optimized Multi-tenancy&lt;/h2&gt;

&lt;p&gt;A Kubernetes deployment with two namespaces both with a number of applications running would look something like this.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./hard-tenancy-k8s.png&#34; alt=&#34;Hard Tenancy Hosted Kubernetes&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: there are other cloud tenant workloads running on the same IaaS infrastructure. Since these are VM Containers they have the same level of isolation as a regular Cloud VM. Thus they can run on the same hypervisor with minimal risk.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Initially there is zero infrastructure deployed to the cloud, thus there is zero cost to the Superuser.&lt;/p&gt;

&lt;p&gt;The Superuser requests a Kubernetes cluster from the cloud. The Cloud provider creates a single Container VM (or 3 for HA) which is running the main Kubernetes API and System Services. The Superuser could choose to deploy pods in the system namespace, or create new namespaces to delegate access to other users.&lt;/p&gt;

&lt;p&gt;The Superuser create two Namespaces &lt;code&gt;foo&lt;/code&gt; and &lt;code&gt;bar&lt;/code&gt;. Kubernetes requests two VM containers from the Cloud for each Namespace&amp;rsquo;s Control Plane (Kubernetes API and System Services). The Superuser delegates access to those Namespaces to some users who each deploy some workloads (Pods), their respective Control Planes request VM containers for each of those workloads.&lt;/p&gt;

&lt;p&gt;At all stages of this, the superuser is only paying for the actual consumed resources. The cloud provider owns the capacity which is available to any user of the cloud.&lt;/p&gt;

&lt;h2 id=&#34;i-m-not-actually-saying-anything-new-here&#34;&gt;I&amp;rsquo;m not actually saying anything new here &amp;hellip;&lt;/h2&gt;

&lt;p&gt;The cloud providers are already working towards this future. You can see this forming by watching what is happening in the Open Source Communities. (Arguably Amazon is already doing this opaquely with Fargate).&lt;/p&gt;

&lt;p&gt;The first hint is &lt;a href=&#34;https://github.com/virtual-kubelet/virtual-kubelet&#34;&gt;Virtual Kubelet&lt;/a&gt; which is an open source tool designed to masquerade as a kubelet.  It connects Kubernetes to other APIs. This would allow Kubernetes to request Container VMs from a Cloud&amp;rsquo;s Container VM scheduler.&lt;/p&gt;

&lt;p&gt;Other hints are the number of emerging VM Container technologies, the already mentioned &lt;a href=&#34;https://katacontainers.io/&#34;&gt;Kata Containers&lt;/a&gt;, but also &lt;a href=&#34;https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/&#34;&gt;Firecracker from Amazon&lt;/a&gt; and &lt;a href=&#34;https://github.com/google/gvisor&#34;&gt;gvisor&lt;/a&gt; from Google.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Tied together with the correct improvements to the Kubernetes hard tenancy model and you&amp;rsquo;ll arrive at the holy grail of Kubernetes.  Full isolation of Kubernetes workloads and a pure per Pod consumption cost model to run workloads on Kubernetes.&lt;/p&gt;

&lt;p&gt;For those who aren&amp;rsquo;t on the public cloud, we don&amp;rsquo;t get the same consumption model as the onus for capacity remains with the infrastructure provider (in this case yourself). You will still get the benefits of higher resource utilization which pays off in lower capacity requirements.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s hope VMware and OpenStack are paying attention and bring us lightweight VM Container technology based hypervisors and the appropriate Virtual Kubelet implementations.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Building a Habitat Supervisor for Kubernetes</title>
      <link>https://tech.paulcz.net/blog/habitat-supervisors-in-kubernetes/</link>
      <pubDate>Fri, 06 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/habitat-supervisors-in-kubernetes/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://habitat.sh&#34;&gt;Habitat&lt;/a&gt; is a project from &lt;a href=&#34;https://chef.io&#34;&gt;Chef&lt;/a&gt; that
provides you a reasonably simple way to build, package, and configure your
application.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Habitat is an integrated solution to building, running, and maintaining your application throughout its lifetime. It builds your application and its services into immutable artifacts with declarative dependencies, and then provides automatic rebuilds of your application and its services as your application code and dependencies have upstream updates.&amp;rdquo; - &lt;a href=&#34;https://www.habitat.sh/tutorials/get-started/&#34;&gt;Habitat Getting Started Guide&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;One of Habitat&amp;rsquo;s core features is that its Supervisor creates a gossip based cluster
for managing configuration and state of your applications. Kubernetes also provides similar functionality this ability with the user defined Kubernetes manifests and the Kubernetes APIs. Initially it may seem odd that you would skip using Kubernetes to provide this functionality, however it does provide a way to have a universal system for your application management.&lt;/p&gt;

&lt;p&gt;Personally I&amp;rsquo;m still on the fence about how useful it is to have this extra abstraction for application lifecycle management on top of what Kubernetes already offers, but I don&amp;rsquo;t discount it as something that could be useful in a lot of organizations.&lt;/p&gt;

&lt;p&gt;Documentation for running Habitat built applications on Kubernetes is scant and feels fairly incomplete so I figured I would spend some time to work it out and come up with something myself.&lt;/p&gt;

&lt;p&gt;Of course the first thing I had to do was decide on an application to build to demonstrate it. Initially I was going to use the &lt;a href=&#34;https://www.habitat.sh/tutorials/get-started/&#34;&gt;basic tutorial&lt;/a&gt; app from the Habitat getting started tutorial, but instead decided I should write a very lightweight golang app to reduce the dependencies required to build and run it.&lt;/p&gt;

&lt;p&gt;The application I wrote is dead simple. Just a &lt;a href=&#34;https://github.com/paulczar/habsup/blob/master/main.go&#34;&gt;few lines&lt;/a&gt; of Golang to provide an API that responds to a &lt;code&gt;GET /health&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func main() {
  handler := health.NewHandler()
  http.Handle(&amp;quot;/health/&amp;quot;, handler)
  http.ListenAndServe(&amp;quot;:8080&amp;quot;, nil)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After writing this simplest of applications I realized I had inadvertently created a way to run the Habitat Supervisor effectively standalone which would also allow me to bootstrap a Habitat Gossip Cluster that other applications can join as needed.&lt;/p&gt;

&lt;p&gt;Next I had to get my Habitat environment set up. I was able to follow the Habitat Tutorial and figure out how to build this Golang app instead of a Ruby app. This was fairly &lt;a href=&#34;https://github.com/paulczar/habsup/tree/master/habitat&#34;&gt;straight forward&lt;/a&gt;
and was some edits to a &lt;a href=&#34;https://github.com/paulczar/habsup/blob/master/habitat/plan.sh&#34;&gt;plan.sh&lt;/a&gt;
file and a few &lt;a href=&#34;https://github.com/paulczar/habsup/tree/master/habitat/hooks&#34;&gt;hooks&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Performing the Build and exporting the &lt;code&gt;.hart&lt;/code&gt; file to a Docker image was fairly easy after I stumbled my way through &lt;code&gt;hab setup&lt;/code&gt; and getting a key etc working (the documentation for this could be improved to provide a more delightful experience).&lt;/p&gt;

&lt;h2 id=&#34;habitat-build-demo&#34;&gt;Habitat Build Demo&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone https://github.com/paulczar/habsup.git
$ cd habsup
$ hab studio enter
$ build
$ hab pkg export docker ./results/paulczar-habsuper-...hart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://tech.paulcz.net/habitat-supervisors-in-kubernetes/images/habstudio.gif&#34; alt=&#34;habitat build&#34; /&gt;&lt;/p&gt;

&lt;p&gt;My next step was to test it using Docker to make sure the app started and cluster formed etc. This mean writing a simple &lt;a href=&#34;https://github.com/paulczar/habsup/blob/master/docker-compose.yaml&#34;&gt;docker-compose.yaml&lt;/a&gt; file to launch three containers and tell them how to connect to eachother with links. and then launch the containers and check that the exposed Habitat Supervisor API is accessible.&lt;/p&gt;

&lt;h2 id=&#34;docker-demo&#34;&gt;Docker Demo&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone https://github.com/paulczar/habsup.git
$ cd habsup
$ docker-compose up -d
$ docker-compose logs -f
$ curl http://localhost:9631/services
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://tech.paulcz.net/habitat-supervisors-in-kubernetes/images/docker.gif&#34; alt=&#34;habitat docker compose&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: you can see the habitat supervisors running the health check at the end once the containers are running.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Now that I had the Supervisor as a standalone image it was time to put together the appropriate &lt;a href=&#34;https://github.com/paulczar/habsup/blob/master/kubernetes/manifests/supervisor.yaml&#34;&gt;Kubernetes manifest&lt;/a&gt;. To do so I had to do some research on the various Kubernetes resources and how they&amp;rsquo;d help me achieve my goal.&lt;/p&gt;

&lt;p&gt;After some experimentation I decided that it made sense to use the &lt;code&gt;StatefulSet&lt;/code&gt; resource for to run the supervisor in and run two services, the first being a headless service (meaning it is internal only) for the gossip protocol and the second being a regular service (with external access possible) for the API. Using a &lt;code&gt;StatefulSet&lt;/code&gt; gave me predictable pod names and starts up the replicas in order which makes it much easier for the gossip protocol to work.&lt;/p&gt;

&lt;p&gt;Initially I was using a single service for both the gossip and API ports but I wanted the Gossip to be internal only, but allow access (if needed) to the API. Creating two services gives me the ability to do both of those things. A headless service also has the benefit of creating a predictable KubeDNS entry for both the service and each pod which can come in handy.&lt;/p&gt;

&lt;p&gt;Another interesting thing I discovered is that Kubernetes doesn&amp;rsquo;t publish the service DNS until at least one pod is running. This created a chicken-and-egg issue if I tried to use a &lt;code&gt;readinessProbe&lt;/code&gt; for the hab api as habitat wouldn&amp;rsquo;t start until DNS was ready and DNS wouldn&amp;rsquo;t be created as it was waiting for a success from the probe. Thankfully there is an alpha feature that you can enable with an annotation &lt;code&gt;service.alpha.kubernetes.io/tolerate-unready-endpoints: &amp;quot;true&amp;quot;&lt;/code&gt; that allows you to use DNS before the pods are ready.&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-demo&#34;&gt;Kubernetes Demo&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone https://github.com/paulczar/habsup.git
$ cd habsup
$ kubectl create -f kubernetes/manifests
$ kubectl get pods -w
$ kubectl logs habitat-supervisor-0
$ curl $(minikube service habitat-supervisor-http --url)/services
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://tech.paulcz.net/habitat-supervisors-in-kubernetes/images/kubernetes.gif&#34; alt=&#34;habitat kubernetes&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Hopefully this was enough to bootstrap a person looking to use Habitat on Kubernetes. It would be fairly trivial to use the manifests I provided and do one of the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Use the Habitat cluster created here as a permanent Habitat cluster and have your applications join and leave that cluster as they come up.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Swap out the use of &lt;code&gt;paulczar/habsup&lt;/code&gt; image with your own image and adjust the ports and other values accordingly and have it run as a self contained cluster.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Getting Habitat to work in Kubernetes was fairly straight forward, however I had to do a few tricky things that shouldn&amp;rsquo;t be necessary. In order for Habitat to get solid adoption on Kubernetes I believe the following needs to be addressed:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Gossip cluster bootstrap relies on an ordered startup with &lt;code&gt;--peer ip-or-dns-of-first&lt;/code&gt;. Habitat should support a Kuberenetes based discovery which would ask the Kubernetes API to provide the peer details to join.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The API should come online with an approriate health status before the cluster is created. This would allow the use of a readinessProbe and avoid the problem I suggested earlier.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Habitat should consider a mode that uses the Kubernetes APIs and the contents of the Manifest to configure itself rather than forming the gossip cluster.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Writing Your First Helm Chart</title>
      <link>https://tech.paulcz.net/blog/getting-started-with-helm/</link>
      <pubDate>Tue, 03 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/getting-started-with-helm/</guid>
      <description>

&lt;p&gt;I recently found myself writing &lt;a href=&#34;https://github.com/IBM/activator-lagom-java-chirper/blob/master/docs/README.md&#34;&gt;instructions&lt;/a&gt; on how to deploy an
application to several Kubernetes platform and ended up writing a different Kubernetes manifests for each
platform. 95% of the content was the same with just a few different directives
based on how the particular platform handles ingress, or if we needed a Registry secret or a TLS certificate.&lt;/p&gt;

&lt;p&gt;Kubernetes manifests are very declarative and don&amp;rsquo;t offer any way to put conditionals or variables that could be set in them. This
is both a good and a bad thing. Enter &lt;a href=&#34;https://docs.helm.sh/&#34;&gt;Helm&lt;/a&gt; a Package Manager for Kubernetes. Helm allows you to package up
your Kubernetes application as a package that can be deployed easily to Kubernetes, One of its features (and the one that interested me)
the ability to template out your Kubernetes manifests.&lt;/p&gt;

&lt;p&gt;If you already have a Kubernetes manifest its very easy to turn it into a Helm Chart that you can then
iterate over and improve as you need to add more flexibility to it. In fact your first iteration of a
Helm chart can be as simple as moving your manifests into a new directory and adding a few lines
to a Chart.yaml file.&lt;/p&gt;

&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;

&lt;p&gt;You&amp;rsquo;ll need the following installed to follow along with this tutorial:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-minikube/&#34;&gt;Minikube&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/helm/blob/master/docs/install.md&#34;&gt;Helm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-kubectl/&#34;&gt;kubectl&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;prepare-environment&#34;&gt;Prepare Environment&lt;/h2&gt;

&lt;p&gt;Bring up a test Kubernetes environment using Minikube:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ minikube start
Starting local Kubernetes v1.7.5 cluster...
Starting VM...
Getting VM IP address...
Moving files into cluster...
Setting up certs...
Connecting to cluster...
Setting up kubeconfig...
Starting cluster components...
Kubectl is now configured to use the cluster.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Wait a minute or so and install Helm&amp;rsquo;s tiller service to Kubernetes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm init
$HELM_HOME has been configured at /home/pczarkowski/.helm.

Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.
Happy Helming!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a path to work in:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ mkdir -p ~/development/my-first-helm-chart
$ cd ~/development/my-first-helm-chart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;If it fails out you may need to wait a few more minutes for minikube to become
accessible.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;create-example-kubernetes-manifest&#34;&gt;Create Example Kubernetes Manifest.&lt;/h2&gt;

&lt;p&gt;Writing a Helm Chart is easier when you&amp;rsquo;re starting with an existing set of
Kubernetes manifests. One of the easiest ways to get a basic working manifest
is to ask Kubernetes to
&lt;a href=&#34;https://blog.heptio.com/using-kubectl-to-jumpstart-a-yaml-file-heptioprotip-6f5b8a63a3ea&#34;&gt;run something and then fetch the manifest&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ mkdir manifests
$ kubectl run example --image=nginx:1.13.5-alpine \
    -o yaml &amp;gt; manifests/deployment.yaml
$ kubectl expose deployment example --port=80 --type=NodePort \
    -o yaml &amp;gt; manifests/service.yaml
$ minikube service example --url       
http://192.168.99.100:30254
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All going well you should be able to hit the provided URL and get the &amp;ldquo;Welcome to nginx!&amp;rdquo;
page. You&amp;rsquo;ll see you now have two Kubernetes manifests saved. We can use these
to bootstrap our helm charts:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ tree manifests
manifests
├── deployment.yaml
└── service.yaml
0 directories, 2 files
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Before we move on we should clean up our environment.  We can use the newly
created manifests to help:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl delete -f manifests
deployment &amp;quot;example&amp;quot; deleted
service &amp;quot;example&amp;quot; deleted
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;create-and-deploy-a-basic-helm-chart&#34;&gt;Create and Deploy a Basic Helm Chart&lt;/h2&gt;

&lt;p&gt;Helm has some tooling to create the scaffolding needed to start developing a
new Helm Chart. We&amp;rsquo;ll create it with a placeholder name of &lt;code&gt;helm&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm create helm
Creating helm
tree helm
helm
├── charts
├── Chart.yaml
├── templates
│   ├── deployment.yaml
│   ├── _helpers.tpl
│   ├── ingress.yaml
│   ├── NOTES.txt
│   └── service.yaml
└── values.yaml
2 directories, 7 files
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Helm will have created a number of files and directories.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Chart.yaml&lt;/code&gt; - the metadata for your Helm Chart.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;values.yaml&lt;/code&gt; - values that can be used as variables in your templates.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;templates/*.yaml&lt;/code&gt; - Example Kubernetes manifests.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;_helpers.tpl&lt;/code&gt; - helper functions that can be used inside the templates.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;templates/NOTES.txt&lt;/code&gt; - templated notes that are displayed on Chart install.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Edit &lt;code&gt;Chart.yaml&lt;/code&gt; so that it looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
description: My First Helm Chart - NGINX Example
name: my-first-helm-chart
version: 0.1.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Copy our example Kubernetes manifests over the provided templates and remove the
currently unused &lt;code&gt;ingress.yaml&lt;/code&gt; and &lt;code&gt;NOTES.txt&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cp manifests/* helm/templates/
$ rm helm/templates/ingress.yaml
$ rm helm/templates/NOTES.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next we should be able to install our helm chart which will deploy our application
to Kubernetes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm install -n my-first-helm-chart helm
NAME:   my-first-helm-chart
LAST DEPLOYED: Tue Oct  3 10:20:57 2017
NAMESPACE: default
STATUS: DEPLOYED

RESOURCES:
==&amp;gt; v1/Service
NAME     CLUSTER-IP  EXTERNAL-IP  PORT(S)       AGE
example  10.0.0.210  &amp;lt;nodes&amp;gt;      80:30254/TCP  0s

==&amp;gt; v1beta1/Deployment
NAME     DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
example  1        1        1           0          0s

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Like before we can use &lt;code&gt;minikube&lt;/code&gt; to get the URL:&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ minikube service example --url    
http://192.168.99.100:30254
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Again accessing that URL via your we browser should get you the default NGINX welcome page.&lt;/p&gt;

&lt;p&gt;Congratulations!  You&amp;rsquo;ve just created and deployed your first Helm chart. However we&amp;rsquo;re
not quite done yet. use Helm to delete your deployment and then lets move on to customizing
the Helm Chart with variables and values:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm del --purge my-first-helm-chart
release &amp;quot;my-first-helm-chart&amp;quot; deleted
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;add-variables-to-your-helm-chart&#34;&gt;Add variables to your Helm Chart&lt;/h2&gt;

&lt;p&gt;Check out &lt;code&gt;helm/values.yaml&lt;/code&gt; and you&amp;rsquo;ll see there&amp;rsquo;s a lot of variables already
defined by the example that helm provided when you created the helm chart. You&amp;rsquo;ll
notice that it is has values for &lt;code&gt;nginx&lt;/code&gt; in there. This is because Helm also uses
nginx as their example. We can re-use some of the values provided but we should clean
it up a bit.&lt;/p&gt;

&lt;p&gt;Edit &lt;code&gt;helm/values.yaml&lt;/code&gt; to look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;replicaCount: 1
image:
  repository: nginx
  tag: 1.13.5-alpine
  pullPolicy: IfNotPresent
  pullSecret:
service:
  type: NodePort
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can access any of these values in our templates using the golang templating
engine. For example accessing &lt;code&gt;replicaCount&lt;/code&gt; would be written as &lt;code&gt;{{ .Values.replicaCount }}&lt;/code&gt;.
Helm also provides information about the Chart and Release which we&amp;rsquo;ll also utilize.&lt;/p&gt;

&lt;p&gt;Update your &lt;code&gt;helm/templates/deployment.yaml&lt;/code&gt; to utilize our values:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: apps/v1beta1
kind: Deployment
metadata:
  creationTimestamp: 2017-10-03T15:03:17Z
  generation: 1
  labels:
    run: &amp;quot;{{ .Release.Name }}&amp;quot;
    chart: &amp;quot;{{ .Chart.Name }}-{{ .Chart.Version }}&amp;quot;
    release: &amp;quot;{{ .Release.Name }}&amp;quot;
    heritage: &amp;quot;{{ .Release.Service }}&amp;quot;     
  name: &amp;quot;{{ .Release.Name }}&amp;quot;
  namespace: default
  resourceVersion: &amp;quot;3030&amp;quot;
  selfLink: /apis/extensions/v1beta1/namespaces/default/deployments/example
  uid: fd03ac95-a84b-11e7-a417-0800277e13b3
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      run: &amp;quot;{{ .Release.Name }}&amp;quot;
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        run: &amp;quot;{{ .Release.Name }}&amp;quot;
    spec:
      {{- if .Values.image.pullSecret }}    
            imagePullSecrets:
              - name: &amp;quot;{{ .Values.image.pullSecret }}&amp;quot;
      {{- end }}          
      containers:
      - image: {{ .Values.image.repository }}:{{ .Values.image.tag }}
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        name: example
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status: {}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Note the use of the &lt;code&gt;if&lt;/code&gt; statement around &lt;code&gt;image.pullSecret&lt;/code&gt; being set. This
sort of conditional becomes very important when making your Helm Chart portable across
different Kubernetes platforms.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Next edit your &lt;code&gt;helm/templates/service.yaml&lt;/code&gt; to look like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: Service
metadata:
  creationTimestamp: 2017-10-03T15:03:30Z
  labels:
    run: &amp;quot;{{ .Release.Name }}&amp;quot;
    chart: &amp;quot;{{ .Chart.Name }}-{{ .Chart.Version }}&amp;quot;
    release: &amp;quot;{{ .Release.Name }}&amp;quot;
    heritage: &amp;quot;{{ .Release.Service }}&amp;quot;  
  name: &amp;quot;{{ .Release.Name }}&amp;quot;
  namespace: default
  resourceVersion: &amp;quot;3066&amp;quot;
  selfLink: /api/v1/namespaces/default/services/example
  uid: 044d2b7e-a84c-11e7-a417-0800277e13b3
spec:
  clusterIP:
  externalTrafficPolicy: Cluster
  ports:
  - nodePort: 30254
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    run: &amp;quot;{{ .Release.Name }}&amp;quot;
  sessionAffinity: None
  type: &amp;quot;{{ .Values.service.type }}&amp;quot;
status:
  loadBalancer: {}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once your files are written out you should be able to install the Helm Chart:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm install -n second helm
NAME:   second
LAST DEPLOYED: Tue Oct  3 10:59:41 2017
NAMESPACE: default
STATUS: DEPLOYED

RESOURCES:
==&amp;gt; v1/Service
NAME    CLUSTER-IP  EXTERNAL-IP  PORT(S)       AGE
second  10.0.0.160  &amp;lt;nodes&amp;gt;      80:30254/TCP  1s

==&amp;gt; v1beta1/Deployment
NAME    DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
second  1        1        1           0          1s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next use minikube to get the URL of the service, but since we templated the
service name to match the release you&amp;rsquo;ll want to use this new name:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ minikube service second --url
http://192.168.99.100:30254
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now lets try something fun. Change the image we&amp;rsquo;re using by upgrading the helm release
and overriding some values on the command line:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ helm upgrade --set image.repository=httpd --set image.tag=2.2.34-alpine second helm
Release &amp;quot;second&amp;quot; has been upgraded. Happy Helming!
LAST DEPLOYED: Tue Oct  3 11:09:30 2017
NAMESPACE: default
STATUS: DEPLOYED

RESOURCES:
==&amp;gt; v1/Service
NAME    CLUSTER-IP  EXTERNAL-IP  PORT(S)       AGE
second  10.0.0.160  &amp;lt;nodes&amp;gt;      80:30254/TCP  9m

==&amp;gt; v1beta1/Deployment
NAME    DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
second  1        1        1           0          9m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then go to our minikube provided URL and you&amp;rsquo;ll see a different message &lt;code&gt;It works!&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;clean-up&#34;&gt;Clean up&lt;/h2&gt;

&lt;p&gt;use &lt;code&gt;minikube delete&lt;/code&gt; to clean up your environment:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;minikube delete
Deleting local Kubernetes cluster...
Machine deleted.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Helm is a very powerful way to package up your Kubernetes manifests to make them
extensible and portable. While it is quite complicated its fairly easy to get started
with it and if you&amp;rsquo;re like me you&amp;rsquo;ll find yourself replacing the Kubernetes manifests
in your code repos with Helm Charts.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s a lot more you can do with Helm, we&amp;rsquo;ve just scratched the surface. Enjoy
using and learning more about them!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Resume for Paul Czarkowski</title>
      <link>https://tech.paulcz.net/page/resume/</link>
      <pubDate>Sun, 25 Jun 2017 09:04:08 -0600</pubDate>
      
      <guid>https://tech.paulcz.net/page/resume/</guid>
      <description>

&lt;h1 id=&#34;paul-czarkowski&#34;&gt;Paul Czarkowski&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Principal Technologist (Developer Advocate) @ Pivotal Software&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;mailto:username.taken@gmail.com&#34;&gt;username.taken@gmail.com&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://tech.paulcz.net/&#34;&gt;http://tech.paulcz.net&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/paulczar&#34;&gt;https://github.com/paulczar&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Work Permits : USA, Australia, UK/Europe.&lt;/p&gt;

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;

&lt;p&gt;I am an experienced hands-on Architect / DevOps Engineer with a long history in Operations and Infrastructure Automation.  I have a broad depth of experience across most IT and Operations related areas with strong experience in using and evangelizing DevOps tools and methodologies.&lt;/p&gt;

&lt;p&gt;My current role at Pivotal has me primarily acting as a Developer Advocate. This is a customer and public facing role in which I work to Advocate for customers and opensource communities into Pivotal, as well as Advocating for Pivotal to customers and communities. I am heavily involved in the Kubernetes community and can be found all over the world advocating for Kubernetes and am an core member of the Helm and Helm Charts community.&lt;/p&gt;

&lt;p&gt;My previous role at IBM has me focused on helping to rebuild the IBM developer advocacy programs and content creation processes.  Previous to that I led the conversion of our OpenStack Automation platform (&lt;a href=&#34;https://github.com/blueboxgroup/ursula&#34;&gt;ursula&lt;/a&gt;) from just Ubuntu to also supporting Redhat Enterprise Linux.  I also architected and built the Blue Box Cloud SRE Operations Platform (which we recently open-sourced as &lt;a href=&#34;https://github.com/IBM/cuttle&#34;&gt;cuttle&lt;/a&gt;) and built a team to maintain it.&lt;/p&gt;

&lt;p&gt;Previous to IBM/Blue Box I was at Rackspace where I worked on a team building a product with Docker on top of Openstack, and before that I worked at EA where I helped build and design the infrastructure for SimCity ( on AWS ) and SWTOR ( own data centers, approx 6,000 servers, 2M+ subscribers at launch ).&lt;/p&gt;

&lt;h2 id=&#34;speaking-engagements&#34;&gt;Speaking Engagements&lt;/h2&gt;

&lt;p&gt;As a Developer Advocate I have travelled to all corners of the globe to speak about Kubernetes, DevOps, and related topics. Some highlighted events, recordings, and slide decks can be found at my Speaking page: &lt;a href=&#34;https://speaking.paulcz.net/&#34;&gt;https://speaking.paulcz.net/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;open-source-and-passion-projects&#34;&gt;Open Source and Passion Projects&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In an effort to reduce the toil involved in managing and deploying to Helm Repositories I wrote a tool called &lt;a href=&#34;https://github.com/helm/chart-releaser&#34;&gt;Chart Releaser&lt;/a&gt; which uses github + github pages to fully host helm chart repositories. This tool was adopted into the official Helm repository as an official project.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;At IBM I involved myself in the Kargo/Kubespray community and made a significant amount of contributions to help improve the quality of the Ansible being written and the composability of the Roles. Surprisingly a year on I&amp;rsquo;m still in the top &lt;a href=&#34;https://github.com/kubernetes-incubator/kubespray/graphs/contributors&#34;&gt;5 contributors&lt;/a&gt; (based on lines of code, which is obviously the most important metric AMIRITE).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Built out a Chef Inspec Repository for &lt;a href=&#34;https://github.com/inspec-stigs/inspec-stig-rhel6&#34;&gt;RedHat 6 STIG auditing&lt;/a&gt; and formed a small community around using &lt;a href=&#34;https://github.com/inspec-stigs&#34;&gt;Inspec for STIG auditing&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;I got tired of fighting openssl commands to create SSL/TLS for development so I built a Docker Image called &lt;a href=&#34;https://github.com/paulczar/omgwtfssl&#34;&gt;omgwtfssl&lt;/a&gt; that takes a few environment variables and spits out a CA/key/cert combo.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Over Christmas 2014 I built out &lt;a href=&#34;https://github.com/factorish/factorish&#34;&gt;Factorish&lt;/a&gt; as a concept to show managing the life-cycle and configuration of applications in Docker using service discovery, and built several example apps such as &lt;a href=&#34;https://github.com/paulczar/docker-percona_galera&#34;&gt;Percona with Galera Replication&lt;/a&gt; and the &lt;a href=&#34;https://github.com/factorish/factorish-elk&#34;&gt;ELK stack&lt;/a&gt;.  Some of these concepts have found their way into tools such as &lt;a href=&#34;https://github.com/joyent/containerpilot&#34;&gt;Container Pilot&lt;/a&gt; and &lt;a href=&#34;https://habitat.sh&#34;&gt;Habitat.sh&lt;/a&gt;.  I also used it as a basis for a &lt;a href=&#34;http://tech.paulcz.net/blog/factorish_and_the_12_fakter_app/&#34;&gt;blog post&lt;/a&gt; and a series of talks I gave on Dockerizing apps that really shouldn’t be Dockerized.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;professional-accomplishments&#34;&gt;Professional Accomplishments&lt;/h2&gt;

&lt;h3 id=&#34;ibm-blue-box&#34;&gt;IBM / Blue Box&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Led the effort to port the Blue Box OpenStack automation tool (&lt;a href=&#34;https://github.com/blueboxgroup/ursula&#34;&gt;ursula&lt;/a&gt;) to support RedHat Enterprise Linux as well as Ubuntu (&lt;a href=&#34;https://www.ibm.com/blogs/bluemix/2017/04/ibm-bluemix-private-cloud-red-hat/&#34;&gt;see&lt;/a&gt;), with full STIG compliance and Chef Inspec for auditing.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Recognized the need for a unified SRE Operations Platform to support growth and built SiteController (&lt;a href=&#34;https://github.com/IBM/cuttle&#34;&gt;cuttle&lt;/a&gt;) and architected and built it, later forming and leading a team to maintain and develop it further.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Successfully led the effort to make Blue Box OpenStack installable in customer data centers with no Internet access utilizing SiteController and overhauling large parts of Ursula.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;rackspace&#34;&gt;Rackspace&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Built the initial Build/Push/Run workflow for the [now mostly defunct] OpenStack PaaS project (Solum) utilizing Docker and the Cedarish style workflow demonstrated by Dokku and DEIS.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Worked on the OpenStack nova-docker driver, and was the first [that I know of] person to successfully run &lt;a href=&#34;https://github.com/paulczar/dockenstack&#34;&gt;OpenStack in Docker&lt;/a&gt; containers.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Maintained Community Chef Cookbooks for Elasticsearch, Logstash, and Kibana.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;electronic-arts&#34;&gt;Electronic Arts&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Planned and Executed migration of live game services for multiple games from their existing expensive datacenters to spare capacity from the cheaper BioWare datacenters.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Built and deployed dozens of websites for game code redemptions and blogs into Amazon using Rightscale.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Helped build and scale Sim City Online servers in Amazon using Rightscale, implemented monitoring and logging systems to help debug and discover load and performance issues during launch instability.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;bioware-star-wars-the-old-republic-swtor&#34;&gt;BioWare – Star Wars The Old Republic (SWTOR)&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Supported the studio during development.   Lead the Online Operations team through purchasing and deploying over 6,000 servers in four data centers to run the online environment. Ensured a successful and glitch free  launch of SWTOR on the 20th December 2011.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Built a large cluster of Xen Hypervisors to provide virtual game servers for development and wrote scripts for deploying game databases from SAN snapshots ( reducing storage requirements from 36Tb to less than 1Tb ).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Successfully deployed a proof of concept private cloud with CloudStack to further increase our Virtualization abilities and create a self-service portal for our developers.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;employment-history&#34;&gt;Employment History&lt;/h2&gt;

&lt;h3 id=&#34;pivotal-software-nov-2017-to-current&#34;&gt;Pivotal Software - Nov 2017 to Current&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Austin, Texas&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Principal Technologist / Developer Advocate&lt;/p&gt;

&lt;h3 id=&#34;bluebox-an-ibm-company-nov-2014-to-nov-2017&#34;&gt;BlueBox an IBM Company – Nov 2014 to Nov 2017&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Austin, Texas&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Technical Lead, IBM Cloud Developer Labs,&lt;/p&gt;

&lt;p&gt;Architect / Senior DevOps Engineer,&lt;/p&gt;

&lt;p&gt;Technical lead of Site Controller team.&lt;/p&gt;

&lt;h3 id=&#34;rackspace-nov-2013-to-nov-2014&#34;&gt;Rackspace – Nov 2013 to Nov 2014&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Austin, Texas&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Senior Operations Engineer&lt;/p&gt;

&lt;h3 id=&#34;ea-bioware-april-2008-to-nov-2013&#34;&gt;EA / BioWare – April 2008 to Nov 2013&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Austin, Texas&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Manager, Systems Engineering &lt;em&gt;( March 2012 to current )&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Lead Systems Engineer  &lt;em&gt;( 2010  to March 2012 )&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Senior Systems Engineer  ( 2008 to 2010 )&lt;/p&gt;

&lt;h3 id=&#34;older&#34;&gt;Older&lt;/h3&gt;

&lt;p&gt;2004 - 2008 : IT Manager, Pandemic Studios.
2001 - 2004 : NOC Manager iTEL Community Telco
2000 – 2001 : Systems Administrator / Web Application Developer BMC Networks
1999 – 2000 : Systems Administrator, Global Info-Links
1998 – 1999 : Computer Technician, Altech Computers
1998             : Computer Technician, Harvey Norman
1997 - 1998  : OzNetCom&lt;/p&gt;

&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;

&lt;p&gt;Available on request.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>About Me</title>
      <link>https://tech.paulcz.net/page/about/</link>
      <pubDate>Fri, 16 Dec 2016 09:04:08 -0600</pubDate>
      
      <guid>https://tech.paulcz.net/page/about/</guid>
      <description>&lt;p&gt;Paul is an Infrastructure Engineer who has been doing IT and Operations for longer than he cares to admit and has worked across many industries, retail, game development, managed hosting, and more.  He has worked in both very large and very small organizations.&lt;/p&gt;

&lt;p&gt;As an avid Technologist he can be found tinkering with emergent technologies (hello running docker in production in 2014) and optimizing and codifying the operations of large legacy applications.&lt;/p&gt;

&lt;p&gt;Paul is an active member of the Austin Technology scene and is often found at the &lt;a href=&#34;https://www.meetup.com/austin-devops/&#34;&gt;Austin DevOps&lt;/a&gt;, &lt;a href=&#34;https://www.meetup.com/CloudAustin/&#34;&gt;Cloud Austin&lt;/a&gt;, &lt;a href=&#34;https://www.meetup.com/Docker-Austin/&#34;&gt;Docker Austin&lt;/a&gt;, and &lt;a href=&#34;https://www.meetup.com/OpenStack-Austin/&#34;&gt;OpenStack Austin&lt;/a&gt; both as a attendee and as a &lt;a href=&#34;https://tech.paulcz.net/page/speaker&#34;&gt;speaker&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Paul also helps organize local community conferences such as &lt;a href=&#34;https://www.devopsdays.org/events/2016-austin/welcome/&#34;&gt;DevOps Days Austin&lt;/a&gt; and &lt;a href=&#34;http://www.containerdaysaustin.com&#34;&gt;Container Days Austin&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Technology isn&amp;rsquo;t his only interest though.  Paul has a love of cooking and can often be found baking sourdough bread and cooking Texas style barbecue in his custom offset pit and competing ( and winning ) in food competitions both locally and interstate.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Paul Talks about stuff</title>
      <link>https://tech.paulcz.net/page/speaker/</link>
      <pubDate>Fri, 16 Dec 2016 09:04:08 -0600</pubDate>
      
      <guid>https://tech.paulcz.net/page/speaker/</guid>
      <description>

&lt;p&gt;An incomplete list of events that I&amp;rsquo;ve spoken at&lt;/p&gt;

&lt;h1 id=&#34;2018&#34;&gt;2018&lt;/h1&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;th&gt;Event&lt;/th&gt;
&lt;th&gt;Title&lt;/th&gt;
&lt;th&gt;Location&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Feb&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.meetup.com/Singapore-Cloud-Foundry-Meetup/events/247104639/&#34;&gt;Cloud Foundry Meetup Singapore&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A DevOps guide to Kubernetes&lt;/td&gt;
&lt;td&gt;Singapore&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Feb&lt;/td&gt;
&lt;td&gt;Devopsdays Charlotte&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://sched.co/DIjN&#34;&gt;A DevOps Guide to Kubernetes&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=jh3mEEAcfDY&#34;&gt;&lt;img src=&#34;https://tech.paulcz.net/img/video.png&#34; alt=&#34;Video&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://www.slideshare.net/PaulCzarkowski/a-devops-guide-to-kubernetes&#34;&gt;&lt;img src=&#34;https://tech.paulcz.net/img/slides.png&#34; alt=&#34;Slides&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Charlotte NC&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;March&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.meetup.com/nycdevops/events/fmgjmnyxfbbc/&#34;&gt;NYC Devops Meetup&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A DevOps guide to Kubernetes &lt;a href=&#34;https://www.slideshare.net/PaulCzarkowski/a-devops-guide-to-kubernetes&#34;&gt;&lt;img src=&#34;https://tech.paulcz.net/img/slides.png&#34; alt=&#34;Slides&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;New York, New York&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;April&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.meetup.com/Madison-Devops/events/kbqsmlyxgbpb/&#34;&gt;Devops Madison&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Devops is dead, servers are dying, and I don&amp;rsquo;t feel too great myself&lt;/td&gt;
&lt;td&gt;Madison WI&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;April&lt;/td&gt;
&lt;td&gt;CodeCamp Wellington&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://codecampwellington.nz/#paul-czarkowski&#34;&gt;Compliance as Code, Transform your security team with DevOps&lt;/a&gt; &lt;a href=&#34;https://www.slideshare.net/PaulCzarkowski/transform-your-devops-practices-with-security&#34;&gt;&lt;img src=&#34;https://tech.paulcz.net/img/slides.png&#34; alt=&#34;Slides&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Wellington NZ&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;April&lt;/td&gt;
&lt;td&gt;Longhorn PHP&lt;/td&gt;
&lt;td&gt;A PHP Developers Intro to Kubernetes &lt;a href=&#34;https://www.slideshare.net/PaulCzarkowski/kubernetes-for-the-php-developer&#34;&gt;&lt;img src=&#34;https://tech.paulcz.net/img/slides.png&#34; alt=&#34;Slides&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Austin TX&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;April&lt;/td&gt;
&lt;td&gt;Devopsdays Seattle&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.devopsdays.org/events/2018-seattle/program/paul-czarkowski/&#34;&gt;Compliance as Code&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=HLN49sLdsbA&#34;&gt;&lt;img src=&#34;https://tech.paulcz.net/img/video.png&#34; alt=&#34;Video&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://www.slideshare.net/PaulCzarkowski/transform-your-devops-practices-with-security&#34;&gt;&lt;img src=&#34;https://tech.paulcz.net/img/slides.png&#34; alt=&#34;Slides&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Seattle WA&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;May&lt;/td&gt;
&lt;td&gt;Spring One Tour - Denver&lt;/td&gt;
&lt;td&gt;Emcee, Intro to Kubernetes &lt;a href=&#34;https://www.slideshare.net/PaulCzarkowski/introduction-to-kubernetes-106900695&#34;&gt;&lt;img src=&#34;https://tech.paulcz.net/img/slides.png&#34; alt=&#34;Slides&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Denver CO&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;May&lt;/td&gt;
&lt;td&gt;Miami Kubernetes meetup&lt;/td&gt;
&lt;td&gt;Intro to Kubernetes &lt;a href=&#34;https://www.slideshare.net/PaulCzarkowski/introduction-to-kubernetes-106900695&#34;&gt;&lt;img src=&#34;https://tech.paulcz.net/img/slides.png&#34; alt=&#34;Slides&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Miami FL&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;May&lt;/td&gt;
&lt;td&gt;VMWare PKS Roadshow Minneapolis&lt;/td&gt;
&lt;td&gt;Application Modernization and Replatforming with PKS/Kubernetes &lt;a href=&#34;https://www.slideshare.net/PaulCzarkowski/application-modernization-with-pks-kubernetes&#34;&gt;&lt;img src=&#34;https://tech.paulcz.net/img/slides.png&#34; alt=&#34;Slides&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Minneapolis MN&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;May&lt;/td&gt;
&lt;td&gt;VMWare PKS Roadshow Reston&lt;/td&gt;
&lt;td&gt;Application Modernization and Replatforming with PKS/Kubernetes &lt;a href=&#34;https://www.slideshare.net/PaulCzarkowski/application-modernization-with-pks-kubernetes&#34;&gt;&lt;img src=&#34;https://tech.paulcz.net/img/slides.png&#34; alt=&#34;Slides&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Minneapolis MN&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;May&lt;/td&gt;
&lt;td&gt;Spring One Tour - St Louis&lt;/td&gt;
&lt;td&gt;Emcee, Intro to Kubernetes &lt;a href=&#34;https://www.slideshare.net/PaulCzarkowski/introduction-to-kubernetes-106900695&#34;&gt;&lt;img src=&#34;https://tech.paulcz.net/img/slides.png&#34; alt=&#34;Slides&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;St Louis MO&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;June&lt;/td&gt;
&lt;td&gt;VMWare PKS Roadshow Chicago&lt;/td&gt;
&lt;td&gt;Application Modernization and Replatforming with PKS/Kubernetes &lt;a href=&#34;https://www.slideshare.net/PaulCzarkowski/application-modernization-with-pks-kubernetes&#34;&gt;&lt;img src=&#34;https://tech.paulcz.net/img/slides.png&#34; alt=&#34;Slides&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Chicago IL&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;June&lt;/td&gt;
&lt;td&gt;Cloud Foundry Meetup Tokyo&lt;/td&gt;
&lt;td&gt;Intro to Kubernetes&lt;/td&gt;
&lt;td&gt;Tokyo, Japan&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;June&lt;/td&gt;
&lt;td&gt;Opensource Summit Tokyo&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://ossalsjp18.sched.com/event/EaYK/bof-kubernetes-day-2-paul-czarkowski-pivotal-software&#34;&gt;Kubernetes Day 2 Operations&lt;/a&gt; &lt;a href=&#34;https://www.slideshare.net/PaulCzarkowski/kubernetes-day-2-operations&#34;&gt;&lt;img src=&#34;https://tech.paulcz.net/img/slides.png&#34; alt=&#34;Slides&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Tokyo, Japan&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;June&lt;/td&gt;
&lt;td&gt;Linuxcon Beijing&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://lc32018.sched.com/event/F6Qa/bosh-a-platform-for-running-platforms-so-you-can-run-platforms-paul-czarkowski-pivotal&#34;&gt;Successful Patterns for running Platforms&lt;/a&gt; &lt;a href=&#34;https://www.slideshare.net/PaulCzarkowski/successful-patterns-for-running-platforms&#34;&gt;&lt;img src=&#34;https://tech.paulcz.net/img/slides.png&#34; alt=&#34;Slides&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Beijing, China&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&#34;2017&#34;&gt;2017&lt;/h1&gt;

&lt;h2 id=&#34;sensu-summit&#34;&gt;Sensu Summit&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Sensu @ Bluebox - &lt;a href=&#34;https://www.youtube.com/watch?v=5i07i9iAIvw&amp;amp;list=PLqLtpBjHqwC_v7lyciQNwIYkN-xtsLrF2&amp;amp;index=12&#34;&gt;Video&lt;/a&gt; | &lt;a href=&#34;http://tech.paulcz.net/sensu-at-bluebox-sensuconf-2017/#/&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;2016&#34;&gt;2016&lt;/h1&gt;

&lt;h2 id=&#34;all-day-devops&#34;&gt;All Day DevOps&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Serverfull computing - &lt;a href=&#34;https://youtu.be/MOrWDvpZTdY?t=4874&#34;&gt;Video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;openstack-summit-barcelona&#34;&gt;OpenStack Summit Barcelona&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;InterOp Panel - &lt;a href=&#34;https://www.openstack.org/videos/barcelona-2016/interop-you-keep-using-that-word-i-do-not-think-it-means-what-you-think-it-means&#34;&gt;Video&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;OpenStack Compliance the DevOps way - &lt;a href=&#34;https://www.openstack.org/videos/barcelona-2016/openstack-compliance-the-devops-way&#34;&gt;Video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;openstack-day-seattle&#34;&gt;OpenStack Day Seattle&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;99 OpenStack clouds on the wall - &lt;a href=&#34;https://www.youtube.com/watch?v=erjUIO0kKI8&#34;&gt;Video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;openstack-summit-austin&#34;&gt;OpenStack Summit Austin&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;A PaaS for Continuous Delivery - &lt;a href=&#34;https://www.openstack.org/videos/austin-2016/a-paas-for-continuous-delivery-of-cloud-native-apps-minus-kubernetes-and-mesos&#34;&gt;Video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;2015&#34;&gt;2015&lt;/h1&gt;

&lt;h2 id=&#34;openstack-summit-tokyo&#34;&gt;OpenStack Summit Tokyo&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Chef vs Ansible vs Puppet vs Salt - &lt;a href=&#34;https://www.openstack.org/videos/tokio-2015/chef-vs-puppet-vs-ansible-vs-salt-whats-best-for-deploying-and-managing-openstack&#34;&gt;Video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;devops-days-austin&#34;&gt;DevOps Days Austin&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Scout-A-Park a story of pragmatic dockerization - &lt;a href=&#34;https://vimeo.com/album/3437844/video/129894208&#34;&gt;Video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;openstack-day-seattle-1&#34;&gt;OpenStack Day Seattle&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Just enough OpenStack for Docker - &lt;a href=&#34;https://www.youtube.com/watch?v=mYYa84WQowQ&#34;&gt;Video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;openstack-summit-vancouver&#34;&gt;OpenStack Summit Vancouver&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Docker Docker Docker Openstack - &lt;a href=&#34;https://www.openstack.org/videos/vancouver-2015/docker-docker-docker-openstack&#34;&gt;Video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;docker-austin-meetup&#34;&gt;Docker Austin meetup&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Factorish - &lt;a href=&#34;https://www.youtube.com/watch?v=uqKPgc8jMkI&#34;&gt;Video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;2014&#34;&gt;2014&lt;/h1&gt;

&lt;h2 id=&#34;dockercon14&#34;&gt;DockerCon14&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Automated Chef cookbook testing with Drone.io and GitHub - &lt;a href=&#34;https://www.youtube.com/watch?v=9_H41VFfKcM&#34;&gt;Video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Flexible Private Docker Registry Infrastructure</title>
      <link>https://tech.paulcz.net/blog/flexible-docker-registry-infrastructure/</link>
      <pubDate>Sun, 10 Jan 2016 10:22:22 -0600</pubDate>
      
      <guid>https://tech.paulcz.net/blog/flexible-docker-registry-infrastructure/</guid>
      <description>

&lt;p&gt;Previously I showed how to run a &lt;a href=&#34;http://tech.paulcz.net/2016/01/deploying-a-secure-docker-registry/&#34;&gt;basic secure Docker Registry&lt;/a&gt;.  I am now going to expand on this to show you something that you might use in production as part of your CI/CD infrastructure.&lt;/p&gt;

&lt;p&gt;The beauty of running Docker is that you &lt;em&gt;can&lt;/em&gt; push an image from a developer&amp;rsquo;s laptop all the way into production which helps ensure that what you see in development and your various test/qa/stage environments are exactly the same as what you run in production.&lt;/p&gt;

&lt;p&gt;So they tell you anyway. The reality is that you don&amp;rsquo;t ever want to push an image built on a developer&amp;rsquo;s machine into production as you can&amp;rsquo;t be sure what is in it.  Instead you want to have a trusted build server build images from a &lt;code&gt;Dockerfile&lt;/code&gt; in your git repository and have it promoted through your environments from there.&lt;/p&gt;

&lt;p&gt;To ensure the integrity of your images you&amp;rsquo;ll want to run a Docker Registry that can be reached by all of your servers (and potentially people), but can only be written to by your build server (and/or an administrative user).&lt;/p&gt;

&lt;p&gt;You could run your &lt;a href=&#34;https://www.docker.com/docker-registry&#34;&gt;Docker Registry&lt;/a&gt; behind a &lt;a href=&#34;https://docs.docker.com/registry/recipes/&#34;&gt;complicated reverse proxy&lt;/a&gt; and create rules about who can GET/POST/etc through to the &lt;a href=&#34;https://www.docker.com/docker-registry&#34;&gt;Docker Registry&lt;/a&gt; however we can use the magic of &amp;ldquo;&lt;a href=&#34;https://github.com/panicsteve/cloud-to-butt&#34;&gt;The Cloud&lt;/a&gt;&amp;rdquo; to reduce the complexity and thus the need for a reverse proxy.&lt;/p&gt;

&lt;p&gt;You will want to use either the &lt;a href=&#34;https://wiki.openstack.org/wiki/Swift&#34;&gt;Openstack Swift&lt;/a&gt; or the &lt;a href=&#34;https://aws.amazon.com/s3/&#34;&gt;Amazon S3&lt;/a&gt; object storage driver for the &lt;a href=&#34;https://www.docker.com/docker-registry&#34;&gt;Docker Registry&lt;/a&gt;. I will demonstrate using Swift, but using S3 should be very similar.&lt;/p&gt;

&lt;p&gt;You will of course want to also build all of these servers with Configuration Management including the commands to actually run the &lt;a href=&#34;https://www.docker.com/docker-registry&#34;&gt;Docker Registry&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;build-server-s&#34;&gt;Build Server(s)&lt;/h2&gt;

&lt;p&gt;For your build server(s) you&amp;rsquo;ll want to be running an OS with Docker installed on it. I use the &lt;a href=&#34;https://hub.docker.com/_/jenkins/&#34;&gt;Jenkins&lt;/a&gt; Docker image on &lt;a href=&#34;http://coreos.com/&#34;&gt;CoreOS&lt;/a&gt; for both my Jenkins Master and Slaves, however this is just personal preference.&lt;/p&gt;

&lt;p&gt;On each server you want to run a &lt;a href=&#34;https://www.docker.com/docker-registry&#34;&gt;Docker Registry&lt;/a&gt; with your Swift credentials passed through to it. Since we&amp;rsquo;re only accessing this via &lt;code&gt;127.0.0.1&lt;/code&gt; we do not need to secure it with TLS or authentication.&lt;/p&gt;

&lt;p&gt;Run the following on each build server to run the Registry backed by Swift, replacing the OpenStack credentials with your own:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;build01$ docker run -d \
              -p 127.0.0.1:5000:5000 \
              --name registry \
              --restart always \
              -e REGISTRY_STORAGE=swift \
              -e REGISTRY_STORAGE_SWIFT_USERNAME=${OS_USERNAME} \
              -e REGISTRY_STORAGE_SWIFT_PASSWORD=${OS_PASSWORD} \
              -e REGISTRY_STORAGE_SWIFT_TENANT=${OS_TENANT} \
              -e REGISTRY_STORAGE_SWIFT_AUTHURL=${OS_AUTH_URL} \
              -e REGISTRY_STORAGE_SWIFT_CONTAINER=docker-registry \
              registry:2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Push an image to make sure it worked:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;build01$ docker pull alpine
Using default tag: latest
latest: Pulling from library/alpine
Digest: sha256:78a756d480bcbc35db6dcc05b08228a39b32c2b2c7e02336a2dcaa196547a41d
Status: Downloaded newer image for alpine:latest
$ docker tag alpine 127.0.0.1:5000/alpine
$ docker push 127.0.0.1:5000/alpine
The push refers to a repository [127.0.0.1:5000/alpine] (len: 1)
74e49af2062e: Pushed 
latest: digest: sha256:a96155be113bb2b4b82ebbc11cf1b511726c5b41617a70e0772f8180afc72fa5 size: 1369
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you have more that one build server try to pull the image from one of the others, since we&amp;rsquo;re backing the &lt;a href=&#34;https://www.docker.com/docker-registry&#34;&gt;Docker Registry&lt;/a&gt; with an object store they should retrieve it just fine:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;build02$ docker pull 127.0.0.1:5000/alpine
Using default tag: latest
latest: Pulling from alpine

340b2f9a2643: Already exists 
Digest: sha256:a96155be113bb2b4b82ebbc11cf1b511726c5b41617a70e0772f8180afc72fa5
Status: Downloaded newer image for 127.0.0.1:5000/alpine:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;regular-server-s&#34;&gt;Regular Server(s)&lt;/h2&gt;

&lt;p&gt;We have a couple of options here.  You can run a &lt;a href=&#34;https://www.docker.com/docker-registry&#34;&gt;Docker Registry&lt;/a&gt; on each server listening only on localhost, or you can run one or more of them on their own servers that will listen on an IP and be secured with TLS.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll cover the former use case, for the latter use case you can adapt the instructions found &lt;a href=&#34;http://tech.paulcz.net/2016/01/deploying-a-secure-docker-registry/&#34;&gt;at my previous blog post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The important step in either case is to start the Registry as read-only so that regular servers cannot alter the contents of the Registry.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://www.docker.com/docker-registry&#34;&gt;Docker Registry&lt;/a&gt; is fairly light-weight when the files are in external storage and thus will use a neglible amount of your system resources and provides the advantages and security of running the registry on localhost and not needed to set &lt;code&gt;--insecure-registry&lt;/code&gt; settings or worrying about TLS certs for the docker daemon.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d \
      -p 127.0.0.1:5000:5000 \
      --name registry \
      --restart always \
      -e REGISTRY_STORAGE_MAINTENANCE_READONLY=&#39;enabled: true&#39; \
      -e REGISTRY_STORAGE=swift \
      -e REGISTRY_STORAGE_SWIFT_USERNAME=${OS_USERNAME} \
      -e REGISTRY_STORAGE_SWIFT_PASSWORD=${OS_PASSWORD} \
      -e REGISTRY_STORAGE_SWIFT_TENANT=${OS_TENANT} \
      -e REGISTRY_STORAGE_SWIFT_AUTHURL=${OS_AUTH_URL} \
      -e REGISTRY_STORAGE_SWIFT_CONTAINER=docker-registry \
      registry:2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With &lt;code&gt;REGISTRY_STORAGE_MAINTENANCE_READONLY=&#39;enabled: true&lt;/code&gt; set, when we try to push to the registry it should fail:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker push 127.0.0.1:5000/alpine
The push refers to a repository [127.0.0.1:5000/alpine] (len: 1)
f4fddc471ec2: Preparing 
Error parsing HTTP response: invalid character &#39;M&#39; looking for beginning of value: &amp;quot;Method not allowed\n&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;user-access-to-registry&#34;&gt;User Access to Registry:&lt;/h2&gt;

&lt;p&gt;If you want to provide access to regular users and don&amp;rsquo;t mind maintaining the password files locally you can adapt my &lt;a href=&#34;http://tech.paulcz.net/2016/01/deploying-a-secure-docker-registry/&#34;&gt;basic secure Docker Registry&lt;/a&gt; blog post to use the object storage backend.&lt;/p&gt;

&lt;p&gt;Assuming you&amp;rsquo;ve followed the instructions provided to create the TLS certificates you can run two &lt;a href=&#34;https://www.docker.com/docker-registry&#34;&gt;Docker Registry&lt;/a&gt;s each pointing at a different &lt;code&gt;htpasswd&lt;/code&gt; file.&lt;/p&gt;

&lt;p&gt;These can run on the same server, or on seperate servers.  They can also be run on multiple servers that are load balanced via an external load balancer or via round-robin-dns for high availability.&lt;/p&gt;

&lt;h3 id=&#34;read-only-users&#34;&gt;Read only Users&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d \
      -p 443:5000 \
      --name registry \
      --restart always \
      -v /opt/registry \
      -e REGISTRY_STORAGE_MAINTENANCE_READONLY=&#39;enabled: true&#39; \
      -e REGISTRY_STORAGE=swift \
      -e REGISTRY_STORAGE_SWIFT_USERNAME=${OS_USERNAME} \
      -e REGISTRY_STORAGE_SWIFT_PASSWORD=${OS_PASSWORD} \
      -e REGISTRY_STORAGE_SWIFT_TENANT=${OS_TENANT} \
      -e REGISTRY_STORAGE_SWIFT_AUTHURL=${OS_AUTH_URL} \
      -e REGISTRY_STORAGE_SWIFT_CONTAINER=docker-registry \
      -e REGISTRY_AUTH=htpasswd \
      -e &amp;quot;REGISTRY_AUTH_HTPASSWD_REALM=Admin Registry Realm&amp;quot; \
      -e REGISTRY_AUTH_HTPASSWD_PATH=/opt/registry/auth/admin.htpasswd \
      -e REGISTRY_HTTP_SECRET=qerldsljckjqr \
      -e REGISTRY_HTTP_TLS_CERTIFICATE=/opt/registry/ssl/cert.pem \
      -e REGISTRY_HTTP_TLS_KEY=/opt/registry/ssl/key.pem \
      registry:2
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;admin-read-write&#34;&gt;Admin Read/Write&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d \
      -p 444:5000 \
      --name registry \
      --restart always \
      -v /opt/registry \
      -e REGISTRY_STORAGE=swift \
      -e REGISTRY_STORAGE_SWIFT_USERNAME=${OS_USERNAME} \
      -e REGISTRY_STORAGE_SWIFT_PASSWORD=${OS_PASSWORD} \
      -e REGISTRY_STORAGE_SWIFT_TENANT=${OS_TENANT} \      
      -e REGISTRY_STORAGE_SWIFT_AUTHURL=${OS_AUTH_URL} \
      -e REGISTRY_STORAGE_SWIFT_CONTAINER=docker-registry \
      -e REGISTRY_AUTH=htpasswd \
      -e &amp;quot;REGISTRY_AUTH_HTPASSWD_REALM=Read Only Registry Realm&amp;quot; \
      -e REGISTRY_AUTH_HTPASSWD_PATH=/opt/registry/auth/users.htpasswd \
      -e REGISTRY_HTTP_SECRET=hlyrehbrvgszd \
      -e REGISTRY_HTTP_TLS_CERTIFICATE=/opt/registry/ssl/cert.pem \
      -e REGISTRY_HTTP_TLS_KEY=/opt/registry/ssl/key.pem \
      registry:2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Before pushing or pull images to these registries you&amp;rsquo;ll need to log in using &lt;code&gt;docker login myregistrydomain.com:443&lt;/code&gt; or &lt;code&gt;docker login myregistrydomain.com:444&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;By using external storage for the Registry we have increased our ability to run a resiliant Docker Registry with no single points of failure. All of the servers access the registry itself via localhost which means they have almost no reliance on external systems (except for a very robust object storage platform) and no need for complicated authentication systems.&lt;/p&gt;

&lt;p&gt;We also provide access to both Admin (read/write) and Regular (read-only) users via &lt;code&gt;htpasswd&lt;/code&gt; files and &lt;code&gt;TLS&lt;/code&gt; certificates/encryption which can be managed by Configuration Management.&lt;/p&gt;

&lt;p&gt;It goes without saying that you should further lock down all of these services with network based access restrictions in the form of Firewall/IPTables/Security-Groups so that only certain trusted networks can access any of the public endpoints we have created.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deploying a Simple and Secure Docker Registry</title>
      <link>https://tech.paulcz.net/blog/deploying-a-secure-docker-registry/</link>
      <pubDate>Sun, 10 Jan 2016 05:22:22 -0600</pubDate>
      
      <guid>https://tech.paulcz.net/blog/deploying-a-secure-docker-registry/</guid>
      <description>

&lt;p&gt;There comes a time in everybody&amp;rsquo;s life where they realize they have to run their own &lt;a href=&#34;https://www.docker.com/docker-registry&#34;&gt;Docker Registry&lt;/a&gt;. Unfortunately there&amp;rsquo;s not a lot of good information on how to run one. &lt;a href=&#34;http://docker.com&#34;&gt;Docker&lt;/a&gt;&amp;rsquo;s documentation is pretty good, but is verbose and across a lot of different pages which means having half a dozen tabs open and searching for the right information.&lt;/p&gt;

&lt;p&gt;While it&amp;rsquo;s pretty common to run the &lt;a href=&#34;https://www.docker.com/docker-registry&#34;&gt;Docker Registry&lt;/a&gt; itself with little to no security settings and fronting it with NGINX or Apache to provide this security I wanted to show how it can be done with just the &lt;a href=&#34;http://docker.com&#34;&gt;Docker&lt;/a&gt; Registry. If you need to do really clever stuff like authenticate against LDAP then you&amp;rsquo;ll want to go down the reverse proxy road.&lt;/p&gt;

&lt;p&gt;This example will demonstrate using just the &lt;a href=&#34;https://www.docker.com/docker-registry&#34;&gt;Docker Registry&lt;/a&gt; itself with both TLS certificate backed encryption and Certificate based endpoint authorization.&lt;/p&gt;

&lt;p&gt;For simplicity it will assume a single registry running on the local filesystem and will avoid using OS specific init (systemd/upstart/etc) systems by focusing just on the docker commands themselves.  This should work on any system capable of running &lt;a href=&#34;http://docker.com&#34;&gt;Docker&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;

&lt;p&gt;Boot a server that has &lt;a href=&#34;http://docker.com&#34;&gt;Docker&lt;/a&gt; installed. For an OS with &lt;a href=&#34;http://docker.com&#34;&gt;Docker&lt;/a&gt; already installed I recommend &lt;a href=&#34;http://coreos.com/&#34;&gt;CoreOS&lt;/a&gt;. However you could just as easily boot Ubuntu or CentOS and run &lt;code&gt;curl -sSL get.docker.com | sudo bash&lt;/code&gt; if you&amp;rsquo;re into that sort of thing.&lt;/p&gt;

&lt;p&gt;SSH into the server and ensure &lt;a href=&#34;http://docker.com&#34;&gt;Docker&lt;/a&gt; is working:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh core@xx.xx.xx.xx
$ docker info
Containers: 0
Images: 0
Server Version: 1.9.1
Storage Driver: overlay
 Backing Filesystem: extfs
Execution Driver: native-0.2
Logging Driver: json-file
Kernel Version: 4.3.3-coreos
Operating System: CoreOS 899.1.0
CPUs: 1
Total Memory: 997.4 MiB
Name: core-01
ID: C5XV:CZ3H:EAO4:ATJ3:ARSO:UOGD:XH3X:UKLZ:V3FO:2LRF:6E3X:CV5K
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;create-certificates&#34;&gt;Create Certificates&lt;/h2&gt;

&lt;p&gt;To keep this as simple as possible I will demonstrate using the &lt;a href=&#34;https://github.com/paulczar/omgwtfssl&#34;&gt;paulczar/omgwtfssl&lt;/a&gt; image to create certificates. If you would rather create them manually via the &lt;code&gt;openssl&lt;/code&gt; cli see my blog post on &lt;a href=&#34;http://tech.paulcz.net/2016/01/secure-docker-with-tls/&#34;&gt;Securing Docker with TLS&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We need to create a place on the filesystem to store the data for the registry as well as certificates and config data:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo mkdir -p /opt/registry/{data,ssl,config}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can create the certificates, add any IPs and DNS that you might address your registry with including that of any loadbalancer or floating IP that you might have:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run --rm \
  -v /opt/registry/ssl:/certs \
  -e SSL_IP=172.17.8.101 \
  -e SSL_DNS=registry.local \
  paulczar/omgwtfssl
----------------------------
| OMGWTFSSL Cert Generator |
----------------------------

--&amp;gt; Certificate Authority
====&amp;gt; Generating new CA key ca-key.pem
Generating RSA private key, 2048 bit long modulus
................+++
.................................+++
e is 65537 (0x10001)
====&amp;gt; Generating new CA Certificate ca.pem
====&amp;gt; Generating new config file openssl.cnf
====&amp;gt; Generating new SSL KEY key.pem
Generating RSA private key, 2048 bit long modulus
..........................................................+++
.............................................+++
e is 65537 (0x10001)
====&amp;gt; Generating new SSL CSR key.csr
====&amp;gt; Generating new SSL CERT cert.pem
Signature ok
subject=/CN=example.com
Getting CA Private Key

core@core-01 ~ $ ls /opt/registry/ssl/
ca-key.pem  ca.pem  ca.srl  cert.pem  key.csr  key.pem  openssl.cnf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our next step is to create a config file &lt;code&gt;/opt/registry/config/registry.env&lt;/code&gt; which will contain a list of Environment Variables that will be passed into the container:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;For this example I&amp;rsquo;m using the same CA certificate for clients as I did for the server, in reality it should probably be a different CA.&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# location of registry data
REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY=/opt/registry/data

# location of TLK key/cert
REGISTRY_HTTP_TLS_KEY=/opt/registry/ssl/key.pem
REGISTRY_HTTP_TLS_CERTIFICATE=/opt/registry/ssl/cert.pem

# location of CA of trusted clients
REGISTRY_HTTP_TLS_CLIENTCAS_0=/opt/registry/ssl/ca.pem
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All that is left to do now is start the registry container, bind mount in the &lt;code&gt;/opt/registry&lt;/code&gt; directory, pass in the config file, and expose port &lt;code&gt;443&lt;/code&gt; to the internal registry port:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d --name registry \
  -v /opt/registry:/opt/registry \
  -p 443:5000 --restart always \
  --env-file /opt/registry/config/registry.env \
  registry:2
Unable to find image &#39;registry:2&#39; locally
2: Pulling from library/registry
Digest: sha256:a842b52833778977f7b4466b90cc829e0f9aae725aebe3e32a5a6c407acd2a03
Status: Downloaded newer image for registry:2
d0106555b2d0aa30691c75c50b279e6a8bd485aa4ba2f203773e971988253169  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can check that we can access it from the server itself by tagging and pushing the &lt;code&gt;alpine&lt;/code&gt; image to it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker pull alpine
Using default tag: latest
latest: Pulling from library/alpine
Digest: sha256:78a756d480bcbc35db6dcc05b08228a39b32c2b2c7e02336a2dcaa196547a41d
Status: Downloaded newer image for alpine:latest
$ docker tag alpine 127.0.0.1/alpine
$ docker push 127.0.0.1/alpine
The push refers to a repository [127.0.0.1/alpine] (len: 1)
74e49af2062e: Pushed 
latest: digest: sha256:a96155be113bb2b4b82ebbc11cf1b511726c5b41617a70e0772f8180afc72fa5 size: 1369
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To check the security settings worked we&amp;rsquo;ll try to access the docker registry from a remote host:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Anywhere you see &lt;code&gt;172.17.8.101&lt;/code&gt; you will want to replace it with the IP or hostname of your docker registry.&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker pull 172.17.8.101/alpine
Using default tag: latest
Error response from daemon: unable to ping registry endpoint https://172.17.8.101/v0/
v2 ping attempt failed with error: Get https://172.17.8.101/v2/: x509: certificate signed by unknown authority
 v1 ping attempt failed with error: Get https://172.17.8.101/v1/_ping: x509: certificate signed by unknown authority
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;On the server we can see this failure in the docker logs:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker logs registry
2016/01/10 16:18:47 http: TLS handshake error from 172.17.8.1:44096: remote error: bad certificate
2016/01/10 16:18:47 http: TLS handshake error from 172.17.8.1:44098: remote error: bad certificate
2016/01/10 16:18:47 http: TLS handshake error from 172.17.8.1:44099: remote error: bad certificate
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are two things causing this failure. The first is that the remote server does not trust the client because it cannot provide the trusted CA certificate as specified in &lt;code&gt;REGISTRY_HTTP_TLS_CLIENTCAS_0&lt;/code&gt;. The second reason for failure is that the client doesn&amp;rsquo;t trust the &lt;code&gt;CA&lt;/code&gt; of the server.&lt;/p&gt;

&lt;p&gt;If we didn&amp;rsquo;t have &lt;code&gt;REGISTRY_HTTP_TLS_CLIENTCAS_0&lt;/code&gt; set we could simply add &lt;code&gt;--insecure-registry 172.17.8.101&lt;/code&gt; to &lt;code&gt;DOCKER_OPTS&lt;/code&gt; in &lt;code&gt;/etc/default/docker&lt;/code&gt;, however since we do have this set we&amp;rsquo;ll want to take the &lt;code&gt;CA.pem&lt;/code&gt; and save it as &lt;code&gt;/etc/docker/certs.d/172.17.8.101/ca.crt&lt;/code&gt; on the remote machine that you want to trust the registry server.&lt;/p&gt;

&lt;p&gt;I do this with the following commands, you may need to do it differently based on how your server is set up for access:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo mkdir -p /etc/docker/certs.d/172.17.8.101
$ sudo scp core@172.17.8.101:/opt/docker/registry/ca.pem \
    /etc/docker/certs.d/172.17.8.101/ca.crt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we have established trust in both directions we can try to access the docker registry again:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker pull 172.17.8.101/alpine
Using default tag: latest
latest: Pulling from alpine

340b2f9a2643: Already exists 
Digest: sha256:a96155be113bb2b4b82ebbc11cf1b511726c5b41617a70e0772f8180afc72fa5
Status: Downloaded newer image for 172.17.8.101/alpine:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Success!   We know have a &lt;a href=&#34;https://www.docker.com/docker-registry&#34;&gt;Docker Registry&lt;/a&gt; that is secured both with Encryption and an authorization based on each client having a specific CA certificate.  This setup is ideal for providing secure access to a private registry for remote servers.&lt;/p&gt;

&lt;p&gt;If you want to do this in a more automated fashion you can look at the various configuration management communities such as &lt;a href=&#34;https://supermarket.chef.io/cookbooks/docker_registry&#34;&gt;chef&lt;/a&gt; for examples.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Securing Docker with TLS certificates</title>
      <link>https://tech.paulcz.net/blog/secure-docker-with-tls/</link>
      <pubDate>Sun, 03 Jan 2016 14:44:30 -0600</pubDate>
      
      <guid>https://tech.paulcz.net/blog/secure-docker-with-tls/</guid>
      <description>

&lt;p&gt;By default Docker (and by extension Docker Swarm) has no authentication or authorization on its API, relying instead on the filesystem security of its unix socket &lt;code&gt;/var/run/docker.sock&lt;/code&gt; which by default is only accessible by the root user.&lt;/p&gt;

&lt;p&gt;This is fine for the basic use case of the default behavior of only accessing the Docker API on the local machine via the socket as the root user. However if you wish to use the Docker API over TCP then you&amp;rsquo;ll want to secure it so that you don&amp;rsquo;t give out root access to anyone that happens to poke you on the TCP port.&lt;/p&gt;

&lt;p&gt;Docker supports using TLS certificates (both on the server and the client) to provide proof of identity. When set up correctly it will only allow clients/servers with a certificate signed by a specific CA to talk to eachother. While not providing fine grained access permissions it does at least allow us to listen on a TCP socket and restrict access with a bonus of also providing encryption.&lt;/p&gt;

&lt;p&gt;Here I will detail what is required to secure Docker (and in turn Docker Swarm) running on a &lt;a href=&#34;http://coreos.com/&#34;&gt;CoreOS&lt;/a&gt; server. I will assume you already have a &lt;a href=&#34;http://coreos.com/&#34;&gt;CoreOS&lt;/a&gt; server running as described in my Docker Swarm &lt;a href=&#34;http://tech.paulcz.net/2016/01/running-ha-docker-swarm/&#34;&gt;post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you are only interested in securing Docker itself and not Docker Swarm then this should apply to any server with Docker installed that uses systemd.  Even on systems without systemd it should provide enough details to secure Docker.&lt;/p&gt;

&lt;h2 id=&#34;creating-certificates&#34;&gt;Creating Certificates&lt;/h2&gt;

&lt;p&gt;I will offer two methods to create the certificates, the first by using &lt;code&gt;openssl&lt;/code&gt; to create a CA and then sign a key/cert pair, the second by using the &lt;a href=&#34;https://hub.docker.com/r/paulczar/omgwtfssl/&#34;&gt;paulczar/omgwtfssl&lt;/a&gt; Docker Image which automates the certificate creation process.&lt;/p&gt;

&lt;p&gt;Either way you&amp;rsquo;ll want to start off by creating directories for both the server and client certificate sets:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo mkdir -p /etc/docker/ssl
$ mkdir -p ~/.docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;For this example we&amp;rsquo;re creating the keys and certificates on the server itself, ideally you would do this on your laptop or via configuration management and never store the CA key on a public server.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;openssl&#34;&gt;OpenSSL&lt;/h3&gt;

&lt;p&gt;First run &lt;code&gt;openssl&lt;/code&gt; to create and sign a CA key and certificate and copy the CA certificate into &lt;code&gt;/etc/docker/ssl&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ openssl genrsa -out ~/.docker/ca-key.pem 2048
.+++
..........................................................................................................+++
e is 65537 (0x10001)

$ openssl req -x509 -new -nodes -key ~/.docker/ca-key.pem \
    -days 10000 -out ~/.docker/ca.pem -subj &#39;/CN=docker-CA&#39;

$ ls ~/.docker/
ca-key.pem  ca.pem

$ sudo cp ~/.docker/ca.pem /etc/docker/ssl
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next we&amp;rsquo;ll need an openssl configuration file for the Docker client &lt;code&gt;~/.docker/openssl.cnf&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
extendedKeyUsage = serverAuth, clientAuth
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Followed by a configuration file for the Docker server &lt;code&gt;/etc/docker/ssl/openssl.cnf&lt;/code&gt;.  Add any DNS or IPs that you might use to access the Docker Server with, this is critical as the Golang SSL libraries are very strict:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
extendedKeyUsage = serverAuth, clientAuth
subjectAltName = @alt_names

[alt_names]
DNS.1 = docker.local
IP.1 = 172.17.8.101
IP.2 = 127.0.0.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next create and sign a certificate for the client:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ openssl genrsa -out ~/.docker/key.pem 2048
....................................+++
.............+++
e is 65537 (0x10001)

$ openssl req -new -key ~/.docker/key.pem -out ~/.docker/cert.csr \
    -subj &#39;/CN=docker-client&#39; -config ~/.docker/openssl.cnf

$ openssl x509 -req -in ~/.docker/cert.csr -CA ~/.docker/ca.pem \
    -CAkey ~/.docker/ca-key.pem -CAcreateserial \
    -out ~/.docker/cert.pem -days 365 -extensions v3_req \
    -extfile ~/.docker/openssl.cnf
Signature ok
subject=/CN=docker-client
Getting CA Private Key
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then do the same for the server:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo openssl genrsa -out /etc/docker/ssl/key.pem 2048
................................................................................+++
....................................+++
e is 65537 (0x10001)

$ sudo openssl req -new -key /etc/docker/ssl/key.pem \
    -out /etc/docker/ssl/cert.csr \
    -subj &#39;/CN=docker-server&#39; -config /etc/docker/ssl/openssl.cnf

$ sudo openssl x509 -req -in /etc/docker/ssl/cert.csr -CA ~/.docker/ca.pem \
    -CAkey ~/.docker/ca-key.pem -CAcreateserial \
    -out /etc/docker/ssl/cert.pem -days 365 -extensions v3_req \
    -extfile /etc/docker/ssl/openssl.cnf
Signature ok
subject=/CN=docker-client
Getting CA Private Key
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;omgwtfssl&#34;&gt;OMGWTFSSL&lt;/h3&gt;

&lt;p&gt;If you want to skip manually creating the certificates you can use the &lt;a href=&#34;https://hub.docker.com/r/paulczar/omgwtfssl/&#34;&gt;paulczar/omgwtfssl&lt;/a&gt; image which is a small (&amp;lt; 10mb) Docker image built specifically for creating certificates for situations like this.&lt;/p&gt;

&lt;p&gt;First we&amp;rsquo;ll create our client certs and use a docker volume binding to put the CA and certs into &lt;code&gt;~/.docker&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run --rm -v $(pwd)/.docker:/certs \
    paulczar/omgwtfssl
----------------------------
| OMGWTFSSL Cert Generator |
----------------------------

--&amp;gt; Certificate Authority
====&amp;gt; Using existing CA Key ca-key.pem
====&amp;gt; Using existing CA Certificate ca.pem
====&amp;gt; Generating new config file openssl.cnf
====&amp;gt; Generating new SSL KEY key.pem
Generating RSA private key, 2048 bit long modulus
.............+++
..........+++
e is 65537 (0x10001)
====&amp;gt; Generating new SSL CSR key.csr
====&amp;gt; Generating new SSL CERT cert.pem
Signature ok
subject=/CN=example.com
Getting CA Private Key
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next we&amp;rsquo;ll take ownership of them back from root (because of the docker volume binding) and then create the server certificates using the same CA using a second volume binding to &lt;code&gt;/etc/docker/ssl&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Since this is a server certificate we need to pass the IP and DNS that the server may respond to via the -e command line arguments.&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo cp ~/.docker/ca.pem /etc/docker/ssl/ca.pem
$ chown -R $USER ~/.docker
$ docker run --rm -v /etc/docker/ssl:/server \
    -v $(pwd)/.docker:/certs \
    -e SSL_IP=127.0.0.1,172.17.8.101 \
    -e SSL_DNS=docker.local -e SSL_KEY=/server/key.pem \
    -e SSL_CERT=/server/cert.pem paulczar/omgwtfssl
----------------------------
| OMGWTFSSL Cert Generator |
----------------------------

--&amp;gt; Certificate Authority
====&amp;gt; Using existing CA Key ca-key.pem
====&amp;gt; Using existing CA Certificate ca.pem
====&amp;gt; Generating new config file openssl.cnf
====&amp;gt; Generating new SSL KEY /server/key.pem
Generating RSA private key, 2048 bit long modulus
.................................+++
..................+++
e is 65537 (0x10001)
====&amp;gt; Generating new SSL CSR key.csr
====&amp;gt; Generating new SSL CERT /server/cert.pem
Signature ok
subject=/CN=example.com
Getting CA Private Key
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;using-the-tls-certificates-with-docker&#34;&gt;Using the TLS certificates with Docker&lt;/h2&gt;

&lt;p&gt;Now we have our TLS certificates created and in the correct locations you need to tell Docker to use the TLS certificate and also verify the client.  You do this by creating a drop in systemd unit to modify the existing Docker systemd unit.&lt;/p&gt;

&lt;p&gt;Create the file &lt;code&gt;custom.conf&lt;/code&gt; in &lt;code&gt;/etc/systemd/system/docker.service.d/&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;If you want to restrict local users from using the docker unix socket remove the second -H command line option, if you already have a custom drop in unit you can add the -H and &amp;ndash;tls* arguments to it.&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Service]
Environment=&amp;quot;DOCKER_OPTS=-H=0.0.0.0:2376 -H unix:///var/run/docker.sock --tlsverify --tlscacert=/etc/docker/ssl/ca.pem --tlscert=/etc/docker/ssl/cert.pem --tlskey=/etc/docker/ssl/key.pem&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Reload systemd and the Docker service:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo systemctl daemon-reload
$ sudo systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now when you try to access Docker via the TCP port you should get a TLS error:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H tcp://127.0.0.1:2376 info
Get http://127.0.0.1:2376/v1.21/containers/json: malformed HTTP response &amp;quot;\x15\x03\x01\x00\x02\x02&amp;quot;.
* Are you trying to connect to a TLS-enabled daemon without TLS?
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is because the Docker client does not know to use TLS to communicate with the server.  We can set some environment variables to enable TLS for the client and use the client key we created:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export DOCKER_HOST=tcp://127.0.0.1:2376
$ export DOCKER_TLS_VERIFY=1
$ export DOCKER_CERT_PATH=~/.docker
$ docker info
docker info
Containers: 0
Images: 0
Server Version: 1.9.1
Storage Driver: overlay
 Backing Filesystem: extfs
Execution Driver: native-0.2
Logging Driver: json-file
Kernel Version: 4.3.3-coreos
Operating System: CoreOS 899.1.0
CPUs: 1
Total Memory: 997.4 MiB
Name: core-01
ID: RGVQ:VDUC:Z5LU:IE7I:J6UJ:TFBJ:SSCO:EWG2:QKAW:5FY6:EIAV:MROK
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;using-the-tls-certificates-with-docker-swarm&#34;&gt;Using the TLS certificates with Docker Swarm&lt;/h2&gt;

&lt;p&gt;To secure Docker Swarm using these TLS certificates you will need to create TLS certificate/key pairs for each server using the same CA.&lt;/p&gt;

&lt;p&gt;to add some arguments to the &lt;code&gt;docker run&lt;/code&gt; command that you start Swarm Manager with the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d --name swarm-manager \
    -v /etc/docker/ssl:/etc/docker/ssl \
    --net=host swarm:latest manage \
    --tlsverify \
    --tlscacert=/etc/docker/ssl/ca.pem \
    --tlscert=/etc/docker/ssl/cert.pem \
    --tlskey=/etc/docker/ssl/key.pem \
    etcd://127.0.0.1:2379
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which you can then access using the docker client:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export DOCKER_HOST=tcp://127.0.0.1:2375
$ export DOCKER_TLS_VERIFY=1
$ export DOCKER_CERT_PATH=~/.docker

$ docker info
Containers: 6
Images: 5
Role: primary
Strategy: spread
Filters: health, port, dependency, affinity, constraint
Nodes: 3
 core-01: 172.17.8.101:2376
  └ Status: Healthy
  └ Containers: 2
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 1.023 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=4.3.3-coreos, operatingsystem=CoreOS 899.1.0, storagedriver=overlay
 core-02: 172.17.8.102:2376
  └ Status: Healthy
  └ Containers: 2
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 1.023 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=4.3.3-coreos, operatingsystem=CoreOS 899.1.0, storagedriver=overlay
 core-03: 172.17.8.103:2376
  └ Status: Healthy
  └ Containers: 2
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 1.023 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=4.3.3-coreos, operatingsystem=CoreOS 899.1.0, storagedriver=overlay
CPUs: 3
Total Memory: 3.068 GiB
Name: core-01
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Deploying a HA Docker Swarm Cluster</title>
      <link>https://tech.paulcz.net/blog/running-ha-docker-swarm/</link>
      <pubDate>Sat, 02 Jan 2016 14:44:30 -0600</pubDate>
      
      <guid>https://tech.paulcz.net/blog/running-ha-docker-swarm/</guid>
      <description>

&lt;p&gt;Given Docker&amp;rsquo;s propensity for creating easy to use tools it shouldn&amp;rsquo;t come as a surprise that Docker Swarm is one of the easier to understand and run of the &amp;ldquo;Docker Clustering&amp;rdquo; options currently out there. I recently built some &lt;a href=&#34;http://terraform.io&#34;&gt;Terraform&lt;/a&gt; configs for deploying a &lt;a href=&#34;https://github.com/openstack/osops-tools-contrib/tree/master/terraform/dockerswarm-coreos&#34;&gt;Highly Available Docker Swarm cluster on Openstack&lt;/a&gt; and learned a fair bit about Swarm in the process.&lt;/p&gt;

&lt;p&gt;This guide is meant to be a platform agnostic howto on installing and running a Highly Available Docker Swarm to show you the ideas and concepts that may not be as easy to understand from just reading some config management code.&lt;/p&gt;

&lt;h2 id=&#34;coreos&#34;&gt;CoreOS&lt;/h2&gt;

&lt;p&gt;The reason for using &lt;a href=&#34;http://coreos.com&#34;&gt;CoreOS&lt;/a&gt; here is that to make Swarm run in High Availability mode as well as being able to support docker networking between hosts we need to use service discovery.  We can choose to use &lt;code&gt;etcd&lt;/code&gt;, &lt;code&gt;consul&lt;/code&gt;, or &lt;code&gt;zookeeper&lt;/code&gt; here, CoreOS comes with &lt;code&gt;etcd&lt;/code&gt; thus makes it an excellent choice for running Docker Swarm.&lt;/p&gt;

&lt;p&gt;You will need three servers capable of running &lt;a href=&#34;http://coreos.com&#34;&gt;CoreOS&lt;/a&gt;.  See the &amp;ldquo;Try Out CoreOS&amp;rdquo; section of their website for various installation methods for different infrastructure. For this guide I will use the official &lt;a href=&#34;https://github.com/coreos/coreos-vagrant&#34;&gt;CoreOS Vagrant Example&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;skip the rest of this section if you install CoreOS for a different platform&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Clone down the Vagrant example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/coreos/coreos-vagrant.git vagrant-docker-swarm 
Cloning into &#39;vagrant-docker-swarm&#39;...
remote: Counting objects: 411, done.
remote: Total 411 (delta 0), reused 0 (delta 0), pack-reused 411
Receiving objects: 100% (411/411), 100.33 KiB | 0 bytes/s, done.
Resolving deltas: 100% (181/181), done.
Checking connectivity... done.
cd vagrant-docker-swarm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Edit the &lt;code&gt;Vagrantfile&lt;/code&gt; to set &lt;code&gt;$num_instances = 3&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;on Unix-like systems you can do this easily with sed&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sed -i &#39;s/\$num_instances = 1/\$num_instances = 3/&#39; Vagrantfile
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Get a new etcd discovery-url:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;if you are on a windows box and don&amp;rsquo;t have curl you can paste the url into a web browser to get the discovery-url&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl https://discovery.etcd.io/new\?size\=3
https://discovery.etcd.io/6a9c62105f04dac40a29b90fbed322ef
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a cloud-init file called &lt;code&gt;user-data&lt;/code&gt; in the base of the repo using the discovery-url from above:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#cloud-config

coreos:
  etcd2:
    discovery: https://discovery.etcd.io/888fd1e440faf680a7abb3fd934da6fd
    advertise-client-urls: http://$public_ipv4:2379
    initial-advertise-peer-urls: http://$public_ipv4:2380
    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
    listen-peer-urls: http://$public_ipv4:2380,http://$public_ipv4:7001
  units:
    - name: etcd2.service
      command: start

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Start up the CoreOS VMs and log into the first one to check everything worked ok:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vagrant up
Bringing machine &#39;core-01&#39; up with &#39;virtualbox&#39; provider...
Bringing machine &#39;core-02&#39; up with &#39;virtualbox&#39; provider...
Bringing machine &#39;core-03&#39; up with &#39;virtualbox&#39; provider...
...
$ vagrant ssh core-01
$ etcdctl member list
3c5901a3db54efa3: name=f1bae7bba7714ed7b4585c6b1256ddb2 peerURLs=http://172.17.8.101:2380 clientURLs=http://172.17.8.101:2379
9eeb141350af8439: name=5c8e57890d114d7d9d7aef662033a6e0 peerURLs=http://172.17.8.103:2380 clientURLs=http://172.17.8.103:2379
ebcc652087dfe6e8: name=de426249d3b34e23a5706d99b4900665 peerURLs=http://172.17.8.102:2380 clientURLs=http://172.17.8.102:2379
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;docker-swarm&#34;&gt;Docker Swarm&lt;/h2&gt;

&lt;p&gt;Now that we have several CoreOS servers with a working etcd cluster we can move on to setting up Docker Swarm.&lt;/p&gt;

&lt;p&gt;We need to modify docker to listen on tcp port &lt;code&gt;2376&lt;/code&gt; as well as registering itself to service discovery (which will allow us to set up overlay networking later on).  We do this by creating a file &lt;code&gt;custom.conf&lt;/code&gt; in &lt;code&gt;/etc/systemd/system/docker.service.d/&lt;/code&gt; on each server.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;if not using vagrant change &lt;code&gt;eth1&lt;/code&gt; to match the primary interface for your server&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Service]
Environment=&amp;quot;DOCKER_OPTS=-H=0.0.0.0:2376 -H unix:///var/run/docker.sock --cluster-advertise eth1:2376 --cluster-store etcd://127.0.0.1:2379&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then need to reload the &lt;code&gt;systemctl&lt;/code&gt; daemon and then restart docker for these changes to take effect.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo systemctl daemon-reload
sudo systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check that you can access docker via tcp on one of your hosts:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H tcp://172.17.8.101:2376 info
Containers: 0
Images: 0
Engine Version: 1.9.1
Storage Driver: overlay
 Backing Filesystem: extfs
Execution Driver: native-0.2
Logging Driver: json-file
Kernel Version: 4.3.3-coreos
Operating System: CoreOS 899.1.0
CPUs: 1
Total Memory: 997.4 MiB
Name: core-01
ID: BK64:WF3J:5JU6:VYLI:YJSO:CAQH:HPYM:MPTG:FMTA:VLE3:HSMP:F4VQ
Cluster store: etcd://127.0.0.1:2379/docker

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;rsquo;re now ready to run Docker Swarm itself. There are two extra components to running Docker Swarm, a Swarm Agent and a Swarm Manager.&lt;/p&gt;

&lt;p&gt;The Swarm Agent watches the local Docker service via it&amp;rsquo;s TCP port and registers it into service discovery (etcd in our case).  We will run this on each server like so:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;set the &amp;ndash;addr= argument to match the primary IP of each node&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d --name swarm-agent \
    --net=host swarm:latest \
        join --addr=172.17.8.101:2376 \
        etcd://127.0.0.1:2379
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The Swarm Manager watches service discovery and exposes a TCP port (2375) which when accessed by a Docker client will perform actions and schedule containers across the Swarm cluster.&lt;/p&gt;

&lt;p&gt;To ensure High Availability of our cluster we&amp;rsquo;ll run a Swarm Manager on each server:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d --name swarm-manager 
    --net=host swarm:latest manage \
    etcd://127.0.0.1:2379
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Assuming everything went smoothly we can now access the swarm cluster via the Swarm Managers TCP port on any of the servers:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H tcp://172.17.8.101:2375 info
Containers: 6
Images: 5
Role: primary
Strategy: spread
Filters: health, port, dependency, affinity, constraint
Nodes: 3
 core-01: 172.17.8.101:2376
  └ Status: Healthy
  └ Containers: 2
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 1.023 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=4.3.3-coreos, operatingsystem=CoreOS 899.1.0, storagedriver=overlay
 core-02: 172.17.8.102:2376
  └ Status: Healthy
  └ Containers: 2
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 1.023 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=4.3.3-coreos, operatingsystem=CoreOS 899.1.0, storagedriver=overlay
 core-03: 172.17.8.103:2376
  └ Status: Healthy
  └ Containers: 2
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 1.023 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=4.3.3-coreos, operatingsystem=CoreOS 899.1.0, storagedriver=overlay
CPUs: 3
Total Memory: 3.068 GiB
Name: core-01
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our next step is to create an overlay network using the &lt;code&gt;docker network&lt;/code&gt; command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H tcp://172.17.8.101:2375 network create --driver overlay my-net
614913b275dee43a63b48d08b4f5e52f7c0e531d70c63eeb8bb35624470da0c4

$ docker -H tcp://172.17.8.101:2375 network ls                            
NETWORK ID          NAME                DRIVER
86ecb0cf32c6        core-02/none        null                
c7a291ed8366        core-01/host        host                
3747364c5961        core-03/none        null                
8245d6d3ac67        core-02/host        host                
614913b275de        my-net              overlay             
61ead145e9dd        core-01/bridge      bridge              
c9457c4f4588        core-03/bridge      bridge              
b8a6c75cb3b9        core-03/host        host                
bdc4d5ccd778        core-02/bridge      bridge              
66afdc892361        core-01/none        null
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally we&amp;rsquo;ll create a Container on one host and then check that it is accessible from another:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;replace the node==XXXX argument with the hostname of one of your hosts, make sure to use a different node for each docker command&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -it --name=web --net=my-net \
    -H tcp://172.17.8.101:2375 \
    --env=&amp;quot;constraint:node==core-01&amp;quot; nginx
e0fe18c946a5692806608f939d4d6f31c670e3f42bf3942a77142bed2095983e

$ docker run -it --rm --net=my-net \
    -H tcp://172.17.8.101:2375 \
    --env=&amp;quot;constraint:node==core02&amp;quot; busybox wget -O- http://web
Connecting to web (10.0.0.2:80)
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you&amp;rsquo;ve been following along you have successfully deployed a Highly Available Docker Swarm cluster.  From here you could use a load balancer to load balance the Swarm Manager port (2375) or even use Round Robin DNS.&lt;/p&gt;

&lt;p&gt;You may have notice there is no authentication or authorization on this and anybody with a Docker binary and TCP access to your hosts could spin up docker containers. This is fairly easily fixed by using Docker&amp;rsquo;s TLS cert based authorization.&lt;/p&gt;

&lt;p&gt;To read how to secure both Docker and Docker Swarm with TLS read the followup post &lt;a href=&#34;http://tech.paulcz.net/2016/01/secure-docker-with-tls/&#34;&gt;Secure Docker with TLS&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Openstacks and Ecosystems</title>
      <link>https://tech.paulcz.net/blog/openstacks-and-ecosystems/</link>
      <pubDate>Sat, 02 Jan 2016 13:00:42 -0600</pubDate>
      
      <guid>https://tech.paulcz.net/blog/openstacks-and-ecosystems/</guid>
      <description>&lt;p&gt;I have recently had a number of lengthy discussions on the &lt;a href=&#34;https://twitter.com/zehicle/status/678736665792356352&#34;&gt;Twitter&lt;/a&gt; about Interop, Users, and Ecosystems. Specifically about our need to focus on the OpenStack ecosystem to extend the OpenStack IaaS user experience to something a bit more platform[ish].&lt;/p&gt;

&lt;p&gt;I wrote a post for &lt;a href=&#34;http://sysadvent.blogspot.com/2015/12/day-16-merry-paasmas-and-very.html&#34;&gt;SysAdvent&lt;/a&gt; this year on developing applications on top of OpenStack using a collection of OpenSource tools to create a PaaS and CI/CD pipelines. I think it turned out quite well and really helped reinforce my beliefs on the subject.&lt;/p&gt;

&lt;p&gt;My buddy and future OpenStack Board member &lt;a href=&#34;https://twitter.com/jjasghar&#34;&gt;JJ Asghar&lt;/a&gt; has been spearheading a new &lt;a href=&#34;https://wiki.openstack.org/wiki/Osops&#34;&gt;OpenStack Operators Project&lt;/a&gt;. I plan to contribute to this project by creating some examples of deploying tools that provide higher level services on top of the OpenStack IaaS layer.&lt;/p&gt;

&lt;p&gt;Given that I am very bullish about the &lt;a href=&#34;http://docker.com&#34;&gt;Docker&lt;/a&gt; ecosystem it makes sense that my first contribution would be focussed on running one of the several &amp;ldquo;Docker container scheduling/cluster&amp;rdquo; tools.&lt;/p&gt;

&lt;p&gt;After playing around with a few of them, I settled on starting with Docker Swarm as its one of the easier to understand and run and doesn&amp;rsquo;t require any special tooling other than a recent install of the Docker binary to use.&lt;/p&gt;

&lt;p&gt;To increase simplicity I chose to use Hashicorp&amp;rsquo;s &lt;a href=&#34;http://terraform.io&#34;&gt;Terraform&lt;/a&gt; and use only the most basic of the OpenStack services to ensure a fairly high likelyhood that it will run on most fairly up to date OpenStack clouds.&lt;/p&gt;

&lt;p&gt;Based on the project&amp;rsquo;s suggestion I posted the Terraform files up to the &lt;a href=&#34;https://github.com/openstack/osops-tools-contrib/tree/master/terraform/dockerswarm-coreos&#34;&gt;osops-tools-contrib&lt;/a&gt; along with fairly comprehensive documentation on using it.&lt;/p&gt;

&lt;p&gt;I hope this and future work I plan to do to create similar examples will help the OpenStack Community out in some small way.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Optimizing your Dockerfiles</title>
      <link>https://tech.paulcz.net/blog/optimizing-your-dockerfiles/</link>
      <pubDate>Sat, 07 Mar 2015 13:25:29 -0600</pubDate>
      
      <guid>https://tech.paulcz.net/blog/optimizing-your-dockerfiles/</guid>
      <description>

&lt;p&gt;Docker images are &amp;ldquo;supposed&amp;rdquo; to be small and fast. However unless you&amp;rsquo;re precompiling GO binaries and dropping them in the &lt;code&gt;busybox&lt;/code&gt; image they can get quite large and complicated. Without a well constructed &lt;code&gt;Dockerfile&lt;/code&gt; to improve build cache hits your docker builds can become unnecessarily slow.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Dockerfile&lt;/code&gt;&amp;rsquo;s are regularly [and incorrectly] treated like &lt;code&gt;bash&lt;/code&gt; scripts and therefore are often written out as a series of commands which you would &lt;code&gt;curl | sudo bash&lt;/code&gt; from a website to install.  This usually makes for an inefficient and slow &lt;code&gt;Dockerfile&lt;/code&gt;&lt;/p&gt;

&lt;!--more --&gt;

&lt;h2 id=&#34;order-matters&#34;&gt;Order Matters&lt;/h2&gt;

&lt;p&gt;When you&amp;rsquo;re building a new &lt;code&gt;Dockerfile&lt;/code&gt; for an application there can be a lot of trial and error in determining what packages are needed and what commands need to run. Optimizing your &lt;code&gt;Dockerfile&lt;/code&gt; ensures that the build cache will hit more often and each build between changes will be faster.&lt;/p&gt;

&lt;p&gt;The general rule of thumb is to sort your commands by frequency of change, the time it takes to run the command and how sharable it is with other images.&lt;/p&gt;

&lt;p&gt;This means that commands like &lt;code&gt;WORKDIR&lt;/code&gt;, &lt;code&gt;CMD&lt;/code&gt;, &lt;code&gt;ENV&lt;/code&gt; should go towards the bottom while a &lt;code&gt;RUN apt-get -y update&lt;/code&gt; should go towards the top as it takes longer to run and can be shared with all of your images.&lt;/p&gt;

&lt;p&gt;Finally any &lt;code&gt;ADD&lt;/code&gt; ( or other commands that invalidate cache ) commands should go as far down the bottom as possible as this is where you&amp;rsquo;re likely to make lots of changes that will invalidate the cache of subsequent commands.&lt;/p&gt;

&lt;h2 id=&#34;choose-your-base-image-wisely&#34;&gt;Choose your base image wisely&lt;/h2&gt;

&lt;p&gt;There&amp;rsquo;s a lot of base images to choose from from the bare OS images like &lt;code&gt;ubuntu:trusty&lt;/code&gt; to application specific ones for &lt;code&gt;python:2&lt;/code&gt; or &lt;code&gt;java:7&lt;/code&gt;.  Common sense might tell you to use &lt;code&gt;ruby:2&lt;/code&gt; to run an ruby based app and &lt;code&gt;python:3&lt;/code&gt; to run a python app.  However now you have two base images with little in common that you need to download and build.  Instead if you use &lt;code&gt;ubuntu:trusty&lt;/code&gt; for both then you only need to download the base image once.&lt;/p&gt;

&lt;h2 id=&#34;use-layers-to-your-advantage&#34;&gt;Use Layers to your advantage&lt;/h2&gt;

&lt;p&gt;Each command in a &lt;code&gt;Dockerfile&lt;/code&gt; is an extra layer. You can very quickly end up with an image that&amp;rsquo;s 30+ layers.  This is not necessarily a problem, but by joining &lt;code&gt;RUN&lt;/code&gt; commands together, and using a single &lt;code&gt;EXPOSE&lt;/code&gt; line to list all of your open ports you can reduce the number of layers.&lt;/p&gt;

&lt;p&gt;By grouping &lt;code&gt;RUN&lt;/code&gt; commands together intelligently you can share more layers between containers.  Of course if you have a common set of packages across multiple containers then you should look at creating a seperate base image containing these that all of your images are built from.&lt;/p&gt;

&lt;p&gt;For each layer that you can share across multiple images you can save a ton of disk space.&lt;/p&gt;

&lt;h2 id=&#34;volume-contaimers&#34;&gt;Volume contaimers&lt;/h2&gt;

&lt;p&gt;If you use Volume containers,  don&amp;rsquo;t bother trying to save space by using a small image,  Use the image of the application you&amp;rsquo;ll be serving data to.  If you do that and &lt;code&gt;docker commit&lt;/code&gt; the data volume you not only have your data commited to the container, but the actual application as well which is very useful for debugging.&lt;/p&gt;

&lt;h2 id=&#34;cheat&#34;&gt;Cheat&lt;/h2&gt;

&lt;p&gt;If you&amp;rsquo;ve built an image and discover when you run it that there&amp;rsquo;s a package missing add it to the bottom of your &lt;code&gt;Dockerfile&lt;/code&gt; rather than in the &lt;code&gt;RUN apt-get&lt;/code&gt; command at the top.  This means you can rebuild the image faster.  Once your image is correct and working you can reorganize your &lt;code&gt;Dockerfile&lt;/code&gt; to clean such changes up before commiting it to source control.&lt;/p&gt;

&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;

&lt;p&gt;A &lt;code&gt;Dockerfile&lt;/code&gt; for installing graphite would look something like this if it was written like a &lt;code&gt;bash&lt;/code&gt; script:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM ubuntu:trusty
MAINTAINER Paul Czarkowski &amp;quot;paul@paulcz.net&amp;quot;

RUN apt-get -yq update

# Apache
RUN \
  apt-get -yqq install \
    apache2 \
    apache2-utils \
    libapache2-mod-python \
    python-dev \
    python-pip \
    python-cairo \
    python-pysqlite2 \
    python-mysqldb \
    python-jinja2
    sqlite3 \
    curl \ 
    wget \
    git \
    software-properties-common

RUN \
  curl -sSL https://bootstrap.pypa.io/get-pip.py | python &amp;amp;&amp;amp; \
    pip install whisper \
    carbon \
    graphite-web \
    &#39;Twisted&amp;lt;12.0&#39; \
    &#39;django&amp;lt;1.6&#39; \
    django-tagging

# Add start scripts etc
ADD . /app

RUN mkdir -p /app/wsgi
RUN useradd -d /app -c &#39;application&#39; -s &#39;/bin/false&#39; graphite
RUN chmod +x /app/bin/*
RUN chown -R graphite:graphite /app
RUN chown -R graphite:graphite /opt/graphite
RUN rm -f /etc/apache2/sites-enabled/*

ADD ./apache-graphite.conf /etc/apache2/sites-enabled/apache-graphite.conf

# Expose ports.
EXPOSE 80 
EXPOSE 2003 
EXPOSE 2004 
EXPOSE 7002

ENV APACHE_CONFDIR /etc/apache2
ENV APACHE_ENVVARS $APACHE_CONFDIR/envvars
ENV APACHE_RUN_USER www-data
ENV APACHE_RUN_GROUP www-data
ENV APACHE_RUN_DIR /var/run/apache2
ENV APACHE_PID_FILE $APACHE_RUN_DIR/apache2.pid
ENV APACHE_LOCK_DIR /var/lock/apache2
ENV APACHE_LOG_DIR /var/log/apache2

WORKDIR /app

# Define default command.
CMD [&amp;quot;/app/bin/start_graphite&amp;quot;]

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However an optmized version of this same Dockerfile based on what was discussed earlier would look like the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 1 - Common Header / Packages
FROM ubuntu:trusty
MAINTAINER Paul Czarkowski &amp;quot;paul@paulcz.net&amp;quot;

RUN apt-get -yq update \
  &amp;amp;&amp;amp; apt-get -yqq install \
    wget \
    curl \
    git \
    software-properties-common

# 2 - Python
RUN \
  apt-get -yqq install \
    python-dev \
    python-pip \
    python-pysqlite2 \
    python-mysqldb

# 3 - Apache
RUN \
  apt-get -yqq install \
    apache2 \
    apache2-utils

# 4 - Apache ENVs
ENV APACHE_CONFDIR /etc/apache2
ENV APACHE_ENVVARS $APACHE_CONFDIR/envvars
ENV APACHE_RUN_USER www-data
ENV APACHE_RUN_GROUP www-data
ENV APACHE_RUN_DIR /var/run/apache2
ENV APACHE_PID_FILE $APACHE_RUN_DIR/apache2.pid
ENV APACHE_LOCK_DIR /var/lock/apache2
ENV APACHE_LOG_DIR /var/log/apache2

# 5 - Graphite and Deps
RUN \
  apt-get -yqq install \
    libapache2-mod-python \
    python-cairo \
    python-jinja2 \
    sqlite3

RUN \
    pip install whisper \
    carbon \
    graphite-web \
    &#39;Twisted&amp;lt;12.0&#39; \
    &#39;django&amp;lt;1.6&#39; \
    django-tagging

# 6 - Other
EXPOSE 80 2003 2004 7002

WORKDIR /app

VOLUME /opt/graphite/data

# Define default command.
CMD [&amp;quot;/app/bin/start_graphite&amp;quot;]

# 7 - First use of ADD
ADD . /app

# 8 - Final setup
RUN mkdir -p /app/wsgi \
  &amp;amp;&amp;amp; useradd -d /app -c &#39;application&#39; -s &#39;/bin/false&#39; graphite \
  &amp;amp;&amp;amp; chmod +x /app/bin/* \
  &amp;amp;&amp;amp; chown -R graphite:graphite /app \
  &amp;amp;&amp;amp; chown -R graphite:graphite /opt/graphite \
  &amp;amp;&amp;amp; rm -f /etc/apache2/sites-enabled/* \
  &amp;amp;&amp;amp; mv /app/apache-graphite.conf /etc/apache2/sites-enabled/apache-graphite.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;1-common-header-packages&#34;&gt;1 - Common Header / Packages&lt;/h3&gt;

&lt;p&gt;This is our most shareable layer.  All the images running on the same host should start with this.  You can see I&amp;rsquo;ve added a few things like &lt;code&gt;curl&lt;/code&gt; and &lt;code&gt;git&lt;/code&gt; which while they&amp;rsquo;re not necessarily needed they&amp;rsquo;re useful for debugging and because they&amp;rsquo;re in such a shareable layer,  they don&amp;rsquo;t take up much room.&lt;/p&gt;

&lt;h3 id=&#34;2-python-3-apache&#34;&gt;2 - Python, 3 - Apache&lt;/h3&gt;

&lt;p&gt;Here we get to our language specifications.   I&amp;rsquo;ve included the Python and Apache sections here because it&amp;rsquo;s not super clear which should go first.&lt;/p&gt;

&lt;p&gt;If we put python first,  then any other image that uses Apache can get a few free python packages,  If we put Apache first then we could have a Ruby app that also includes that layer and get Apache for free ( hell you can just give it python for free anyways ).&lt;/p&gt;

&lt;h3 id=&#34;4-apache-envs&#34;&gt;4 - Apache Envs&lt;/h3&gt;

&lt;p&gt;I&amp;rsquo;m calling these out seperately for a few reasons.&lt;/p&gt;

&lt;p&gt;Firstly, they should come either directly directly after the Apache section so that it&amp;rsquo;s easier to make them common ( and cached ) between multiple images.   You might not think it matters since calls like &lt;code&gt;ENV&lt;/code&gt; are so cheap, but I have seen random &lt;code&gt;ENV&lt;/code&gt; calls take 10 seconds or so.  If you have a lot, then its good to keep them cached, but you also don&amp;rsquo;t want a changed &lt;code&gt;ENV&lt;/code&gt; to invalidated the cache of installing Apache.&lt;/p&gt;

&lt;p&gt;They&amp;rsquo;re a pretty good example of something you might want to start with at the bottom of your container and move them up higher once you&amp;rsquo;re unlikely to change them again.&lt;/p&gt;

&lt;p&gt;Secondly, to mention that I really wish Docker provided a way to specify multiple ENVS on the same line so that I can reduce the number of layers I end up with.&lt;/p&gt;

&lt;h3 id=&#34;5-graphite-and-deps&#34;&gt;5 - Graphite and Deps&lt;/h3&gt;

&lt;p&gt;This contains some Graphite specific &lt;code&gt;apt&lt;/code&gt; and &lt;code&gt;pip&lt;/code&gt; packages.  You could join them into a single command by joining them with &lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt; but I kept them seperate so that if &lt;code&gt;pip&lt;/code&gt; package requirements change it won&amp;rsquo;t need to also reget the &lt;code&gt;apt&lt;/code&gt; packages.&lt;/p&gt;

&lt;h3 id=&#34;6-other&#34;&gt;6 - Other&lt;/h3&gt;

&lt;p&gt;This contains a bunch of cheap commands like &lt;code&gt;ADD&lt;/code&gt; and &lt;code&gt;VOLUME&lt;/code&gt; they&amp;rsquo;re probably less likely to change than the previous package installs, but are also cheaper to run, so its less important if their cache is invalidated.&lt;/p&gt;

&lt;p&gt;Keep them towards the bottom though as you don&amp;rsquo;t want any changes to them to invalidate the cache for a more costly command.&lt;/p&gt;

&lt;h3 id=&#34;7-first-add&#34;&gt;7 - First ADD&lt;/h3&gt;

&lt;p&gt;You should wait until the last possible moment to use the &lt;code&gt;ADD&lt;/code&gt; command as any commands after it are never cached.&lt;/p&gt;

&lt;h3 id=&#34;8-final-setup&#34;&gt;8 - Final setup&lt;/h3&gt;

&lt;p&gt;I have grouped these final commands into a single layer and they&amp;rsquo;re after the &lt;code&gt;ADD&lt;/code&gt; commands as they manipulate files that come from the &lt;code&gt;ADD&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&#34;fin&#34;&gt;FIN.&lt;/h2&gt;

&lt;p&gt;Hopefully this has given you some insight into how to build a better &lt;code&gt;Dockerfile&lt;/code&gt;.  These are all things I have learned from experience in building my own Docker images and while they may not apply to all situations ( or may be flat out wrong ) they defintely seem to improve my development experience.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Factorish and The Twelve-Fakter App</title>
      <link>https://tech.paulcz.net/blog/factorish_and_the_12_fakter_app/</link>
      <pubDate>Tue, 06 Jan 2015 13:29:27 -0600</pubDate>
      
      <guid>https://tech.paulcz.net/blog/factorish_and_the_12_fakter_app/</guid>
      <description>&lt;p&gt;Unless you&amp;rsquo;ve been living under a rock (in which case I envy you) you&amp;rsquo;ve heard a fair bit about The &lt;a href=&#34;http://12factor.net&#34;&gt;Twelve-Factor App&lt;/a&gt;. A wonderful stateless application that is completely disposable and can run anywhere from your own physical servers to &lt;a href=&#34;http://deis.io&#34;&gt;Deis&lt;/a&gt;, &lt;a href=&#34;http://cloudfoundry.org&#34;&gt;Cloud Foundry&lt;/a&gt; or &lt;a href=&#34;http://heroku.com&#34;&gt;Heroku&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Chances are you&amp;rsquo;re stuck writing and running an application that is decidely not 12Factor, nor will it ever be.  In a perfect world you&amp;rsquo;d scrap it and rewrite it as a dozen microservices that are loosely coupled but run and work indepently of eachother. The reality however is you could never get the okay to do that.&lt;/p&gt;

&lt;p&gt;Fortunately with the rise of &lt;a href=&#34;http://docker.com&#34;&gt;Docker&lt;/a&gt; and its ecosystem it has become easier to not only write 12Factor apps, but also to fake it by producing a Docker container that acts like a 12Factor app, but contains something that is decidedly not.  I call this the 12Fakter app.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve been playing with this concept for a while, but over Christmas I spent a bunch of time trying to figure out the best ways to fake out the 12 Factors and feel that I&amp;rsquo;ve come up with something that works pretty well and in the process created a Vagrant based development sandbox called &lt;a href=&#34;http://github.com/paulczar/factorish&#34;&gt;Factorish&lt;/a&gt; which I used to create &lt;a href=&#34;http://github.com/paulczar/12fakter-wordpress&#34;&gt;12fakter-wordpress&lt;/a&gt; and &lt;a href=&#34;https://github.com/paulczar/docker-elk_confd&#34;&gt;elk_confd&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;fakter-i-codebase&#34;&gt;Fakter I. Codebase&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;One codebase tracked in revision control, many deploys&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The goal here is to have both your app and deployment tooling in the same codebase which is stored in source control.  This means adding a &lt;code&gt;Dockerfile&lt;/code&gt;, and &lt;code&gt;Vagrantfile&lt;/code&gt; and other pieces of tooling into your codebase.  If however you have a monolithic codebase that contains more than just your app you can create a seperate codebase ( use git! ) containing this tooling and have that tooling collect the application from its existing codebase.&lt;/p&gt;

&lt;p&gt;You should be able to achieve this by either merging &lt;a href=&#34;http://github.com/paulczar/factorish&#34;&gt;Factorish&lt;/a&gt; into your existing git repo,  or fork it and use the &lt;code&gt;Dockerfile&lt;/code&gt; in it to pull the actual application code in as part of the build process.&lt;/p&gt;

&lt;h2 id=&#34;fakter-ii-dependencies&#34;&gt;Fakter II. Dependencies&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Explicitly declare and isolate dependencies&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is a really easy win with Docker,  The very nature of Docker both Explicitly declares your dependencies in the form of the &lt;code&gt;Dockerfile&lt;/code&gt; and Isolates them in the form of the built Docker image.&lt;/p&gt;

&lt;h3 id=&#34;declaration&#34;&gt;Declaration&lt;/h3&gt;

&lt;h4 id=&#34;app-example-dockerfile&#34;&gt;/app/example/Dockerfile&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;FROM python:2

# Base deps layer
RUN \
  apt-get update &amp;amp;&amp;amp; apt-get install -yq \
  make \
  ca-certificates \
  net-tools \
  sudo \
  wget \
  vim \
  strace \
  lsof \
  netcat \
  lsb-release \
  locales \
  socat \
  supervisor \
  --no-install-recommends &amp;amp;&amp;amp; \
  locale-gen en_US.UTF-8

# etcdctl and confd layer
RUN \
  curl -sSL -o /usr/local/bin/etcdctl https://s3-us-west-2.amazonaws.com/opdemand/etcdctl-v0.4.6 \
  &amp;amp;&amp;amp; chmod +x /usr/local/bin/etcdctl \
  &amp;amp;&amp;amp; curl -sSL -o /usr/local/bin/confd https://github.com/kelseyhightower/confd/releases/download/v0.7.1/confd-0.7.1-linux-amd64 \
  &amp;amp;&amp;amp; chmod +x /usr/local/bin/confd

ADD . /app
WORKDIR /app

# app layer
RUN \
  useradd -d /app -c &#39;application&#39; -s &#39;/bin/false&#39; app &amp;amp;&amp;amp; \
  chmod +x /app/bin/* &amp;amp;&amp;amp; \
  pip install -r /app/example/requirements.txt

# Define default command.
CMD [&amp;quot;/app/bin/boot&amp;quot;]

# Expose ports.
EXPOSE 8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You might notice I have sets of commands joined together with &lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt; in my &lt;code&gt;Dockerfile&lt;/code&gt;, I do this to control the docker layers more to try and end up with fewer more meaningful layers.&lt;/p&gt;

&lt;h3 id=&#34;isolation&#34;&gt;Isolation&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ docker build -t factorish/example example
Sending build context to Docker daemon 20.99 kB
Sending build context to Docker daemon
Step 0 : FROM python:2
 ---&amp;gt; 96e13ecb4dba
...
...
Step 8 : EXPOSE 8080
 ---&amp;gt; Running in 8dc9a04eaf78
 ---&amp;gt; 374cb835239c
Removing intermediate container 8dc9a04eaf78
Successfully built 374cb835239c
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;fakter-iii-configuration&#34;&gt;Fakter III. Configuration&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Store config in the environment&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Another easy win with Docker.   You can pass in environment variables in the &lt;code&gt;Dockerfile&lt;/code&gt; as we as when running the docker container using the &lt;code&gt;-e&lt;/code&gt; option like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d -e TEXT=bacon factorish/example
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However chances are your app reads from a config file rather than environment variables. There are [at least] two fairly simple ways to achieve this.&lt;/p&gt;

&lt;h3 id=&#34;sed-inline-replacement&#34;&gt;sed inline replacement&lt;/h3&gt;

&lt;p&gt;use a startup script to edit your config file and replace values in it with the values of the environment variables using &lt;code&gt;sed&lt;/code&gt; before runnin your app:&lt;/p&gt;

&lt;h4 id=&#34;app-bin-boot&#34;&gt;/app/bin/boot&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
sed -i &amp;quot;s/xxxTEXTxxx/${TEXT}&amp;quot; /app/example/example.conf
python /app/example/app.py
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;confd-templating&#34;&gt;confd templating&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kelseyhightower/confd&#34;&gt;confd&lt;/a&gt; is a tool written specifically for templating config files from data sources such as environment variables.  This is a much better option as it also opens up the ability to use service discovery tooling like &lt;a href=&#34;https://coreos.com/using-coreos/etcd/&#34;&gt;etcd&lt;/a&gt; (also supported in Factorish) rather than environment variables.&lt;/p&gt;

&lt;h4 id=&#34;app-conf-d-example-conf-toml&#34;&gt;/app/conf.d/example.conf.toml&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;[template]
src   = &amp;quot;example.conf&amp;quot;
dest  = &amp;quot;/app/example/example.conf&amp;quot;
keys = [&amp;quot;/services/example&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;app-templates-example-conf&#34;&gt;/app/templates/example.conf&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;[example]
text: {{ getv &amp;quot;/services/example/text&amp;quot; }}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;The &lt;code&gt;{{ }}&lt;/code&gt; syntax above is the golang/confd macros used to perform tasks like fetching variables from etcd or environment.&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&#34;app-bin-boot-1&#34;&gt;/app/bin/boot&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
confd -onetime
python /app/example/app.py
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;fakter-iv-backing-services&#34;&gt;Fakter IV. Backing Services&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Treat backing services as attached resources&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Anything that is needed to store persistent data should be treated as an external dependency to your application.  As far as your app is concerned there should be no difference between a local MySQL server or Amazon&amp;rsquo;s RDS.&lt;/p&gt;

&lt;p&gt;This is easier for some backing services than others.  For example if your app requires a MySQL database its relatively straight forward.  Whereas a local filesystem for storing images is harder, but can be solved:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Docker: volume mounts, data containers&lt;/li&gt;
&lt;li&gt;Remote Storage: netapp, nfs, fuse-s3fs&lt;/li&gt;
&lt;li&gt;Clustered FS: drdb, gluster&lt;/li&gt;
&lt;li&gt;Ghetto: rsync + concerned&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The docker volume mounts actually work really well in a vagrant based development environment because you can pass your code all the way into the container from your workstation,  however there are definitely some security considerations to think about if you want to do volume mounts in production.&lt;/p&gt;

&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;

&lt;p&gt;A fictional &lt;strong&gt;PHP&lt;/strong&gt; based blog about bacon requires a database and a filestore:&lt;/p&gt;

&lt;h4 id=&#34;app-templates-config-php&#34;&gt;/app/templates/config.php&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;define(&#39;DB_NAME&#39;, &#39;{{ getv &amp;quot;/db/name&amp;quot; }}&#39;);
define(&#39;DB_USER&#39;, &#39;{{ getv &amp;quot;/db/user&amp;quot; }}&#39;);
define(&#39;DB_PASSWORD&#39;, &#39;{{ getv &amp;quot;/db/pass&amp;quot; }}&#39;);
define(&#39;DB_HOST&#39;, &#39;{{ getv &amp;quot;/db/host&amp;quot; }}&#39;);
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;docker-run-command&#34;&gt;Docker Run command&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d -e DB_NAME=bacon -e DB_USER=bacon \
  -e DB_PASSWORD=bacon $DB_HOST=my.database.com \
  -v /mnt/nfs/bacon:/app/bacon factorish/bacon-blog
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;confd will use the environment variables passed in via the &lt;code&gt;docker run&lt;/code&gt; command to fill out the variables called in the &lt;code&gt;{{ }}&lt;/code&gt; macros.  Note that confd transforms the environment variables so that the environment variable &lt;code&gt;DB_USER&lt;/code&gt; will be read by &lt;code&gt;{{ getv &amp;quot;/db/user&amp;quot; }}&lt;/code&gt;.  This is done to normalize the macro across the various data source options.&lt;/p&gt;

&lt;h2 id=&#34;fakter-v-build-release-run&#34;&gt;Fakter V. Build, Release, Run&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Strictly separate build and run stages&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;build&#34;&gt;Build&lt;/h3&gt;

&lt;p&gt;Converts a code repo into an executable bundle. Sound familiar?  Yup, we&amp;rsquo;ve already solved this with our &lt;code&gt;Dockerfile&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;release&#34;&gt;Release&lt;/h3&gt;

&lt;p&gt;Takes the build and combines it with the current configuration. In a purely docker based system this can be split between the &lt;strong&gt;Build&lt;/strong&gt; (versioning and defaults) and &lt;strong&gt;Run&lt;/strong&gt; (current config) stages. However systems like Heroku and Deis have a seperate step for this which they handle internally.&lt;/p&gt;

&lt;h3 id=&#34;run&#34;&gt;Run&lt;/h3&gt;

&lt;p&gt;Runs the application by launching a set of the app&amp;rsquo;s processes against a selected release.  In a docker based system this is simply the &lt;code&gt;$ docker run&lt;/code&gt; command which can be called via a deploy script, or a init script (systemd/runit) or a scheduler like &lt;a href=&#34;https://coreos.com/using-coreos/clustering/&#34;&gt;fleet&lt;/a&gt; or &lt;a href=&#34;http://mesos.apache.org/&#34;&gt;mesos&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;fakter-vi-processes&#34;&gt;Fakter VI. Processes&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Execute the app as one or more stateless processes&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Your application inside the docker container should behave like a standard linux process running in the foreground and be stateless and share-nothing.  Being inside a docker container means that this is hidden and therefore we can fairly easily fake this but you do need to think about process management and logging which are discussed later and is further explored &lt;a href=&#34;http://tech.paulcz.net/2014/12/multi-process-docker-images-done-right/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;fakter-vii-port-binding&#34;&gt;Fakter VII. Port binding&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Export services via port binding&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Your application should appear to be completely self contained and not require runtime injection of a webserver.  Thankfully this is pretty easy to fake in a docker container as any extra processes are isolated in the container and effectively invisible to the outside.&lt;/p&gt;

&lt;p&gt;It is still preferable to use a native language based web library such as jetty (java) or flask (python) but for languages like PHP using apache or nginx is ok.&lt;/p&gt;

&lt;p&gt;Docker itself takes care of the port binding by use of the &lt;code&gt;-p&lt;/code&gt; option on the command line.  It&amp;rsquo;s useful to register the port and host IP to somewhere ( etcd ) to allow for loadbalancers and other services to easily locate your application.&lt;/p&gt;

&lt;h2 id=&#34;fakter-viii-concurrency&#34;&gt;Fakter VIII. Concurrency&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Scale out via the process model&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We should be able to scale up or down simply by creating or destroying docker containers containing the application.  Any upstream load balancers as an external dependency would need to be notified of the container starting ( usually a fairly easy API call) and stopping.  But these are external dependencies and should be solved outside of your application itself.&lt;/p&gt;

&lt;p&gt;Inside the container your application should not daemonize or write pid files (if unavoidable, not too difficult to script around) and use tooling like &lt;code&gt;upstart&lt;/code&gt; or &lt;code&gt;supervisord&lt;/code&gt; if there is more than one process that needs to be run.&lt;/p&gt;

&lt;h2 id=&#34;fakter-ix-disposability&#34;&gt;Fakter IX. Disposability&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Maximize robustness with fast startup and graceful shutdown&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Docker helps a lot with this.   We want to ensure that we&amp;rsquo;re optimized for fast yet reliable startup as well as graceful shutdown.  Your app should be able to be shut down gracefully when &lt;code&gt;docker kill&lt;/code&gt; is called and just as importantly there should be minimal if any external effect if the application crashes or stops ungracefully.&lt;/p&gt;

&lt;p&gt;The container itself should kill itself if the app inside it stops working right.  If your app is running behind a &lt;a href=&#34;http://tech.paulcz.net/2014/12/multi-process-docker-images-done-right/&#34;&gt;supervisor&lt;/a&gt; this can be a achieved with a really lightweight healthcheck script like this.&lt;/p&gt;

&lt;h4 id=&#34;app-bin-healhthcheck&#34;&gt;/app/bin/healhthcheck&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
while [[ ! -z $(netstat -lnt | awk &amp;quot;\$6 == \&amp;quot;LISTEN\&amp;quot; &amp;amp;&amp;amp; \$4 ~ \&amp;quot;.$PORT\&amp;quot; &amp;amp;&amp;amp; \$1 ~ \&amp;quot;tcp.?\&amp;quot;&amp;quot;) ]] ; do
  [[ -n $ETCD_HOST ]] &amp;amp;&amp;amp; etcdctl set /service/web/hosts/$HOST $PORT --ttl 10 &amp;gt;/dev/null
  sleep 5
done
kill `cat /var/run/supervisord.pid`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You&amp;rsquo;ll note that I&amp;rsquo;m also publishing host and port values to etcd if &lt;code&gt;$ETCD_HOST&lt;/code&gt; is set.  This can then be used to notify loadbalancers and the like when services start or stop.&lt;/p&gt;

&lt;h2 id=&#34;fakter-x-dev-prod-parity&#34;&gt;Fakter X. Dev/prod parity&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Keep development, staging, and production as similar as possible&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;By following the previous fackters we&amp;rsquo;ve done most of the work to make this possible.  We use Vagrant in development to deploy your app (and any backing services) using the appropriate provisioning methodology ( the same ones we&amp;rsquo;d use for production).&lt;/p&gt;

&lt;p&gt;By wrapping the application in a docker container it is portable across just about any system that is capable of running docker.&lt;/p&gt;

&lt;p&gt;By provisioning with the same tooling to both dev and prod (and any other envs),  any deployment of development (should happen frequently) is also a test of most of the tooling used to deploy to production.&lt;/p&gt;

&lt;h2 id=&#34;fakter-xi-logs&#34;&gt;Fakter XI. Logs&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Treat logs as event streams&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Your application ( even inside the container ) should always log to stdout. By writing to stdout of your process we can utilize the docker logging subsystem which when combined with tooling like &lt;a href=&#34;https://registry.hub.docker.com/u/progrium/logspout/&#34;&gt;logspout&lt;/a&gt; makes it very easy to push all logs to a central system.&lt;/p&gt;

&lt;p&gt;If your app &lt;em&gt;has&lt;/em&gt; to write to a logfile you should be able to configure that log file to be &lt;code&gt;/dev/stdout&lt;/code&gt; which should cause it to write to stdout of the process. If your app only writes to syslog then configure it to write to a remote syslog. Basically do whatever you can to ensure you don&amp;rsquo;t log to the local filesystem.&lt;/p&gt;

&lt;h3 id=&#34;example-1&#34;&gt;Example&lt;/h3&gt;

&lt;p&gt;This example shows running &lt;code&gt;Supervisord&lt;/code&gt; as your primary process in the docker container and &lt;code&gt;nginx&lt;/code&gt; writing logs to stdout which in turn are written to the containers &lt;code&gt;stdout&lt;/code&gt;.  A more thorough writeup on using &lt;a href=&#34;http://tech.paulcz.net/2014/12/multi-process-docker-images-done-right/&#34;&gt;supervisor&lt;/a&gt; inside docker containers can be found &lt;a href=&#34;http://tech.paulcz.net/2014/12/multi-process-docker-images-done-right/&#34;&gt;here&lt;/a&gt;:&lt;/p&gt;

&lt;h4 id=&#34;etc-supervisor-conf-d-nginx&#34;&gt;/etc/supervisor/conf.d/nginx&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;[supervisord]
logfile=/dev/null
pidfile=/var/run/supervisord.pid
nodaemon=true

[program:nginx]
command=/usr/sbin/nginx
redirect_stderr=true
stdout_logfile=/dev/stdout
stdout_logfile_maxbytes=0
auto_start=true
autorestart=true
user=root
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;etc-nginx-sites-enabled-app&#34;&gt;/etc/nginx/sites-enabled/app&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;worker_processes 1;
daemon off;
error_log /dev/stdout;
http {
  access_log /dev/stdout;
  server {
    listen            *:8080;
    root              /app/bacon-blog;
    index             index.php;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For a more detailed post on using logspout to produce consumable logs check out &lt;a href=&#34;https://twitter.com/behemphi&#34;&gt;@behemphi&lt;/a&gt;&amp;rsquo;s blog post - &lt;a href=&#34;http://stackengine.com/docker-logs-aggregating-ease/&#34;&gt;Docker Logs – Aggregating with Ease&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;fakter-xii-admin-processes&#34;&gt;Fakter XII. Admin processes&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Run admin/management tasks as one-off processes&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This one is pretty easy.  Tasks such as database migrates should be run in one off throw-away containers.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -t -e DB_SERVER=user@pass:db.server.com myapp:1.3.2 rake db:migrate
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Most of the fakters above are relatively straight forward to utilize and can be built upon slowly, no need to perfect things before working on them.  They can also be utilized with any existing provisioning / config management tooling that you already have.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re already using &lt;a href=&#34;http://chef.io&#34;&gt;chef&lt;/a&gt; for deploying your application you can use the &lt;a href=&#34;https://supermarket.chef.io/cookbooks/docker&#34;&gt;docker cookbook&lt;/a&gt; to start running docker containers instead and write out confd templates rather than the final config file which confd will then use to do the final configuration of your app from the environment variables you pass through to the &lt;code&gt;docker_run&lt;/code&gt; resource in the cookbook.&lt;/p&gt;

&lt;p&gt;Making your application act like a 12Factor app may not be enough to run it on a purely hosted PAAS like Heroku, but chances are you&amp;rsquo;ll be able to run it on a Docker based PAAS like Deis.  You can go full stack with Mesos or CoreOS+Fleet+ETCD or you can stick to Ubuntu servers running docker.&lt;/p&gt;

&lt;p&gt;The flexibility that the 12fakter application gives you means that you can move to a more modern infrastructure at your own pace when it makes sense without having to abandon or completely rewrite your existing applications.&lt;/p&gt;

&lt;p&gt;Please check out &lt;a href=&#34;http://github.com/paulczar/factorish&#34;&gt;Factorish&lt;/a&gt; and some of the example 12fakter apps like &lt;a href=&#34;http://github.com/paulczar/12fakter-wordpress&#34;&gt;12fakter-wordpress&lt;/a&gt; and &lt;a href=&#34;https://github.com/paulczar/docker-elk_confd&#34;&gt;elk_confd&lt;/a&gt;. to see how easy it can be to start making your applications act like 12Factor apps.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Multi Process Docker Images Done Right</title>
      <link>https://tech.paulcz.net/blog/multi-process-docker-images-done-right/</link>
      <pubDate>Mon, 22 Dec 2014 21:31:03 -0600</pubDate>
      
      <guid>https://tech.paulcz.net/blog/multi-process-docker-images-done-right/</guid>
      <description>&lt;h2 id=&#34;for-some-values-of-right&#34;&gt;For some values of &amp;lsquo;right&amp;rsquo;&lt;/h2&gt;

&lt;p&gt;Almost since &lt;a href=&#34;http://docker.com&#34;&gt;Docker&lt;/a&gt; was first introduced to the world there has been a fairly strong push to keeping containers to be single process.   This makes a lot of sense and definitely plays into the &lt;a href=&#34;http://12factor.net&#34;&gt;12 Factor&lt;/a&gt; way of thinking where all application output should be pushed to &lt;code&gt;stdout&lt;/code&gt; and docker itself with tools like &lt;a href=&#34;https://github.com/progrium/logspout&#34;&gt;logspout&lt;/a&gt; now has fairly strong tooling to deal with those logs.&lt;/p&gt;

&lt;p&gt;Sometimes however it just makes sense to run more than one process in a container,  a perfect example would be running &lt;a href=&#34;https://github.com/kelseyhightower/confd&#34;&gt;confd&lt;/a&gt; as well as your application in order to modify the application&amp;rsquo;s config file based on changes in service discovery systems like &lt;a href=&#34;https://github.com/coreos/etcd&#34;&gt;etcd&lt;/a&gt;.   The &lt;a href=&#34;https://docs.docker.com/articles/ambassador_pattern_linking/&#34;&gt;ambassador&lt;/a&gt; container way of working can achieve similar things, but I&amp;rsquo;m not sure that running two containers with a process each to run your application is any better than running one container with two processes.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re going run multiple processes you have a few options to do it.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Start the container with the first process adnd then use the new &lt;code&gt;docker exec&lt;/code&gt; command to start the second.&lt;/li&gt;
&lt;li&gt;Start them in sequence in a &lt;code&gt;bash&lt;/code&gt; script and background all but the last process with a &lt;code&gt;&amp;amp;&lt;/code&gt; at the end of the line.&lt;/li&gt;
&lt;li&gt;Use a Process Supervisor such as Supervisord or Runit.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I haven&amp;rsquo;t really messed around with the first option, maybe it could work out, but you&amp;rsquo;d lose the logs from the second process as it would need to output via the first process&amp;rsquo; &lt;code&gt;stdout&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;the-bash-script&#34;&gt;The Bash Script&lt;/h2&gt;

&lt;p&gt;Up until recently the way I have been running multiple processes is via the &lt;code&gt;bash&lt;/code&gt; script method, but it feels really clumsy and fragile and while it works I&amp;rsquo;ve never been particularly fond of it.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s an snippet from such a script from my &lt;a href=&#34;https://github.com/paulczar/docker-elk_confd&#34;&gt;docker-elk_confd&lt;/a&gt; project which builds out the [ELK]() stack using values in &lt;code&gt;etcd&lt;/code&gt; to orchestrate clustering and configuration via &lt;code&gt;confd&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo Starting ${APP_NAME}

confd -node $ETCD -config-file /app/confd.toml -confdir /app &amp;amp;
/opt/elasticsearch/bin/elasticsearch -p /app/elasticsearch.pid &amp;amp;

# while the port is listening, publish to etcd
while [[ ! -z $(netstat -lnt | awk &amp;quot;\$6 == \&amp;quot;LISTEN\&amp;quot; &amp;amp;&amp;amp; \$4 ~ \&amp;quot;.$PUBLISH\&amp;quot; &amp;amp;&amp;amp; \$1 ~ \&amp;quot;$PROTO.?\&amp;quot;&amp;quot;) ]] ; do
  publish_to_etcd
  sleep 5 # sleep for half the TTL
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see I&amp;rsquo;ve started two processes &lt;code&gt;elasticsearch&lt;/code&gt; and &lt;code&gt;confd&lt;/code&gt; both backgrounded and then I finish with a loop which publishes data to etcd every 5 seconds until the &lt;code&gt;elasticsearch&lt;/code&gt; process quits listening on its published tcp port.  This works, but it leaves me feeling a bit icky.&lt;/p&gt;

&lt;h2 id=&#34;process-supervisor&#34;&gt;Process Supervisor&lt;/h2&gt;

&lt;p&gt;I have used various supervisors in containers before but never really liked the experience as I could never get all the logs out to &lt;code&gt;stdout&lt;/code&gt; and using the standard docker logging mechanisms so I&amp;rsquo;ve always gone back to the &lt;code&gt;bash&lt;/code&gt; script method.  Recently while working on the ELK project mentioned above I decided to give using a process supervisor another chance.&lt;/p&gt;

&lt;p&gt;My primary measure of success for using a supervisor going forward was to come up with a way to push all output to the supervisor&amp;rsquo;s stdout so that I can use the regular docker logging.&lt;/p&gt;

&lt;p&gt;I decided to try with &lt;a href=&#34;http://supervisord.org&#34;&gt;supervisor&lt;/a&gt; as a starting point because it is a fairly small install and has an easily templatable config.   At about the same time I was looking at this I found a &lt;a href=&#34;http://supervisord.org&#34;&gt;blog post&lt;/a&gt; ( I believe it was linked in a recent Docker Weekly ) that talked about using &lt;code&gt;supervisor&lt;/code&gt; in docker containers.  They had even (sortof) solved the logging problem,  however the logging was appended with debug lines and made it messy and difficult to read.  I figured there had to be a cleaner way.&lt;/p&gt;

&lt;p&gt;Reading through the documentation I saw that you can specify a file to log each supervised process to.   I just needed a way to hijack that config item to write to supervisor&amp;rsquo;s stdout instead.   Turns out that&amp;rsquo;s quite easy as there&amp;rsquo;s a special device &lt;code&gt;/dev/stdout&lt;/code&gt; which links to &lt;code&gt;/dev/self/fd/1&lt;/code&gt; which is the &lt;code&gt;stdout&lt;/code&gt; for the running application.   I quickly threw together a test and it did indeed pipe the logs from the process through &lt;code&gt;stdout&lt;/code&gt; of supervisor.&lt;/p&gt;

&lt;p&gt;I end up with a &lt;code&gt;/etc/supervisord.conf&lt;/code&gt; ( which is written out by confd before supervisor is started ) file that looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[supervisord]
logfile=/dev/null
pidfile=/var/run/supervisord.pid
nodaemon=true

[program:publish_etcd]
command=/app/bin/publish_etcd
redirect_stderr=true
stdout_logfile=/dev/stdout
stdout_logfile_maxbytes=0
auto_start=true
autorestart=true

[program:confd]
command=confd -node %(ENV_ETCD)s -config-file /app/confd.toml -confdir /app
redirect_stderr=true
stdout_logfile=/dev/stdout
stdout_logfile_maxbytes=0
auto_start=true
autorestart=true

[program:elasticsearch]
command=/opt/elasticsearch/bin/elasticsearch
redirect_stderr=true
stdout_logfile=/dev/stdout
stdout_logfile_maxbytes=0
auto_start=true
autorestart=true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and my boot script that docker runs the following to launch my app:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo Starting ${APP_NAME}
/usr/bin/supervisord -c /etc/supervisor/supervisord.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All output from &lt;code&gt;Elasticsearch&lt;/code&gt;, &lt;code&gt;confd&lt;/code&gt;, &lt;code&gt;supervisord&lt;/code&gt; now output via the docker logging systems so that I can see what is going on by running:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker logs elasticsearch
docker logs -f 7270755ce94c03dda930fbdedeee7722dddf6fdbbf8902aaee52c9f94f2147ca
2014-12-23T04:46:02Z 7270755ce94c confd[37]: INFO /opt/elasticsearch/config/elasticsearch.yml has md5sum 08a09998560b7b786eca1e594b004ddc should be d83b49b485b5acad2666aa03b1ee90a0
2014-12-23T04:46:02Z 7270755ce94c confd[37]: INFO Target config /opt/elasticsearch/config/elasticsearch.yml out of sync
2014-12-23T04:46:02Z 7270755ce94c confd[37]: INFO Target config /opt/elasticsearch/config/elasticsearch.yml has been updated
2014-12-23T04:46:02Z 7270755ce94c confd[37]: INFO /etc/supervisor/supervisord.conf has mode -rw-r--r-- should be -rwxr-xr-x
2014-12-23T04:46:02Z 7270755ce94c confd[37]: INFO /etc/supervisor/supervisord.conf has md5sum 99dc7e8a1178ede9ae9794aaecbca436 should be ad9bc3735991d133a09f4fc665e2305f
2014-12-23T04:46:02Z 7270755ce94c confd[37]: INFO Target config /etc/supervisor/supervisord.conf out of sync
2014-12-23T04:46:02Z 7270755ce94c confd[37]: INFO Target config /etc/supervisor/supervisord.conf has been updated
Starting elasticsearch
2014-12-23 04:46:02,245 CRIT Supervisor running as root (no user in config file)
2014-12-23 04:46:02,251 INFO supervisord started with pid 51
2014-12-23 04:46:03,255 INFO spawned: &#39;publish_etcd&#39; with pid 54
2014-12-23 04:46:03,258 INFO spawned: &#39;elasticsearch&#39; with pid 55
2014-12-23 04:46:03,260 INFO spawned: &#39;confd&#39; with pid 56
==&amp;gt; sleeping for 20 seconds, then testing if elasticsearch is up.
[2014-12-23 04:46:04,146][INFO ][node                     ] [Sultan] version[1.4.2], pid[55], build[927caff/2014-12-16T14:11:12Z]
[2014-12-23 04:46:04,149][INFO ][node                     ] [Sultan] initializing ...
[2014-12-23 04:46:04,156][INFO ][plugins                  ] [Sultan] loaded [], sites []
2014-12-23 04:46:05,158 INFO success: publish_etcd entered RUNNING state, process has stayed up for &amp;gt; than 1 seconds (startsecs)
2014-12-23 04:46:05,159 INFO success: elasticsearch entered RUNNING state, process has stayed up for &amp;gt; than 1 seconds (startsecs)
2014-12-23 04:46:05,161 INFO success: confd entered RUNNING state, process has stayed up for &amp;gt; than 1 seconds (startsecs)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One last thing that I should mention.  the &lt;code&gt;publish_etcd&lt;/code&gt; talk in the supervisor config is running a script that contains the &lt;code&gt;while&lt;/code&gt; loop to make sure that &lt;code&gt;elasticsearch&lt;/code&gt; is listening on the approriate port, If that loop is broken it means that&lt;code&gt;elasticsearch&lt;/code&gt; is not responding and it sends a kill signal to &lt;code&gt;supervisor&lt;/code&gt; which then causes the container to shoot itself in the head because the rest of the  processes running are useless without &lt;code&gt;elasticsearch&lt;/code&gt; running.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>BreadOps - Continuous Delivery of Fresh Baked Bread</title>
      <link>https://tech.paulcz.net/blog/breadops-continous-delivery-of-fresh-baked-bread/</link>
      <pubDate>Wed, 03 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/breadops-continous-delivery-of-fresh-baked-bread/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;https://lh3.googleusercontent.com/-rMMuR_Itcmk/VH9eSsVZ18I/AAAAAAAAOc8/bJBp9UaoMI0/s1024/20141203_124907.jpg&#34; alt=&#34;the best way to eat fresh bread&#34; /&gt;
&amp;ldquo;See how this sparkly devop princess bakes bread every day with almost no effort at all with this one weird trick&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Store bought bread is shit.  Even the  &amp;ldquo;artisanal&amp;rdquo; bread at most supermarkets is little better than cake baked in a bread shaped mold ( seriously check next time you&amp;rsquo;re at a supermarket ).  You might be lucky and have a really good bread baker near you,  but like butchers and other important crafts they have all but disappeared.  My solution to this was to start baking bread myself.  I did a ton of research, started my own sourdough starter ( 5 years and going strong! ) and started baking bread regularly.&lt;/p&gt;

&lt;p&gt;Reading Boyd&amp;rsquo;s excellent blog post on &lt;a href=&#34;http://stackengine.com/laundryops-practical-devops-at-home/&#34;&gt;LaundryOps&lt;/a&gt; made me realize that I should write this up as I had somewhat unwittingly applied these DevOps practices to baking bread.&lt;/p&gt;

&lt;!--more --&gt;

&lt;p&gt;For a while it was tough going making bread all the time,  starters and doughs are fickle beasts and required constant care and feeding ( literally, you have to feed a sourdough starter at least twice a day ).   After almost giving up several times I started to apply what I now know as &lt;em&gt;devops techniques&lt;/em&gt;. Over time I worked to constantly improve my processes specifically optimizing for my time.  I can even apply CAMS across it:&lt;/p&gt;

&lt;h2 id=&#34;culture&#34;&gt;Culture&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Lactobacillus &amp;hellip; HAHAHAHA Bakers joke!&lt;/li&gt;
&lt;li&gt;A long fermentation time which converts more of the sugars into gas and makes the proteins more digestible&amp;hellip; Wait still wrong culture.&lt;/li&gt;
&lt;li&gt;Minimal human interaction ( approx 5 minutes per loaf ) reduces impact on normal life.&lt;/li&gt;
&lt;li&gt;Healthy Fresh bread for you and your family&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;automation&#34;&gt;Automation&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Slow ferment reduces requirement to knead to almost zero.&lt;/li&gt;
&lt;li&gt;Refrigerating the dough allows me to take control of the timetable.&lt;/li&gt;
&lt;li&gt;Usable for 5-7 days from the fridge.  &lt;strong&gt;Multiple loaves from the one batch.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;measurement&#34;&gt;Measurement&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;How much active time did I spend on it ?&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;How long did it take to ferment?&lt;/li&gt;
&lt;li&gt;How is the crust?  How is the crumb ?&lt;/li&gt;
&lt;li&gt;Is it better or worse than the previous iteration ?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;sharing&#34;&gt;Sharing&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;A &lt;em&gt;lot&lt;/em&gt; of people have received loaves of bread from me during experimentation.&lt;/li&gt;
&lt;li&gt;Ensure this process is approachable by others.&lt;/li&gt;
&lt;li&gt;This blog post.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The final technique that I came up with is not exactly unique and is similar to a number of several hundred page thirty dollar books aimed at making baking bread more accessible to the home cook, where it differs is that this is just a short blog post and is free as in beer and speech.  It&amp;rsquo;s fairly descriptive and may seem like a lot of work, but I tend to average a little over 5 minutes of active time per loaf of bread which is a negligible amount of time.&lt;/p&gt;

&lt;h2 id=&#34;ingredients-and-tools&#34;&gt;Ingredients and Tools&lt;/h2&gt;

&lt;p&gt;I worked hard to ensure that you don&amp;rsquo;t need to use any specialized tools.   No need for a standmixer or other expensive tools.  There are only two things (listed first) that you might not have in your kitchen, and they&amp;rsquo;re both so versatile that you &lt;em&gt;should&lt;/em&gt; have them.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh4.googleusercontent.com/v_kKOKFKupOdtI2_44MImjl5L8GYzq7QYMs1BxRyGgc=w1229-h692-no&#34; alt=&#34;things what you need to make bread&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Kitchen Scale[1]

&lt;ul&gt;
&lt;li&gt;capable of measuring to the gram&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Baking Stone[2]

&lt;ul&gt;
&lt;li&gt;large square one, the biggest that will fit in your oven&lt;/li&gt;
&lt;li&gt;A cast iron pan or dutch oven will do in a pinch.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;One large and one small mixing bowl&lt;/li&gt;
&lt;li&gt;Plastic Wrap

&lt;ul&gt;
&lt;li&gt;always cover your dough.&lt;/li&gt;
&lt;li&gt;a showercap makes a great reusable cover.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Flour

&lt;ul&gt;
&lt;li&gt;almost any flour will work, even All Purpose&lt;/li&gt;
&lt;li&gt;I recomend you start with King Arthur Bread Flour.&lt;/li&gt;
&lt;li&gt;I usually do a mix of KABF and stone ground whole wheat.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Salt

&lt;ul&gt;
&lt;li&gt;any salt as long as its not iodized.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Water

&lt;ul&gt;
&lt;li&gt;tap water is fine.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Yeast ( if using )

&lt;ul&gt;
&lt;li&gt;I used Fleischmansn&amp;rsquo;s Active Dry.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Olive Oil

&lt;ul&gt;
&lt;li&gt;For oiling the bowls used in the final shaping.&lt;/li&gt;
&lt;li&gt;I usually line the bowl with floured cheesecloth instead.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;[1] Measuring by volume is a mugs game.  Flour and Salt across different brands have different sized grains and this are different weights for the same volume.&lt;/p&gt;

&lt;p&gt;[2] you could get away with using a baking tray, but a baking stone or a cast iron skillet will give you the best results.&lt;/p&gt;

&lt;h2 id=&#34;the-starter&#34;&gt;The Starter&lt;/h2&gt;

&lt;p&gt;I use a &lt;a href=&#34;http://www.sourdoughhome.com/index.php?content=startermyway2&#34;&gt;sourdough starter&lt;/a&gt;, and I would recommend that you do the same &amp;hellip; But unless you already have a starter or have a friend who will give you some ( I&amp;rsquo;d be happy to give you some of mine ) You&amp;rsquo;ll probably want to use regular yeast which is what I&amp;rsquo;ll describe below.  If you have a sourdough starter then skip this step.&lt;/p&gt;

&lt;p&gt;Mix 50g Flour, 50g Water, 3g yeast until it forms a paste and then cover with plastic wrap:
&lt;img src=&#34;https://lh3.googleusercontent.com/-IKFdhJcTr7c/VHtUZeGewaI/AAAAAAAAObE/NQpFh4jQiCs/w1228-h691-no/20141124_123134.jpg&#34; alt=&#34;Starter&#34; /&gt;&lt;/p&gt;

&lt;p&gt;After 4-6 hours it should be all bubby and smell yeasty and ready for The Mix:
&lt;img src=&#34;https://lh6.googleusercontent.com/-_ap54clqH38/VH9em5xPMRI/AAAAAAAAOdk/syJCBF_2AJA/s640/20141201_085112.jpg&#34; alt=&#34;fermented starter&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;the-mix&#34;&gt;The Mix&lt;/h2&gt;

&lt;p&gt;Add the Starter and 375g of water to the large bowl and mix with a fork until it looks like milk:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Metric protip: a gram of water is the same as a milliliter of water&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh4.googleusercontent.com/-0Se5tsoJoH4/VHtUWeoTVdI/AAAAAAAAOa0/57hyMA0sBPk/w1228-h691-no/20141124_140942.jpg&#34; alt=&#34;don&#39;t drink it silly!&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Add 500g of flour and 15g salt.  Mix it until fully incorporated ( we&amp;rsquo;re not kneading here, just making sure there is no dry flour left ):
&lt;img src=&#34;https://lh5.googleusercontent.com/-FGpOS40YpWM/VH9elHUFGCI/AAAAAAAAOdc/tKkrmoXplWw/s640/20141201_085637.jpg&#34; alt=&#34;mixed&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;These quantities form a ratio that can be used to make batches as big or as small as your want.  I usually double it which gives me a decent sized loaf every two days.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;the-warm-ferment&#34;&gt;The Warm Ferment&lt;/h2&gt;

&lt;p&gt;Cover the bowl and leave it at room temperature for 4 to 8 hours.  The timing doesn&amp;rsquo;t have to be precise,  you&amp;rsquo;re just looking for sings of the yeast to start fermenting the flour and creating air bubbles.  Cover and refrigerate for at least 24 hours, up to 5 days:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh3.googleusercontent.com/-DRtoK892ZEI/VH-U6UZeLFI/AAAAAAAAOd8/pmbYgIUPmJU/s640/20141105_101952.jpg&#34; alt=&#34;fermentation activated&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;the-shaping&#34;&gt;The Shaping&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Whenever you are working with the dough, it is important to try to lose as little air as possible.  Do not punch it down.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh6.googleusercontent.com/-bXb_SAwYxSY/VHtUTmXk2SI/AAAAAAAAOak/ME5g9N5mkpM/w1228-h691-no/20141129_143244.jpg&#34; alt=&#34;ready for shaping&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Lightly flour the bench and turn out your dough.   Cut off a piece to use and put the rest back in the bowl and back into the fridge:
&lt;img src=&#34;https://lh6.googleusercontent.com/-B3N3OIQFnsA/VHtUSAIq2OI/AAAAAAAAOac/GsZeRXf4yjk/w1228-h691-no/20141129_143342.jpg&#34; alt=&#34;fresh from the cold ferment&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Protip: You can use the final piece of dough as the starter for the next batch and skip a step.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Gently stretch the piece dough out into a squaring shape and then fold each edge into the middle:
&lt;img src=&#34;https://lh3.googleusercontent.com/-jU4S-UI5kvw/VHtUQoyLJYI/AAAAAAAAOaU/VyQFf1qb06Y/w1228-h691-no/20141129_143427.jpg&#34; alt=&#34;folding&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Turn the dough over and shape it into a ball:
&lt;img src=&#34;https://lh3.googleusercontent.com/-wHWpKg-1IVM/VHtUPA1T5QI/AAAAAAAAOaM/ANaEWZyDuJ0/w1228-h691-no/20141129_143521.jpg&#34; alt=&#34;dough balls&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Watch this &lt;a href=&#34;https://www.youtube.com/watch?feature=player_detailpage&amp;amp;v=4VdVrib2PQo#t=20&#34;&gt;video&lt;/a&gt; (not mine) for the technique used to shape the dough into balls.
&lt;div class=&#34;embed video-player&#34;&gt;
&lt;iframe class=&#34;youtube-player&#34; type=&#34;text/html&#34; width=&#34;640&#34; height=&#34;385&#34; src=&#34;http://www.youtube.com/embed/4VdVrib2PQo&#34; allowfullscreen frameborder=&#34;0&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;Oil your shaping bowl and flip your ball over so that the top of the dough is facing down in the bowl and cover it:
&lt;img src=&#34;https://lh5.googleusercontent.com/-xbzq6cwYJis/VHtUNHuDbiI/AAAAAAAAOaE/8EdixPbGN1k/w1228-h691-no/20141129_144021.jpg&#34; alt=&#34;final shaping&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can easily make Ciabatta ( just fold it over itself in one direction and crimp the edges ), or Baguettes ( harder to explain, but youtube has plenty of guides ), or any other style of bread that you prefer.   Even works great for pizza bases!&lt;/p&gt;

&lt;h2 id=&#34;the-final-ferment&#34;&gt;The Final Ferment&lt;/h2&gt;

&lt;p&gt;Let the dough warm back up to room temperature and the yeasts to wake up and get active again.   This will probably take about two hours.  Crank the oven on to 450F after about an hour.  This gives the oven a good solid hour to get up to temperature and stabilize.&lt;/p&gt;

&lt;h2 id=&#34;the-bake&#34;&gt;The Bake&lt;/h2&gt;

&lt;p&gt;Turn the oven up to 500F, open the door, and slide the rack with the pizza stone out so that you can get to it without burning yourself.   upend the bowl onto the stone and run a knife quickly over the top to create a shallow cut:
&lt;img src=&#34;https://lh3.googleusercontent.com/-Pzk8zAWIWt0/VHtULim3WvI/AAAAAAAAOZ8/NTrxiDnOXmk/w1228-h691-no/20141129_155656.jpg&#34; alt=&#34;ready for baking&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Bake for 5 minutes, then turn the oven back down to 450F and bake for another 20 minutes:
&lt;img src=&#34;https://lh4.googleusercontent.com/-hX_Gpo4YIOE/VHtTrWwzT8I/AAAAAAAAOZ0/EY1snf5ymew/w1228-h691-no/20141129_180734.jpg&#34; alt=&#34;baked&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;the-cooldown&#34;&gt;The Cooldown&lt;/h2&gt;

&lt;p&gt;Avoid the tempation to cut into the bread while its still hot.  Bread continues to cook as it cools down and its important to not allow steam and heat to escape.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh4.googleusercontent.com/-8v2AonhRmTE/VH9eUSh3hxI/AAAAAAAAOdE/kx4in89KNTk/s640/20141202_184332.jpg&#34; alt=&#34;some other styles&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;the-eatening&#34;&gt;The Eatening&lt;/h2&gt;

&lt;p&gt;Fresh bread is best enjoyed simply with some high quality extra virgin olive oil and balsamic vinegar:
&lt;img src=&#34;https://lh3.googleusercontent.com/-_AuYSwAc_8A/VHtTnddC1CI/AAAAAAAAOZc/9cgRejB_s4g/w1228-h691-no/20141129_181010.jpg&#34; alt=&#34;ready to eat&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;tips-and-tricks&#34;&gt;Tips and Tricks&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Make your starter from Ikea&amp;rsquo;s &lt;a href=&#34;http://www.ikea.com/us/en/catalog/products/00229031/&#34;&gt;BRÖDMIX FLERKORN&lt;/a&gt; which contains all sorts of interesting grains as well as Yeast and &amp;ldquo;Sourdough Powder&amp;rdquo; whatever that is.&lt;/li&gt;
&lt;li&gt;Make &lt;a href=&#34;http://food.paulcz.net/2010/05/sourdough-pancakes.html&#34;&gt;sourdough pancakes&lt;/a&gt; from your leftover starter.&lt;/li&gt;
&lt;li&gt;Shape your dough into a &lt;a href=&#34;http://food.paulcz.net/2013/02/austin-style-pizza.html&#34;&gt;pizza dough&lt;/a&gt; and bake it in a cast iron skillet.&lt;/li&gt;
&lt;li&gt;You don&amp;rsquo;t have to make round loafs. use a sandwhich loaf tin, or make ciabatta or baguettes.  It&amp;rsquo;s all it the shaping.&lt;/li&gt;
&lt;li&gt;If you&amp;rsquo;re brave make your dough wetter and ferment for longer.   you&amp;rsquo;ll get cazy large holes in your crumb.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>EZBake - A new way to converge docker containers with chef</title>
      <link>https://tech.paulcz.net/blog/ezbake-a-new-way-to-converge-docker-containers-with-chef/</link>
      <pubDate>Tue, 13 May 2014 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/ezbake-a-new-way-to-converge-docker-containers-with-chef/</guid>
      <description>&lt;p&gt;&lt;code&gt;EZ Bake&lt;/code&gt; came from an idea I had while watching the &lt;a href=&#34;https://twitter.com/hangops&#34;&gt;HangOps&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=clLFKIeSADo&amp;amp;feature=youtu.be&#34;&gt;episode 2014-04-11&lt;/a&gt; in which they were talking about &lt;code&gt;Docker&lt;/code&gt; and Config Management being complementary rather than adversary.&lt;/p&gt;

&lt;p&gt;I have expermented with using &lt;code&gt;Chef&lt;/code&gt; and &lt;code&gt;Docker&lt;/code&gt; together in the &lt;a href=&#34;https://tech.paulcz.net/2013/09/creating-immutable-servers-with-chef-and-docker-dot-io.html&#34;&gt;past&lt;/a&gt; but wanted to tackle the problem from a slightly different angle.  I&amp;rsquo;ve recently been working on some PAAS stuff, both &lt;a href=&#34;http://deis.io&#34;&gt;Deis&lt;/a&gt; and &lt;a href=&#34;http://solum.io&#34;&gt;Solum&lt;/a&gt; these both utilize the tooling from &lt;a href=&#34;https://github.com/flynn/flynn&#34;&gt;Flynn&lt;/a&gt; which builds heroku style &lt;code&gt;buildpacks&lt;/code&gt; in &lt;code&gt;Docker&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;EZ Bake takes chef recipes designed for &lt;code&gt;chef-solo&lt;/code&gt; ( but could easily be extended to do the same for &lt;code&gt;chef-zero&lt;/code&gt;, or &lt;code&gt;chef-client&lt;/code&gt; with a server) in a tarball via &lt;code&gt;stdin&lt;/code&gt; and converges a docker node using that recipe.&lt;/p&gt;

&lt;p&gt;This methodology seems a little weird at first,  but it gives you the ability to ship your Chef cookbooks as self-contained tarballs, or even more interestingly use the &lt;code&gt;git archive&lt;/code&gt; command from your git repository to do this automatically and then pipe that directly to the &lt;code&gt;docker run&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;In order to recognize and run your cookbook ( or repo ) it needs to contain the following files: &lt;code&gt;Berksfile&lt;/code&gt;, &lt;code&gt;solo.json&lt;/code&gt;, &lt;code&gt;solo.rb&lt;/code&gt; in the root of your cookbook.   There is some provision for providing different locations for these via environment variables.   This is pre-ChefDK and will probably become easier with ChefDK.&lt;/p&gt;

&lt;p&gt;I have provided an example in the ezbake repo that will install Java7 in the container.&lt;/p&gt;

&lt;p&gt;This example shows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Converging a container using a local chef recipe&lt;/li&gt;
&lt;li&gt;Committing the container to an image on completion&lt;/li&gt;
&lt;li&gt;Removing the build container&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Running the new image&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone paulczar/ezbake
$ cd ezbake/examples
$ ID=$(tar cf - . | sudo docker run -i -a stdin paulczar/ezbake) \
&amp;amp;&amp;amp; sudo docker attach $ID \
&amp;amp;&amp;amp; sudo docker commit $ID java7 
&amp;amp;&amp;amp; sudo docker rm $ID

Running Berkshelf to collect your cookbooks:
Installing java (1.22.0) from site: &#39;http://cookbooks.opscode.com/api/v1/cookbooks&#39;
Converging your container:
[2014-04-12T22:10:24+00:00] INFO: Forking chef instance to converge...
....
[2014-04-12T22:16:52+00:00] INFO: Chef Run complete in 154.563192281 seconds
[2014-04-12T22:16:52+00:00] INFO: Running report handlers
[2014-04-12T22:16:52+00:00] INFO: Report handlers complete

$ sudo docker run -t java7 java -version
java version &amp;quot;1.7.0_51&amp;quot;
Java(TM) SE Runtime Environment (build 1.7.0_51-b13)
Java HotSpot(TM) 64-Bit Server VM (build 24.51-b03, mixed mode)

&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This could easily be built into a CI pipeline.   a git webhook could call jenkins which would clone the repo and then use a command like  &lt;code&gt;git archive master | docker run -i -a stdin paulczar/ezbake&lt;/code&gt; to converge a container from it.&lt;/p&gt;

&lt;p&gt;It could also very easily be used in &lt;code&gt;Deis&lt;/code&gt; or &lt;code&gt;Solum&lt;/code&gt; as an alternative to a Heroku buildpack.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Running DEIS.IO on Rackspace Cloud</title>
      <link>https://tech.paulcz.net/blog/running-deis-io-on-rackspace-cloud/</link>
      <pubDate>Sun, 23 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/running-deis-io-on-rackspace-cloud/</guid>
      <description>&lt;p&gt;I recently did a presentation at the Cloud Austin meetup titled &lt;a href=&#34;http://tech.paulcz.net/presentation-cloud-austin-deis/#/&#34;&gt;Docking with Unicorns&lt;/a&gt; about new PAAS on the block &lt;a href=&#34;http://deis.io&#34;&gt;DEIS&lt;/a&gt;.   Building out DEIS is quite easy,  make more easy by some tight integration they have with Rackspace Cloud.    If you&amp;rsquo;re interested in what deis is go through my slides linked above, and the documentation on their website.    If you want to build out an environment to kick the tires a bit,  then click &amp;lsquo;Read on&amp;rsquo; below and follow me down the rabbit hole.&lt;/p&gt;

&lt;h2 id=&#34;chef-setup&#34;&gt;Chef setup&lt;/h2&gt;

&lt;p&gt;Chef offers a free hosted service for up to five servers.  That&amp;rsquo;s plenty for this exercise so go to the &lt;a href=&#34;https://www.getchef.com/account&#34;&gt;registration page&lt;/a&gt; and create yourself a user.  At some point it will prompt you to generate and save a key, do that and download it.&lt;/p&gt;

&lt;p&gt;Once you have signed up you can download a knife config file and generate a validation key from the &lt;a href=&#34;https://manage.opscode.com/organizations&#34;&gt;Organizations&lt;/a&gt; page.  We can save those down and then move them to a local working directory.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh5.googleusercontent.com/-3R-Z-bRi_s0/UwpipiLhhWI/AAAAAAAAN0Q/W6q_Rb7NFy8/w1240-h663-no/opscode-org-page.png&#34; alt=&#34;chef org setup&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;prepare-working-environment&#34;&gt;Prepare Working Environment&lt;/h3&gt;

&lt;p&gt;Create a &lt;code&gt;~/paas&lt;/code&gt; working directory and configure your local chef tools like this ( change the Download location to match the files you downloaded above ) :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mkdir -p ~/paas/.chef
$ cd ~/paas
$ mv ~/Downloads/&amp;lt;username&amp;gt;.pem .chef/
$ mv ~/Downloads/knife.rb .chef/
$ mv ~/Downloads/&amp;lt;username&amp;gt;-validator.pem .chef/

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;clone-the-deis-repository&#34;&gt;Clone the Deis Repository&lt;/h3&gt;

&lt;p&gt;Clone the deis project into your paas working directory:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd ~/paas
$ git clone https://github.com/opdemand/deis.git
Cloning into &#39;deis&#39;...
remote: Reusing existing pack: 5651, done.
Receiving objects: 100% (5651/5651), 2.16 MiB | 1.37 MiB/s, done.
remote: Total 5651 (delta 0), reused 0 (delta 0)
Resolving deltas: 100% (3131/3131), done.
Checking connectivity... done

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;install-pre-reqs&#34;&gt;Install Pre-reqs&lt;/h3&gt;

&lt;p&gt;Assuming you have a working &lt;code&gt;Ruby 1.9.3+&lt;/code&gt; and the &lt;code&gt;bundler&lt;/code&gt; gem installed you should be able to use the &lt;code&gt;Gemfile&lt;/code&gt; from the deis project to ensure you have all the necessary tools:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd ~/paas/deis
$ bundle install
bundle install
Fetching gem metadata from https://rubygems.org/.......
Fetching additional metadata from https://rubygems.org/..
Using i18n (0.6.9)
Using multi_json (1.8.4)
Using activesupport (3.2.16)
Using addressable (2.3.5)
...
Using bundler (1.5.2)
Your bundle is complete!
Use `bundle show [gemname]` to see where a bundled gem is installed.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;I had some errors installing the eventmachine gem and had to follow &lt;a href=&#34;https://github.com/gitlabhq/gitlabhq/issues/1051#issuecomment-9176547&#34;&gt;this fix&lt;/a&gt; to get bundle install to work&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;test-chef-connectivity&#34;&gt;Test Chef Connectivity&lt;/h3&gt;

&lt;p&gt;To make sure we configured chef correctly and installed knife as part of the bundle we can run a quick knife command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec knife client list
&amp;lt;USERNAME&amp;gt;-validator
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;create-an-environment-for-deis&#34;&gt;Create an Environment for Deis&lt;/h3&gt;

&lt;p&gt;Deis is currently hardcoded to use the &lt;code&gt;_default&lt;/code&gt; chef environment.    There is a current &lt;a href=&#34;https://github.com/opdemand/deis/issues/523&#34;&gt;issue&lt;/a&gt; on their github to resolve this.   Once that is done I&amp;rsquo;ll update these instructions to create a &lt;code&gt;deis&lt;/code&gt; environment.&lt;/p&gt;

&lt;h3 id=&#34;upload-the-deis-cookbooks&#34;&gt;Upload the Deis Cookbooks&lt;/h3&gt;

&lt;p&gt;If that went well we can upload our cookbooks:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd ~/paas/deis
$ bundle exec berks install
Installing apt (2.3.8) from site: &#39;http://cookbooks.opscode.com/api/v1/cookbooks&#39;
Installing docker (0.31.0) from site: &#39;http://cookbooks.opscode.com/api/v1/cookbooks&#39;
Installing rsyslog (1.10.2) from site: &#39;http://cookbooks.opscode.com/api/v1/cookbooks&#39;
Installing sudo (2.3.0) from site: &#39;http://cookbooks.opscode.com/api/v1/cookbooks&#39;
...
$ bundle exec berks upload
Using apt (2.3.8)
Using docker (0.31.0)
Using rsyslog (1.10.2)
Using sudo (2.3.0)
Installing deis (0.5.1) from git: &#39;https://github.com/opdemand/deis-cookbook.git&#39; with branch: &#39;master&#39; at ref: &#39;6361706a1d3245d2a061ed55f5dd4b7cb60d5e5c&#39;
Using git (2.7.0)
...
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;create-deis-databags&#34;&gt;Create Deis Databags&lt;/h3&gt;

&lt;p&gt;Deis uses some databags to help manage application state.  We can create them like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec knife data bag create deis-formations
Created data_bag[deis-formations]
$ bundle exec knife data bag create deis-apps
Created data_bag[deis-apps]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;prepare-infrastructure&#34;&gt;Prepare Infrastructure&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;m using Rackspace cloud servers for this as I have the (&lt;a href=&#34;http://developer.rackspace.com/blog/developer-love-welcome-to-the-rackspace-cloud-developer-discount.html)[Rackspace&#34;&gt;http://developer.rackspace.com/blog/developer-love-welcome-to-the-rackspace-cloud-developer-discount.html)[Rackspace&lt;/a&gt; Developer Discount] which is enough discount to host this for free.&lt;/p&gt;

&lt;p&gt;Since Deis will want your rackspace credentials to configure worker nodes I recomment creating a user under (&lt;a href=&#34;https://mycloud.rackspace.com/account#users/create)[User&#34;&gt;https://mycloud.rackspace.com/account#users/create)[User&lt;/a&gt; Management] in your account to use for this.&lt;/p&gt;

&lt;h3 id=&#34;create-a-cloud-load-balancer&#34;&gt;Create a Cloud Load Balancer&lt;/h3&gt;

&lt;p&gt;Log into mycloud.rackspace.com and click on the (&lt;a href=&#34;https://mycloud.rackspace.com/load_balancers)[Load&#34;&gt;https://mycloud.rackspace.com/load_balancers)[Load&lt;/a&gt; Balancers] button.  Select the Dallas Region (DFW) and hit &lt;code&gt;Create Load Balancer&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Set the Name to &lt;code&gt;deis&lt;/code&gt; and check the region is set to &lt;code&gt;Dallas (DFW)&lt;/code&gt; and hit &lt;code&gt;Create Load Balancer&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://lh4.googleusercontent.com/-E4cZvoKWlYU/Uwpiqr9xOKI/AAAAAAAAN0o/P3vGqPC8A98/w793-h592-no/rackspace-create-lb.png&#34; alt=&#34;creating load balancer&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Take note of the public IP of the Load Balancer, we&amp;rsquo;ll need it later.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://lh4.googleusercontent.com/-ORvf6nzEduU/Uwpiqk5eP0I/AAAAAAAAN0k/WZ-NaJn3eJg/w770-h567-no/rackspace-lb.png&#34; alt=&#34;load balancer created&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;wildcard-dns&#34;&gt;Wildcard DNS&lt;/h3&gt;

&lt;p&gt;Deis&amp;rsquo; proxy layer requires you to set up Wildcard DNS to point to your proxy layer.  There are many ways to achieve this here are two options:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Rackspace Cloud DNS can host wildcard DNS entries, if you already have DNS hosted by rackspace using Cloud DNS simply add an A record for &lt;code&gt;*.deis&lt;/code&gt; under your domain and point it to the IP of your load balancer.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The (&lt;a href=&#34;http://xip.io)[xip.io&#34;&gt;http://xip.io)[xip.io&lt;/a&gt;] domain does wildcard DNS based on your IP.  We can use this with our Cloud Load Balancer to load balance our applications.   My Load Balancer has a public IP of &lt;code&gt;50.56.167.26&lt;/code&gt; therefore my wildcard domain will be &lt;code&gt;50.56.167.26.xip.io&lt;/code&gt;.   Remember this.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;configure-knife-for-rackspace&#34;&gt;Configure Knife for Rackspace&lt;/h3&gt;

&lt;p&gt;The bundle install above already installed the rackspace knife plugin so we just need to add some details to &lt;code&gt;.chef/knife.rb&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat &amp;lt;&amp;lt;&#39;EOF&#39; &amp;gt;&amp;gt; $HOME/.chef/knife.rb
knife[:rackspace_api_username] = &amp;quot;#{ENV[&#39;OS_USERNAME&#39;]}&amp;quot;
knife[:rackspace_api_key]      = &amp;quot;#{ENV[&#39;OS_PASSWORD&#39;]}&amp;quot;
knife[:rackspace_version]      = &#39;v2&#39;
knife[:rackspace_region]       = :dfw
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;install-rackspace-nova-client&#34;&gt;Install Rackspace Nova Client&lt;/h3&gt;

&lt;p&gt;We also need the Nova client:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo pip install rackspace-novaclient
$ cat &amp;lt;&amp;lt;&#39;EOF&#39; &amp;gt;&amp;gt; ~/paas/.chef/openrc
export OS_AUTH_URL=https://identity.api.rackspacecloud.com/v2.0/
export OS_AUTH_SYSTEM=rackspace
export OS_REGION_NAME=DFW
export OS_USERNAME=&amp;lt;RACKSPACE_USERNAME&amp;gt;
export NOVA_RAX_AUTH=1
export OS_PASSWORD=&amp;lt;RACKSPACE_API_KEY&amp;gt;
export OS_NO_CACHE=1
export OS_TENANT_NAME=&amp;lt;RACKSPACE_USERNAME&amp;gt;
EOF
$ source ~/paas/.chef/openrc
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;test-rackspace-connectivity&#34;&gt;Test Rackspace Connectivity&lt;/h3&gt;

&lt;p&gt;Make sure you can connect to Rackspace with Knife:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec knife rackspace server list
Instance ID  Name  Public IP  Private IP  Flavor  Image  State
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Make sure you can connect to Rackspace with nova:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ nova list
+--------------------------------------+-----------------+--------+------------+-------------+----------------------------------------------------------------------------------------+
| ID                                   | Name            | Status | Task State | Power State | Networks                                                                               |
+--------------------------------------+-----------------+--------+------------+-------------+----------------------------------------------------------------------------------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;build-base-images-for-controller-and-nodes&#34;&gt;Build base images for Controller and Nodes.&lt;/h2&gt;

&lt;p&gt;This isn&amp;rsquo;t strictly necessary,  but will help build your nodes quicker on subsequent builds.&lt;/p&gt;

&lt;h3 id=&#34;launce-a-new-instance&#34;&gt;Launce a new instance:&lt;/h3&gt;

&lt;p&gt;If we create a base image and pre-install some software we&amp;rsquo;ll get a faster booting system for auto-provisioning:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec knife rackspace server create \
  --image &#39;80fbcb55-b206-41f9-9bc2-2dd7aac6c061&#39; \
  --node-name &#39;deis-base-image&#39; \
  --flavor &#39;performance1-1&#39;
...
...
Instance ID: 56760bf1-b977-405e-9348-f70b15a14b87
Host ID: 97da00a12312a7e455bda70c6dfab8833953e2a03b081aeedfd68152
Name: deis-base-image
Flavor: 1 GB Performance
Image: Ubuntu 12.04 
Metadata: []
Public DNS Name: 23-253-69-98.xip.io
Public IP Address: 23.253.69.98
Private IP Address: 10.208.101.31
Password: **************
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Take note of the &lt;code&gt;Instance ID&lt;/code&gt;, &lt;code&gt;Public IP Address&lt;/code&gt; and &lt;code&gt;Password&lt;/code&gt;.  We&amp;rsquo;ll need them later.&lt;/p&gt;

&lt;h3 id=&#34;add-users-keys-to-instance&#34;&gt;Add users / keys to instance&lt;/h3&gt;

&lt;p&gt;We&amp;rsquo;re going to add our ssh key as well as a local &lt;code&gt;deis-ops&lt;/code&gt; user to the image to make it easier to manage and troubleshoot later:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ DEIS_IP=&amp;lt;IP_OF_SERVER&amp;gt;
$ ssh-copy-id root@$DEIS_IP
root@162.242.144.193&#39;s password: 
Number of key(s) added: 1
Now try logging into the machine, with:   &amp;quot;ssh &#39;root@162.242.144.193&#39;&amp;quot;
and check to make sure that only the key(s) you wanted were added.
$ ssh root@$DEIS_IP
Welcome to Ubuntu 12.04.3 LTS (GNU/Linux 3.2.0-55-virtual x86_64)

 * Documentation:  https://help.ubuntu.com/

  System information as of Sun Feb 23 18:34:40 UTC 2014

  System load:  0.08              Processes:           60
  Usage of /:   5.5% of 19.68GB   Users logged in:     0
  Memory usage: 6%                IP address for eth0: 162.242.144.193
  Swap usage:   0%                IP address for eth1: 10.208.135.114

  Graph this data and manage this system at https://landscape.canonical.com/

Last login: Sun Feb 23 18:33:02 2014 from cpe-24-27-47-27.austin.res.rr.com
root@deis-base-image:~# useradd --comment &#39;deis ops user&#39; --home-dir &#39;/home/deis-ops&#39; \
  --shell &#39;/bin/bash&#39; --create-home deis-ops
root@deis-base-image:~# mkdir -p /home/deis-ops/.ssh &amp;amp;&amp;amp; \
   cp /root/.ssh/authorized_keys /home/deis-ops/.ssh/authorized_keys &amp;amp;&amp;amp; \
  chown -R deis-ops:deis-ops /home/deis-ops &amp;amp;&amp;amp; \
  chmod 0700 /home/deis-ops/.ssh &amp;amp;&amp;amp; \
  chmod 0600 /home/deis-ops/.ssh/authorized_keys &amp;amp;&amp;amp; \
  echo &#39;deis-ops ALL=(ALL) NOPASSWD:ALL&#39; &amp;gt; /etc/sudoers.d/deis-ops &amp;amp;&amp;amp; \
  chmod 0440 /etc/sudoers.d/deis-ops
root@deis-base-image:~# exit
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check that you can log in with these new creds:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh deis-ops@$DEIS_IP
deis$ sudo bash
root@deis$ exit
deis$ exit
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;finish-preparing-node-image&#34;&gt;Finish preparing node image&lt;/h3&gt;

&lt;p&gt;Next we&amp;rsquo;re going to update the kernel and prepare the base node image.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh deis-ops@$DEIS_IP &#39;sudo apt-get update&#39;
$ scp contrib/rackspace/*.sh deis-ops@$DEIS_IP:~/
$ ssh deis-ops@$DEIS_IP &#39;sudo ~/prepare-node-image.sh&#39;
$ ssh deis-ops@$DEIS_IP &#39;sudo apt-get install -yq linux-image-generic-lts-raring linux-headers-generic-lts-raring&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;create-an-image-from-this-server&#34;&gt;Create an image from this server&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ nova image-create deis-base-image deis-node-image
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After a few minutes you should see this response to running &lt;code&gt;nova image-list&lt;/code&gt;, if you&amp;rsquo;re impatient like me wrap your command with a &lt;code&gt;watch&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ watch &#39;nova image-list | grep deis&#39;
| df958d26-6515-4dd9-a449-920e74ea93a2 | deis-base-image                                              | ACTIVE | 0fc7f68b-176d-49a9-82ff-2d5893d32acd |

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once the image is active we can move onto the next steps.&lt;/p&gt;

&lt;h3 id=&#34;prepare-controller-image&#34;&gt;Prepare controller image&lt;/h3&gt;

&lt;p&gt;Next we want to prepare the VM for the controller image:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh deis-ops@$DEIS_IP &#39;sudo ~/prepare-controller-image.sh&#39;
$ ssh deis-ops@$DEIS_IP &#39;sudo apt-get install -yq linux-image-generic-lts-raring linux-headers-generic-lts-raring&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;create-an-image-from-this-server-1&#34;&gt;Create an image from this server&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ nova image-create deis-base-image deis-base-image
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After a few minutes you should see this response to running &lt;code&gt;nova image-list&lt;/code&gt;, if you&amp;rsquo;re impatient like me wrap your command with a &lt;code&gt;watch&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ watch &#39;nova image-list | grep deis-node&#39;
| f2236fa6-1e2d-4746-ac87-a3dd6b2de811 | deis-node-image                                              | ACTIVE | 633d5d88-54b3-463c-80fe-c119f4eb33a3 |

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;delete-the-instance&#34;&gt;Delete the instance&lt;/h3&gt;

&lt;p&gt;No need to keep the instance around and keep paying for it once you have the image:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec knife rackspace server list | grep deis  
42899699-68e7-4785-9f49-e0050f86249a  deis-base-image  162.242.144.193  10.208.135.114  performance1-1  80fbcb55-b206-41f9-9bc2-2dd7aac6c061  active
$ bundle exec knife rackspace server delete 42899699-68e7-4785-9f49-e0050f86249a --purge
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;create-the-deis-controller-server&#34;&gt;Create the Deis Controller server&lt;/h2&gt;

&lt;h3 id=&#34;launch-the-server&#34;&gt;Launch the Server&lt;/h3&gt;

&lt;p&gt;Launch the server from the image you created earlier:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ nova image-list | grep  deis-base-image
| a58c9895-6349-442a-bba7-99611900209d | deis-base-image
$ knife rackspace server create \
  --image a58c9895-6349-442a-bba7-99611900209d \
  --rackspace-metadata &amp;quot;{\&amp;quot;Name\&amp;quot;: \&amp;quot;deis-controller\&amp;quot;}&amp;quot; \
  --rackspace-disk-config MANUAL \
  --server-name deis-controller \
  --node-name deis-controller \
  --flavor &#39;performance1-2&#39;
Instance ID: bb713170-9322-424a-8837-863a4b396705
Name: deis-controller
Flavor: 2 GB Performance
Image: deis-base-image
...
Public IP Address: 23.253.104.13
Private IP Address: 10.208.132.190
Password: CQwDU4m97nvF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Take note of the &lt;code&gt;Instance ID&lt;/code&gt; and &lt;code&gt;Public IP Address&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If you have an easy to manage domain add an A record for &lt;code&gt;deis&lt;/code&gt; to it for the Public IP address.  If not
add an entry to your hosts file ( or do both! I did ):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo sh -c &amp;quot;echo &#39;&amp;lt;IP_OF_SERVER&amp;gt; deis&#39; &amp;gt;&amp;gt; /etc/hosts&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;modify-chef-admin-group&#34;&gt;Modify Chef Admin Group&lt;/h3&gt;

&lt;p&gt;On the Chef management website click (&lt;a href=&#34;https://manage.opscode.com/groups/admins/edit)[Groups&#34;&gt;https://manage.opscode.com/groups/admins/edit)[Groups&lt;/a&gt;] and add the &lt;code&gt;deis-controller&lt;/code&gt; client and your validator client to the &lt;code&gt;admins&lt;/code&gt; group.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh5.googleusercontent.com/-oSqB1Tdnn4c/UwpioPAXpJI/AAAAAAAANz4/xa8BdmRuTzQ/w579-h580-no/chef-admins.png&#34; alt=&#34;chef admins group&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;converge-the-deis-controller-server&#34;&gt;Converge the Deis Controller Server&lt;/h3&gt;

&lt;p&gt;Edit the &lt;code&gt;deis-controller&lt;/code&gt; node via this command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ EDITOR=vi knife node edit deis-controller
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;make it look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;name&amp;quot;: &amp;quot;deis-controller&amp;quot;,
  &amp;quot;chef_environment&amp;quot;: &amp;quot;_default&amp;quot;,
  &amp;quot;normal&amp;quot;: {
    &amp;quot;tags&amp;quot;: [

    ]
  },
  &amp;quot;run_list&amp;quot;: [
    &amp;quot;recipe[deis::controller]&amp;quot;
  ]
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;then converge the node by running chef client on it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh deis-ops@deis sudo chef-client
[2014-02-23T19:25:32+00:00] INFO: Forking chef instance to converge...
[2014-02-23T19:25:32+00:00] INFO: *** Chef 11.6.2 ***
[2014-02-23T19:25:33+00:00] INFO: Run List is [recipe[deis::controller]]
[2014-02-23T19:25:33+00:00] INFO: Run List expands to [deis::controller]
[2014-02-23T19:25:33+00:00] INFO: Starting Chef Run for deis-controller
[2014-02-23T19:25:33+00:00] INFO: Running start handlers
[2014-02-23T19:25:33+00:00] INFO: Start handlers complete.
...
$
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;testing-deis&#34;&gt;Testing Deis&lt;/h2&gt;

&lt;h3 id=&#34;install-the-deis-client-with-pip&#34;&gt;Install the Deis Client with pip&lt;/h3&gt;

&lt;p&gt;The Deis client is written in python and can be installed by &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo pip install deis  
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;register-admin-user&#34;&gt;Register Admin User&lt;/h3&gt;

&lt;p&gt;First user to register becomes the Admin:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ deis register http://deis:8000
username: admin
password: 
password (confirm): 
email: admin@example.com
Registered admin
Logged in as admin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Push your public key to deis:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ deis keys:add ~/.ssh/id_rsa.pub 
Uploading SSH_KEY to Deis...done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;check the web server is serving content by browsing to (&lt;a href=&#34;http://deis)[http://deis&#34;&gt;http://deis)[http://deis&lt;/a&gt;] and entering your admin credentials.&lt;/p&gt;

&lt;h3 id=&#34;teach-deis-your-provider-credentials&#34;&gt;Teach Deis your provider credentials&lt;/h3&gt;

&lt;p&gt;Deis will automatically provision worker nodes if you teach it your credentials.&lt;/p&gt;

&lt;p&gt;We already have our Rackspace credentials saved to &lt;code&gt;~/paas/.chef/openrc&lt;/code&gt; but Deis wants them named differently:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export RACKSPACE_USERNAME=$OS_USERNAME
$ export RACKSPACE_API_KEY=$OS_PASSWORD
$ deis providers:discover
No EC2 credentials discovered.
Discovered Rackspace credentials: ****************
Import Rackspace credentials? (y/n) : y
Uploading Rackspace credentials... done
No DigitalOcean credentials discovered.
No Vagrant VMs discovered.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;deploy-formations-layers&#34;&gt;Deploy Formations &amp;amp; Layers&lt;/h2&gt;

&lt;h3 id=&#34;formation&#34;&gt;Formation&lt;/h3&gt;

&lt;p&gt;Formations are collections of infrastructure for serving applications.   We&amp;rsquo;ll call our first Formation &lt;code&gt;dev&lt;/code&gt; for development.&lt;/p&gt;

&lt;p&gt;Create formation (using the wildcard domain from our cloud load balancer created earlier in the &lt;code&gt;--domain&lt;/code&gt; argument):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ deis formations:create dev --domain=50.56.167.26.xip.io
Creating formation... done, created dev
See `deis help layers:create` to begin building your formation
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;layers&#34;&gt;Layers&lt;/h3&gt;

&lt;p&gt;Layers are a heterogenerous collection of nodes that perform one of two function:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Proxy - Directs traffic to the appropriate container running the application.&lt;/li&gt;
&lt;li&gt;Runtime - Runs the containers that hold the applications.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We&amp;rsquo;re going to create a layer called &lt;code&gt;nodes&lt;/code&gt; that will perform both the proxy and runtime functions:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ deis layers:create dev nodes rackspace-dfw --proxy=y --runtime=y
Creating nodes layer... done in 4s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;note&lt;/em&gt; There&amp;rsquo;s currently a &lt;a href=&#34;https://github.com/opdemand/deis/issues/541&#34;&gt;bug&lt;/a&gt; that causes the first creation of a layer to fail.  if that happens run the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;deis formations:create dev --domain=50.56.167.26.xip.io
Creating formation... done, created dev

See `deis help layers:create` to begin building your formation
$ deis layers:create dev nodes rackspace-dfw --proxy=y --runtime=y
Creating nodes layer... 500 INTERNAL SERVER ERROR
&amp;lt;h1&amp;gt;Server Error (500)&amp;lt;/h1&amp;gt;
$ deis layers:destroy dev nodes
Destroying nodes layer... done in 0s
$ deis layers:create dev nodes rackspace-dfw --proxy=y --runtime=y
Creating nodes layer... done in 2s

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;build-nodes&#34;&gt;Build Nodes&lt;/h3&gt;

&lt;p&gt;Next we tell deis to spin up two Cloud Servers which will become members of the &lt;code&gt;nodes&lt;/code&gt; layer.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ deis nodes:scale dev nodes=2
Scaling nodes... but first, coffee!
done in 345s
Use `deis create --formation=dev` to create an application
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This can sometimes take longer than the &lt;code&gt;deis&lt;/code&gt; cli timeout.   Don&amp;rsquo;t fear,  just wait a bit longer, this could be a great time to explore the &lt;code&gt;deis&lt;/code&gt; cli by running &lt;code&gt;deis help&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;update-cloud-load-balancer&#34;&gt;Update Cloud Load Balancer&lt;/h2&gt;

&lt;p&gt;Add these two nodes to the (&lt;a href=&#34;https://mycloud.rackspace.com/load_balancers)[Cloud&#34;&gt;https://mycloud.rackspace.com/load_balancers)[Cloud&lt;/a&gt; Load Balancer] we created earlier.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh6.googleusercontent.com/-yaJfxoyDk4M/UwpioEndiOI/AAAAAAAANz0/aXannmisdbE/w903-h407-no/cloud-servers-list.png&#34; alt=&#34;cloud server list&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This is simple to do through the GUI:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Click on your load balancer and under &lt;code&gt;Nodes&lt;/code&gt; click the &lt;code&gt;Add Cloud Servers&lt;/code&gt; button.&lt;/li&gt;
&lt;li&gt;Check the box beside the two &lt;code&gt;dev-nodes&lt;/code&gt; servers and click &lt;code&gt;Add Selected Servers&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://lh6.googleusercontent.com/-zm6sB7l7YVk/Uwpin4BNJPI/AAAAAAAANzw/b-_J2ieyIuE/w773-h476-no/cloud-lb-nodes.png&#34; alt=&#34;cloud lb servers&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;deploy-an-application&#34;&gt;Deploy an Application&lt;/h2&gt;

&lt;p&gt;So great, you have a PaaS, but what do you do now?  Deploy some apps of course!&lt;/p&gt;

&lt;h3 id=&#34;nodejs-example-app&#34;&gt;NodeJS Example App&lt;/h3&gt;

&lt;p&gt;Download the NodeJS example application so like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mkdir -p ~/paas/apps
$ cd ~paas/apps
$ git clone https://github.com/opdemand/example-nodejs-express.git
$ cd example-nodejs-express
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;create-an-application-in-deis&#34;&gt;Create an Application in Deis&lt;/h3&gt;

&lt;p&gt;Use the Deis command line tool to create a new application:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ deis create      
Creating application... done, created exotic-sandwich
Git remote deis added
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;push-your-application-to-deis&#34;&gt;Push your Application to Deis&lt;/h3&gt;

&lt;p&gt;This will push, deploy and Launch the app.  The first one will take some time as deis has to download some docker images,  subsequent apps will be much faster:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git push deis master                     
git push deis master
Counting objects: 184, done.
Delta compression using up to 4 threads.
Compressing objects: 100% (89/89), done.
Writing objects: 100% (184/184), 28.77 KiB | 0 bytes/s, done.
Total 184 (delta 103), reused 165 (delta 92)
-----&amp;gt; Node.js app detected
-----&amp;gt; Requested node range: 0.10.x
-----&amp;gt; Resolved node version: 0.10.26
-----&amp;gt; Downloading and installing node
-----&amp;gt; Installing dependencies
       npm WARN package.json example-nodejs-express@0.0.1 No repository field.
       npm http GET https://registry.npmjs.org/express
       npm http 200 https://registry.npmjs.org/express
...
-----&amp;gt; Caching node_modules directory for future builds
-----&amp;gt; Cleaning up node-gyp and npm artifacts
-----&amp;gt; Building runtime environment
-----&amp;gt; Discovering process types
       Procfile declares types -&amp;gt; web
-----&amp;gt; Compiled slug size is 5.5M
-----&amp;gt; Building Docker image
Uploading context 5.698 MB
Uploading context 
Step 0 : FROM deis/slugrunner
 ---&amp;gt; bb0a27915014
Step 1 : RUN mkdir -p /app
 ---&amp;gt; Running in 1ae5cdeaad9a
 ---&amp;gt; 6e6467466d48
Step 2 : ADD slug.tgz /app
 ---&amp;gt; 191a4345b1e4
Step 3 : ENTRYPOINT [&amp;quot;/runner/init&amp;quot;]
 ---&amp;gt; Running in d322512d5865
 ---&amp;gt; 2866cf3e37c9
Successfully built 2866cf3e37c9
-----&amp;gt; Pushing image to private registry
       Launching... done, v2

-----&amp;gt; exotic-sandwich deployed to Deis
       http://exotic-sandwich.50.56.167.26.xip.io

       To learn more, use `deis help` or visit http://deis.io

To ssh://git@deis:2222/exotic-sandwich.git
 * [new branch]      master -&amp;gt; master

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;did-it-work&#34;&gt;Did it work ?&lt;/h2&gt;

&lt;p&gt;Open your web browser to the URL in the output of the previous command.  In my case this was &lt;code&gt;http://exotic-sandwich.50.56.167.26.xip.io&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If everything worked the text in the browser window should read &lt;code&gt;Powered by Deis&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh6.googleusercontent.com/-cxuysxM_oM8/UwpipfiKFMI/AAAAAAAAN0U/M7T9dC6xJ-E/w446-h171-no/deis-app-1.png&#34; alt=&#34;deis app&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;configure-and-scale-your-application&#34;&gt;Configure and Scale your application&lt;/h2&gt;

&lt;p&gt;We can set config parameters for our apps by running &lt;code&gt;deis config&lt;/code&gt;.   The example app we&amp;rsquo;re using has a config paramater &amp;lsquo;POWERED_BY&amp;rsquo; so we can set that by running:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ deis config:set POWERED_BY=&#39;DEIS and Rackspace&#39;
=== exotic-sandwich
POWERED_BY: DEIS and Rackspace
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://lh6.googleusercontent.com/-J5AcNytZLOQ/UwpipEdpeBI/AAAAAAAAN0E/WXWC08rxsBU/w507-h157-no/deis-app-2.png&#34; alt=&#34;deis app2&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Expecting visitors?  Let&amp;rsquo;s scale your app to 5 nodes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ deis scale web=5
Scaling containers... but first, coffee!
done in 54s

=== exotic-sandwich Containers

--- web: `node server.js`
web.1 up 2014-02-23T20:22:07.241Z (dev-nodes-2)
web.2 up 2014-02-23T20:28:21.778Z (dev-nodes-1)
web.3 up 2014-02-23T20:28:21.788Z (dev-nodes-2)
web.4 up 2014-02-23T20:28:21.799Z (dev-nodes-1)
web.5 up 2014-02-23T20:28:21.810Z (dev-nodes-2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can see what your app is doing by running &lt;code&gt;deis info&lt;/code&gt; and &lt;code&gt;deis logs&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ deis info
=== exotic-sandwich Application
{
  &amp;quot;updated&amp;quot;: &amp;quot;2014-02-23T20:28:21.812Z&amp;quot;, 
  &amp;quot;uuid&amp;quot;: &amp;quot;ef618db6-f5a8-4cab-a7d9-d01e78036e3a&amp;quot;, 
  &amp;quot;created&amp;quot;: &amp;quot;2014-02-23T20:16:51.931Z&amp;quot;, 
  &amp;quot;formation&amp;quot;: &amp;quot;dev&amp;quot;, 
  &amp;quot;owner&amp;quot;: &amp;quot;admin&amp;quot;, 
  &amp;quot;id&amp;quot;: &amp;quot;exotic-sandwich&amp;quot;, 
  &amp;quot;containers&amp;quot;: &amp;quot;{\&amp;quot;web\&amp;quot;: 5}&amp;quot;
}

=== exotic-sandwich Containers

--- web: `node server.js`
web.1 up 2014-02-23T20:22:07.241Z (dev-nodes-2)
web.2 up 2014-02-23T20:28:21.778Z (dev-nodes-1)
web.3 up 2014-02-23T20:28:21.788Z (dev-nodes-2)
web.4 up 2014-02-23T20:28:21.799Z (dev-nodes-1)
web.5 up 2014-02-23T20:28:21.810Z (dev-nodes-2)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;$ deis logs
Feb 23 20:22:57 dev-nodes exotic-sandwich[web.1]: Server listening on port 10001 in development mode
Feb 23 20:25:38 dev-nodes exotic-sandwich[web.1]: Server listening on port 10001 in development mode
Feb 23 20:26:49 dev-nodes exotic-sandwich[web.1]: Server listening on port 10001 in development mode
Feb 23 20:28:28 dev-nodes exotic-sandwich[web.3]: Server listening on port 10003 in development mode
Feb 23 20:28:29 dev-nodes exotic-sandwich[web.5]: Server listening on port 10005 in development mode
Feb 23 20:29:11 dev-nodes exotic-sandwich[web.2]: Server listening on port 10002 in development mode
Feb 23 20:29:12 dev-nodes exotic-sandwich[web.4]: Server listening on port 10004 in development mode
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Congratulations!  You&amp;rsquo;ve successfully built out your own cost effective PAAS and deployed your first application to it.&lt;/p&gt;

&lt;p&gt;Speaking of costs &amp;hellip;  How much would this cost to run per month ?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Cloud Load Balancer - $10.95 / month&lt;/li&gt;
&lt;li&gt;Deis Controller - $57.60 / month&lt;/li&gt;
&lt;li&gt;Deis Nodes (x2) - $115.20 / month&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Total:  $183.75 / month.&lt;/p&gt;

&lt;p&gt;You could run all of this on a single server without a load balancer,  which means it would be just $57.60/month, which with the &lt;a href=&#34;http://developer.rackspace.com/devtrial/&#34;&gt;Rackspace Developer Discount&lt;/a&gt; would reduce down to just $7.60/month.&lt;/p&gt;

&lt;h1 id=&#34;epilogue&#34;&gt;Epilogue&lt;/h1&gt;

&lt;h2 id=&#34;cleanup&#34;&gt;Cleanup&lt;/h2&gt;

&lt;p&gt;Destroy your app:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ deis destroy

 !    WARNING: Potentially Destructive Action
 !    This command will destroy the application: exotic-sandwich
 !    To proceed, type &amp;quot;exotic-sandwich&amp;quot; or re-run this command with --confirm=exotic-sandwich

&amp;gt; exotic-sandwich
Destroying exotic-sandwich... done in 21s
Git remote deis removed
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;list your servers:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec knife rackspace server list
Instance ID                           Name             Public IP       Private IP      Flavor          Image                                 State 
7c43ecb9-1ba3-454c-a5f4-637b56961d68  dev-nodes        23.253.102.184  10.208.135.137  performance1-2  2d59cbce-92fa-412b-8a5e-6eb426ce7dc9  active
f89c4b25-6486-422a-907a-16b3b3223a5e  dev-nodes        23.253.102.158  10.208.137.18   performance1-2  2d59cbce-92fa-412b-8a5e-6eb426ce7dc9  active
bb713170-9322-424a-8837-863a4b396705  deis-controller  23.253.104.13   10.208.132.190  performance1-2  a58c9895-6349-442a-bba7-99611900209d  active
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Delete your servers by running the following command for each:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec knife rackspace server delete 7c43ecb9-1ba3-454c-a5f4-637b56961d68 --purge
Instance ID: 7c43ecb9-1ba3-454c-a5f4-637b56961d68
Host ID: e0da0172f321babe99aec9686c7b99ac7fa5ff8fa1ada934f5fae842
Name: dev-nodes
Flavor: 2 GB Performance
Image: deis-node-image
Public IP Address: 23.253.102.184
Private IP Address: 10.208.135.137

Do you really want to delete this server? (Y/N) y
[WARNING] Error Parsing response json - Yajl::ParseError
WARNING: Deleted server 7c43ecb9-1ba3-454c-a5f4-637b56961d68
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Clean up your chef:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec knife data bag delete deis-apps
$ bundle exec knife data bag delete deis-formations
$ bundle exec knife client delete dev-nodes-1
$ bundle exec knife client delete dev-nodes-2
$ bundle exec knife node delete dev-nodes-1
$ bundle exec knife node delete dev-nodes-2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Delete your glance images:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ nova image-delete deis-base-image
$ nova image-delete deis-node-image
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally delete your Cloud Load Balancer from the &lt;a href=&#34;https://mycloud.rackspace.com/load_balancers&#34;&gt;Rackspace UI&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Managing docker services with this one easy trick</title>
      <link>https://tech.paulcz.net/blog/managing-docker-services-with-this-one-easy-trick/</link>
      <pubDate>Sun, 20 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/managing-docker-services-with-this-one-easy-trick/</guid>
      <description>&lt;p&gt;I have been having a lot of internal debate about the idea of running more than one service in a docker container.   A Docker container is built to run a single process in the foreground and to live for only as long as that process is running.  This is great in a utopian world where servers are immutable and sysadmins drink tiki drinks on the beach,  however it doesn&amp;rsquo;t always translate well to the real world.&lt;/p&gt;

&lt;p&gt;Examples where you might want to be able to run multiple servers span from the simple use case of running &lt;code&gt;sshd&lt;/code&gt; as well as your application to running a web app such as &lt;code&gt;wordpress&lt;/code&gt; where you might want both &lt;code&gt;apache&lt;/code&gt; and &lt;code&gt;mysql&lt;/code&gt; running in the same container.&lt;/p&gt;

&lt;p&gt;Wrapping your applications in a supervisor daemon such as &lt;code&gt;runit&lt;/code&gt; seems like a perfect fit for this.  All you need to do is install &lt;code&gt;runit&lt;/code&gt; as part of your &lt;code&gt;dockerfile&lt;/code&gt; and then create appropriate service directories for the apps you want to run in the container.    I was doing some testing of this when I realized a quirk of &lt;code&gt;runit&lt;/code&gt; which I could exploit for evil.&lt;/p&gt;

&lt;p&gt;To start or stop a service with &lt;code&gt;runit&lt;/code&gt; is simply a matter of creating or deleting a symlink in a service directory,   so in theory if you could expose that directory to the server hosting the container you could exploit that to start and stop services from outside of the container.  &lt;code&gt;Docker&lt;/code&gt; volume mapping allows exactly this!&lt;/p&gt;

&lt;p&gt;Below you will find examples of running three services (logstash,elasticsearch,kibana) that make up the &lt;code&gt;logstash&lt;/code&gt; suite.&lt;/p&gt;

&lt;h2 id=&#34;start-by-cloning-the-demo-git-repository-and-run-demo-sh&#34;&gt;Start by cloning the demo git repository and run demo.sh&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/paulczar/docker-runit-demo.git
$ cd docker-runit-demo
$ ./demo.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;demo-sh-script&#34;&gt;demo.sh script&lt;/h3&gt;

&lt;h4 id=&#34;step-1-build-the-container&#34;&gt;Step 1:  Build the container&lt;/h4&gt;

&lt;p&gt;The script uses the below &lt;code&gt;Dockerfile&lt;/code&gt; to build the base container that we&amp;rsquo;ll be running.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Installs runit for service management
#
# Author: Paul Czarkowski
# Date: 10/20/2013

FROM paulczar/jre7
MAINTAINER Paul Czarkowski &amp;quot;paul@paulcz.net&amp;quot;

RUN apt-get update

RUN apt-get -y install curl wget git nginx
RUN apt-get -y install runit || echo

CMD [&amp;quot;/usr/sbin/runsvdir-start&amp;quot;]

&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;step-2-install-the-applications&#34;&gt;Step 2: Install the applications&lt;/h4&gt;

&lt;p&gt;This will take a few minutes the first time as it needs to download &lt;code&gt;logstash&lt;/code&gt;, &lt;code&gt;kibana&lt;/code&gt;, and &lt;code&gt;elasticsearch&lt;/code&gt; and stage them in a local &lt;code&gt;./opt&lt;/code&gt;directory.&lt;/p&gt;

&lt;h4 id=&#34;step-3-start-the-docker-container&#34;&gt;Step 3: Start the Docker container&lt;/h4&gt;

&lt;p&gt;Starts the &lt;code&gt;Docker&lt;/code&gt; container with the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -d -p 8080:80 -p 5014:514 -p 9200:9200 \
  -v $BASE/opt:/opt \
  -v $BASE/sv:/etc/sv \
  -v $BASE/init:/etc/init \
  -v $BASE/service:/etc/service \
  demo/runit
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The container should be up and running&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker ps
ID                  IMAGE               COMMAND                CREATED             STATUS              PORTS
eb495ad92ba0        demo/runit:latest   /usr/sbin/runsvdir-s   4 seconds ago       Up 3 seconds        5014-&amp;gt;514, 8080-&amp;gt;80, 9200-&amp;gt;9200   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However there aren&amp;rsquo;t any services running!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl localhost:8080
curl: (56) Recv failure: Connection reset by peer
$ curl localhost:9200
curl: (56) Recv failure: Connection reset by peer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can start the services with the following commands&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd service
$ ln -s ../sv/elasticsearch
$ ln -s ../sv/logstash
$ ln -s ../sv/kibana
cd ..
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now see the services are running, test the ports and send some data to logstash.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl localhost:8080      
&amp;lt;!DOCTYPE html&amp;gt;&amp;lt;!--[if IE 8]&amp;gt;&amp;lt;html class=&amp;quot;no-js lt-ie9&amp;quot; lang=&amp;quot;en&amp;quot;&amp;gt;&amp;lt;![endif]--&amp;gt;&amp;lt;!--[if gt IE 8]&amp;gt;&amp;lt;!--&amp;gt;&amp;lt;html class=&amp;quot;no-js&amp;quot; lang=&amp;quot;en&amp;quot;&amp;gt;
...
curl localhost:9200
{
  &amp;quot;ok&amp;quot; : true,
  &amp;quot;status&amp;quot; : 200,
...
$tail -100 /var/log/syslog | nc localhost 5014
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Stop a service ?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ rm service/elasticsearch
$ rm service/logstash
$ rm service/kibana
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;bonus-round-logs&#34;&gt;Bonus Round: Logs!&lt;/h2&gt;

&lt;p&gt;The beautify of doing this is that we&amp;rsquo;re actually logging the application output to a mounted volume.   This means we now have access to their logs from the host machine.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ tail opt/logstash/logs/current
$ tail opt/elasticsearch-0.90.5/logs/current
$ tail opt/kibana/logs/access.log
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;cleanup&#34;&gt;Cleanup&lt;/h2&gt;

&lt;p&gt;Unfortunately any files created inside the docker instance are owned by root ( an artifact of docker daemon running as root ).   If you&amp;rsquo;re in The following script will clean out any such files after you&amp;rsquo;ve stopped the docker container.&lt;/p&gt;

&lt;p&gt;It will delete any files/dirs inside your current directory that are owned by root.  Obviously it can be very dangerous to run &amp;hellip; so be careful where you run it from!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo find . -uid 0   -exec rm -rfv {} \;
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Creating immutable servers with chef and docker.io</title>
      <link>https://tech.paulcz.net/blog/creating-immutable-servers-with-chef-and-docker-dot-io/</link>
      <pubDate>Sat, 07 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/creating-immutable-servers-with-chef-and-docker-dot-io/</guid>
      <description>&lt;p&gt;Building applications in a &lt;a href=&#34;http://docker.io&#34;&gt;docker.io&lt;/a&gt; Dockerfile is relatively simple,  but sometimes you want to just install the application exactly as you would normally via already built chef cookbooks.   Turns out this is actually pretty simple.&lt;/p&gt;

&lt;p&gt;The first thing you&amp;rsquo;ll need to do is build a container with chef-client and berkshelf installed.   You can grab the one I&amp;rsquo;ve built by running &lt;code&gt;docker pull paulczar/chef-solo&lt;/code&gt; or build one youself from a &lt;code&gt;Dockerfile&lt;/code&gt; that looks a little something like the following&amp;hellip;&lt;/p&gt;

&lt;h3 id=&#34;creating-a-docker-io-container-with-chef-and-berkshelf&#34;&gt;Creating a docker.io container with chef and berkshelf&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# DOCKER-VERSION 0.5.3
FROM ubuntu:12.10
MAINTAINER Paul Czarkowski &amp;quot;paul@paulcz.net&amp;quot;

RUN apt-get -y update
RUN apt-get -y install curl build-essential libxml2-dev libxslt-dev git
RUN curl -L https://www.opscode.com/chef/install.sh | bash
RUN echo &amp;quot;gem: --no-ri --no-rdoc&amp;quot; &amp;gt; ~/.gemrc
RUN /opt/chef/embedded/bin/gem install berkshelf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;you&amp;rsquo;ll notice I&amp;rsquo;m using the embedded chef ruby to install the berkshelf gem,  this is a handy shortcut to avoid messing around with random ruby versions from your distributions packaging.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;run &lt;code&gt;$ docker build -t paulczar/chef-solo .&lt;/code&gt; to build a usable docker container from the above &lt;code&gt;Dockerfile&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;using-chef-solo-and-berkshelf-to-build-an-application-in-a-docker-io-container&#34;&gt;Using chef-solo and berkshelf to build an application in a docker.io container&lt;/h3&gt;

&lt;p&gt;My &lt;a href=&#34;https://github.com/paulczar/docker-chef-solo&#34;&gt;example application&lt;/a&gt; will install &lt;code&gt;Kibana3&lt;/code&gt; to your docker container.   I&amp;rsquo;ll step through how it works below.&lt;/p&gt;

&lt;h4 id=&#34;chef-solo&#34;&gt;Chef-Solo&lt;/h4&gt;

&lt;p&gt;To run &lt;code&gt;chef-solo&lt;/code&gt; successfully we require two files.   &lt;code&gt;solo.rb&lt;/code&gt; to set up file locations, and `solo.json&amp;rsquo; to set up the json / run list required for your application.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;root = File.absolute_path(File.dirname(__FILE__))

file_cache_path root
cookbook_path root + &#39;/cookbooks&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  &amp;quot;kibana&amp;quot;: {
    &amp;quot;webserver_listen&amp;quot;: &amp;quot;0.0.0.0&amp;quot;
  },
  &amp;quot;run_list&amp;quot;: [
    &amp;quot;recipe[kibana::default]&amp;quot;
  ]
} 
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;berkshelf&#34;&gt;Berkshelf&lt;/h4&gt;

&lt;p&gt;To run &lt;code&gt;berkshelf&lt;/code&gt; we need to build a Berksfile which contains a list of all the chef cookbooks required for the applocation.   Berkshelf will download these cookbooks to a local directory which will be usable by chef-solo.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;site :opscode

cookbook &#39;build-essential&#39;
cookbook &#39;apache2&#39;
cookbook &#39;git&#39;
cookbook &#39;kibana&#39;, github: &#39;lusis/chef-kibana&#39;
cookbook &#39;nginx&#39; , github: &#39;opscode-cookbooks/nginx&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;You can see some of the cookbooks are being pulled from the opscode repository,  whereas others are being pulled directly from github.&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&#34;dockerfile&#34;&gt;Dockerfile&lt;/h4&gt;

&lt;p&gt;All that&amp;rsquo;s left now is to create a Dockerfile that will bring it all together.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# DOCKER-VERSION 0.5.3
FROM paulczar/chef-client
MAINTAINER Paul Czarkowski &amp;quot;paul@paulcz.net&amp;quot;

RUN apt-get -y update
ADD . /chef
RUN cd /chef &amp;amp;&amp;amp; /opt/chef/embedded/bin/berks install --path /chef/cookbooks
RUN chef-solo -c /chef/solo.rb -j /chef/solo.json
RUN echo &amp;quot;daemon off;&amp;quot; &amp;gt;&amp;gt; /etc/nginx/nginx.conf

CMD [&amp;quot;nginx&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Run &lt;code&gt;$ docker build -t demo/kibana3 .&lt;/code&gt; to build your application.&lt;/p&gt;

&lt;p&gt;It will add the local files ( &lt;code&gt;solo.rb&lt;/code&gt;, &lt;code&gt;solo.json&lt;/code&gt;, &lt;code&gt;Berksfile&lt;/code&gt; ) to /chef in the server and then call berkshelf to download the cookbooks and chef-solo to install your application.   Finally it will give &lt;code&gt;nginx&lt;/code&gt; a directive to run in the foreground so that we don&amp;rsquo;t have to do any sneaky prcess control to get it to work with the way &lt;code&gt;docker.io&lt;/code&gt; runs processes.&lt;/p&gt;

&lt;p&gt;To run the resultant &lt;code&gt;docker.io&lt;/code&gt; container you simply need to run &lt;code&gt;$ docker run -d -p 80 demo/kibana3&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Logstash &#43; Opscode Omnibus</title>
      <link>https://tech.paulcz.net/blog/logstash-plus-opscode-omnibus/</link>
      <pubDate>Mon, 06 May 2013 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/logstash-plus-opscode-omnibus/</guid>
      <description>&lt;p&gt;At &lt;a href=&#34;http://devopsdays.org/events/2013-austin/&#34;&gt;DevOps Days Austin&lt;/a&gt; &lt;a href=&#34;http://twitter.com/mattray&#34;&gt;@mattray&lt;/a&gt; did an Openspace session on &lt;a href=&#34;https://github.com/opscode/omnibus-ruby&#34;&gt;Omnibus&lt;/a&gt; which is a toolset based around the concept of installing an app and all of it&amp;rsquo;s prerequisites from source into a directory and then building a package ( either .deb or .rpm ) of that using &lt;a href=&#34;https://github.com/jordansissel/fpm&#34;&gt;fpm&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Having battled many times with OS Packages trying to get newer versions of Ruby, or Redis or other software installed and having to hunt down some random package repo or manually build from source this seems like an excellent idea.&lt;/p&gt;

&lt;p&gt;To learn the basics I decided to build an &lt;a href=&#34;https://github.com/paulczar/omnibus-fpm&#34;&gt;omnibus package for fpm&lt;/a&gt; which helped me work out the kinks and learn the basics.&lt;/p&gt;

&lt;p&gt;From there I moved onto something a little more ambitious&amp;hellip; &lt;a href=&#34;http://logstash.net/&#34;&gt;logstash&lt;/a&gt;, which is an awesome opensource project for log aggregation and searching.&lt;/p&gt;

&lt;p&gt;Using Omnibus I took the Logstash .jar file and bundled in Redis, Kibana, Kibana3(+NodeJS), RabbitMQ, Elasticsearch along with all of their depedencies into a big fat package which installs to /opt/logstash and includes init scripts and default configs for each.&lt;/p&gt;

&lt;p&gt;The Logstash Omnibus project can be found &lt;a href=&#34;https://github.com/paulczar/omnibus-logstash&#34;&gt;here&lt;/a&gt;.  I also uploaded the resultant packages for &lt;a href=&#34;https://s3-us-west-2.amazonaws.com/paulcz-packages/logstash-omnibus-1.1.10_amd64.deb&#34;&gt;Ubuntu 12.04&lt;/a&gt; and &lt;a href=&#34;https://s3-us-west-2.amazonaws.com/paulcz-packages/logstash-omnibus-1.1.10.el6.x86_64.rpm&#34;&gt;RHEL 6&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This gives us a really powerful platform to deploy logstash and all of its prequisites in a completely repeatable manner and not have to worry about the existing versions of Ruby, Java, etc.    It also gives a super simple testing platform where a new user to logstash can install logstash with a single &lt;code&gt;dpkg&lt;/code&gt; or &lt;code&gt;rpm&lt;/code&gt; command and immediately be able to push logs to it via syslog or redis.&lt;/p&gt;

&lt;p&gt;Read more about using and building the &lt;a href=&#34;https://github.com/paulczar/omnibus-logstash/blob/master/README.md&#34;&gt;Logstash Omnibus package here&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Vagrant&#43;Puppet&#43;FPM=Amazeballs</title>
      <link>https://tech.paulcz.net/blog/vagrant-plus-puppet-plus-fpm-equals-amazeballs/</link>
      <pubDate>Sun, 07 Apr 2013 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/vagrant-plus-puppet-plus-fpm-equals-amazeballs/</guid>
      <description>&lt;p&gt;Lately I&amp;rsquo;ve been doing a lot of prototyping with &lt;a href=&#34;http://www.vagrantup.com/&#34;&gt;Vagrant&lt;/a&gt;, specifically for a couple of distinct activities:-&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;building puppet modules using &lt;a href=&#34;https://github.com/elasticdog/puppet-sandbox&#34;&gt;the excellent puppet sandbox&lt;/a&gt; project&lt;/li&gt;
&lt;li&gt;and building RPM packages with &lt;a href=&#34;https://github.com/jordansissel/fpm&#34;&gt;FPM&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I realized I was spending a bunch of time flipping back and forth between Vagrant environments and I had no quick way to utilize RPMs built with FPM inside my puppet modules.&lt;/p&gt;

&lt;p&gt;An idea was born.   I forked off the &lt;a href=&#34;https://github.com/paulczar/puppet-sandbox&#34;&gt;puppet sandbox&lt;/a&gt; project and added a Yum repo module &lt;code&gt;repository&lt;/code&gt; to the standalone puppet provisioner that vagrant uses when it first brings up a box.   It adds a Yum repo on the puppet server called &lt;code&gt;sandbox&lt;/code&gt; and adds a repo file to the client boxes pointing to the repo.   Now I can simply push an RPM to &lt;code&gt;packages/rpm&lt;/code&gt; and run &lt;code&gt;vagrant provision puppet&lt;/code&gt; which reruns puppet and rebuilds the yum repo.&lt;/p&gt;

&lt;p&gt;Given that I often flip back and forth between Ubuntu and CentOS boxes I also created &lt;code&gt;Vagrantfile.centos63&lt;/code&gt; and &lt;code&gt;Vagrantfile.precise64&lt;/code&gt; so I can swiftly destroy the existing environment and bring up another of a different flavour by simply symlinking &lt;code&gt;Vagrantfile&lt;/code&gt; to the appropriate file.&lt;/p&gt;

&lt;p&gt;This worked out pretty well for a while until I realized I was still jumping back and forth between vagrant environments and I realized I had another improvement to make.   So I then went on to create a definition in the puppet sandbox &lt;code&gt;Vagrantfile&lt;/code&gt; file for a &lt;code&gt;FPM server&lt;/code&gt; and a new module in the provisioner to install FPM on it.   Given that this module simply adds a few packages this module works for both CentOS and Ubuntu.&lt;/p&gt;

&lt;p&gt;I also created a couple of sample scripts to download source and build RPMs for both Redis and Elasticsearch which get pushed via the provisioner to &lt;code&gt;/tmp/redis-rpm.sh&lt;/code&gt; and &lt;code&gt;/tmp/elasticsearch-rpm.sh&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Now ( For CentOS boxes at least ) I can very quickly iterate on puppet modules and create RPM packages on the fly and have them instantly available.   The process is very simple and looks a little something like this :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/paulczar/puppet-sandbox
$ cd puppet-sandbox
$ vagrant up puppet fpm client1
$ vagrant ssh fpm
[vagrant@fpm ~]$ sudo /tmp/redis-rpm.sh
  ... 
  ... A bunch of scrolling text while files are downloaded and rpm is built
  ...
[vagrant@fpm ~]$ exit
$ vagrant provision puppet
$ vagrant ssh client1
[vagrant@client1 ~]$ sudo yum clean all
[vagrant@client1 ~]$ sudo yum -y install redis
[vagrant@client1 ~]$ sudo service redis-server start
[vagrant@client1 ~]$ redis-cli ping
PONG
[vagrant@client1 ~]$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If I&amp;rsquo;m building a puppet module that needs redis I can now add the following to it&amp;rsquo;s init.pp ( or more properly create a module for redis and request it from the module I&amp;rsquo;m building )&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  package { &#39;redis&#39;:
    ensure =&amp;gt; &#39;present&#39;;
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Of course Debian/Ubuntu doesn&amp;rsquo;t use Yum/RPM for package management.    I&amp;rsquo;d love to accept a pull request from somebody who wants to extend it to also support a local APT repository.   I left breadcrumbs in the &lt;code&gt;repository&lt;/code&gt; module for some appropriate classes to be spliced in&amp;hellip;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Creating a Github Pages Blog With Octopress</title>
      <link>https://tech.paulcz.net/blog/creating-a-github-pages-blog-with-octopress/</link>
      <pubDate>Sat, 15 Dec 2012 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/creating-a-github-pages-blog-with-octopress/</guid>
      <description>&lt;p&gt;A lot of tech bloggers will write their blog posts in &lt;a href=&#34;http://daringfireball.net/projects/markdown/&#34;&gt;Markdown&lt;/a&gt;, convert it to HTML and paste that HTML into their blog of choice and then in the blog&amp;rsquo;s editor clean it up to suit their blog.   This is an excellent way to create easy to read portable documents that can easily be published in multiple formats.&lt;/p&gt;

&lt;p&gt;However what if there was a way to skip the second part of that and just create a markdown page, submit it into your source control ( you &lt;em&gt;do&lt;/em&gt; use source control right? ) and your blog would automagically update.&lt;/p&gt;

&lt;p&gt;In comes &lt;a href=&#34;http://octopress.org/&#34;&gt;Octopress&lt;/a&gt;,  it&amp;rsquo;s a framework that wraps around &lt;a href=&#34;https://help.github.com/articles/using-jekyll-with-pages&#34;&gt;Jekyll&lt;/a&gt; which is &lt;a href=&#34;https://github.com/&#34;&gt;Github&amp;rsquo;s&lt;/a&gt; blogging engine that powers &lt;a href=&#34;http://pages.github.com/&#34;&gt;Github Pages&lt;/a&gt;.   Essentially you edit Markdown files and &lt;a href=&#34;http://octopress.org/&#34;&gt;Octopress&lt;/a&gt; will compile it into a static-html &lt;a href=&#34;https://help.github.com/articles/using-jekyll-with-pages&#34;&gt;Jekyll&lt;/a&gt; blog.     This means that your blog will be lightning fast ( no need to run an interpreted language in your web server ) and ultra portable.&lt;/p&gt;

&lt;p&gt;Another side benefit is that you can host it for free on &lt;a href=&#34;https://github.com/&#34;&gt;Github&lt;/a&gt; ( as long as you&amp;rsquo;re okay with sharing your source &amp;hellip; and you should be! ) or for free on &lt;a href=&#34;http://www.heroku.com/&#34;&gt;Heroku&lt;/a&gt; ( don&amp;rsquo;t have to share your source ) or host it on any simple no frills Apache, LightHTTP, nginx, node.js, etc server.&lt;/p&gt;

&lt;p&gt;Here is how I&amp;rsquo;m porting my blogger site to Octopress hosted on Github Pages.   I&amp;rsquo;m not using any of the fancy &lt;a href=&#34;https://github.com/mojombo/jekyll/wiki/blog-migrations&#34;&gt;Jekyll migration tools&lt;/a&gt; as I only have a few posts and it will help me get used to the extended syntax that Octopress uses in Markdown.&lt;/p&gt;

&lt;p&gt;As usual the first step is to install any dependencies.   These instructions are for &lt;a href=&#34;http://www.ubuntu.com/&#34;&gt;Ubuntu 12.10&lt;/a&gt; &amp;hellip; modify to suit your OS of choice.&lt;/p&gt;

&lt;p&gt;Most of these steps are taken directly from the &lt;a href=&#34;http://octopress.org/docs/&#34;&gt;Octopress Documentation&lt;/a&gt;,   I&amp;rsquo;m just condensing them into a single document to suit the exact scenario being described in this post.&lt;/p&gt;

&lt;h2 id=&#34;before-you-begin&#34;&gt;Before You Begin&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Install Git&lt;/li&gt;
&lt;li&gt;Install Ruby 1.9.3 via your OS package management or &lt;a href=&#34;http://octopress.org/docs/setup/rbenv/&#34;&gt;rbenv&lt;/a&gt; or &lt;a href=&#34;http://octopress.org/docs/setup/rvm/&#34;&gt;RVM&lt;/a&gt;.&lt;br /&gt;
&lt;em&gt;If using package management may need to install ruby-dev&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Check your Ruby version is at least 1.9.3 and install bundler:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ruby --version 
sudo gem install bundler
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;initial-setup&#34;&gt;Initial Setup&lt;/h2&gt;

&lt;p&gt;Clone the octopress repository and set it up&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone git://github.com/imathis/octopress.git octopress
cd octopress
bundle install

rake install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;rsquo;re going to use Github pages.   Octopress has some rake tasks to make this easier for you.    Your blog will be hosted at &lt;code&gt;http://username.github.com&lt;/code&gt; and you need to create a &lt;a href=&#34;https://github.com/repositories/new&#34;&gt;new Github repository&lt;/a&gt; called &lt;code&gt;username.github.com&lt;/code&gt; that github pages will use the master branch as the html source for your blog.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rake setup_github_pages
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This rake points our clone to the new repistory we just set up, configures your blog&amp;rsquo;s URL and sets up a master branch in the &lt;code&gt;_deploy&lt;/code&gt; directory for deployment.&lt;/p&gt;

&lt;p&gt;edit &lt;code&gt;_config.yml&lt;/code&gt; and fill in your blog name and other details.   There&amp;rsquo;s also some configs for twitter/G+/etc plugins that are worth configuring.&lt;/p&gt;

&lt;h2 id=&#34;write-some-blog-content&#34;&gt;Write some blog content&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Great time to read &lt;a href=&#34;http://octopress.org/docs/blogging&#34;&gt;Blogging Basics&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Create an &lt;code&gt;About&lt;/code&gt; page and a &lt;code&gt;First Post!&lt;/code&gt; blog post:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rake new_page[&amp;quot;About&amp;quot;]
rake new_post[&amp;quot;First Post!&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Edit the Markdown pages that it creates for you with your preferred &lt;a href=&#34;http://sourceforge.net/p/retext/home/ReText/&#34;&gt;Markdown editor&lt;/a&gt;.   The output of the rake commands should provide appropriate hints as to the location of the created files.&lt;/p&gt;

&lt;p&gt;Generate and preview the blog:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rake generate
rake preview
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will generate the contents of your blog and allow you to preview it at [&lt;a href=&#34;http://localhost:4000&#34;&gt;http://localhost:4000&lt;/a&gt;].&lt;/p&gt;

&lt;p&gt;Once you&amp;rsquo;re happy with the contents we can deploy your blog for the first time.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rake deploy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will copy the generated files into &lt;code&gt;_deploy/&lt;/code&gt;, add them to git, commit and push them up to the master branch. In a few seconds you should get an email from Github telling you that your commit has been received and will be published on your site.   Being your first commit it could take 10 minutes for the blog to be available at [&lt;a href=&#34;http://username.github.com&#34;&gt;http://username.github.com&lt;/a&gt;]&lt;/p&gt;

&lt;p&gt;Don&amp;rsquo;t forget to commit your changes to the source branch:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git add .
git commit -m &#39;Added About page and first post!&#39;
git push origin source
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;want-to-edit-your-blog-from-another-machine-or-edit-an-existing-octopress-blog&#34;&gt;Want to edit your blog from another machine,  or edit an existing octopress blog?&lt;/h2&gt;

&lt;p&gt;This is pretty simple ( assuming you have the prerequisites already install ).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;If you run Dropbox you can do this inside of your dropbox folders to make this instantly avaiable on any system you use.&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone git@github.com:username/username.github.com.git
cd username.github.com
git checkout source
mkdir _deploy
cd _deploy
git init
git remote add origin git@github.com:username/username.github.com.git
git pull origin master
cd ..
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;once this is done you can run &lt;code&gt;rake new_post[&amp;quot;title&amp;quot;]&lt;/code&gt; and all the other rake commands needed to edit/preview/publish your blog.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Running KVM and Openvswitch on Ubuntu 12.10</title>
      <link>https://tech.paulcz.net/blog/running-kvm-and-openvswitch-on-ubuntu-12-dot-10/</link>
      <pubDate>Thu, 13 Dec 2012 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/running-kvm-and-openvswitch-on-ubuntu-12-dot-10/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve got an aging VMWare ESXi 4.0 server that needs to be replaced with something a little more modern and flexing.   Obviously at home I don&amp;rsquo;t need all the cool features that licensed VMWare comes with,  but I do want more than just the basic free version.&lt;/p&gt;

&lt;p&gt;After a few weeks of installing and testing alternatives  ( I&amp;rsquo;d really love to run openstack,  but it&amp;rsquo;s just not worth it at home for a single box ) I&amp;rsquo;ve settled on Ubuntu 12.10 server running KVM and Openvswitch.&lt;/p&gt;

&lt;p&gt;After installing Ubuntu 12.10 I did the following to get KVM up and running&amp;hellip;   I cribbed this mostly from &lt;a href=&#34;http://blog.allanglesit.com/2012/10/linux-kvm-ubuntu-12-10-with-openvswitch/&#34;&gt;blog.allanglesit.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Install updates and Pre-requisites&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;First of all, make sure Ubuntu is fully up to date:
&lt;em&gt;sudo to root as pretty much every command here needs to be run as root&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;{% codeblock lang:bash %}
sudo bash
apt-get update
apt-get upgrade
{% endcodeblock %}&lt;/p&gt;

&lt;p&gt;Now we can go ahead and install the necessary packages:&lt;/p&gt;

&lt;p&gt;{% codeblock lang:bash %}
apt-get -y install aptitude apt-show-versions ntp ntpdate vim kvm &lt;br /&gt;
 libvirt-bin vlan virtinst virt-manager virt-viewer openssh-server &lt;br /&gt;
 iperf pv openvswitch-controller openvswitch-brcompat &lt;br /&gt;
 openvswitch-switch nfs-common
{% endcodeblock %}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kill off the default libvirt bridge and nuke ebtables&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We want to delete the default libvirt interface and we don&amp;rsquo;t need ebtables so we&amp;rsquo;ll get rid of that.&lt;/p&gt;

&lt;p&gt;{% codeblock lang:bash %}
virsh net-destroy default
virsh net-autostart &amp;ndash;disable default
service libvirt-bin stop
service qemu-kvm stop
aptitude purge -y ebtables
service openvswitch-switch restart
service openvswitch-controller restart
{% endcodeblock %}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Configure network interfaces&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll be using just a single interface which will be used for both the bridge and the host itself.    We will also be bridging that network into the the vswitch and then configuring an interface for the host OS.    The network configuration will look something like this:&lt;/p&gt;

&lt;p&gt;{% codeblock /etc/network/interfaces %}&lt;/p&gt;

&lt;h1 id=&#34;the-loopback-network-interface&#34;&gt;The loopback network interface&lt;/h1&gt;

&lt;p&gt;auto lo
iface lo inet loopback&lt;/p&gt;

&lt;h1 id=&#34;the-primary-network-interface-bridge&#34;&gt;The primary network interface - bridge!&lt;/h1&gt;

&lt;p&gt;auto eth0
iface eth0 inet manual
up ifconfig $IFACE 0.0.0.0 up
down ifconfig $IFACE down&lt;/p&gt;

&lt;h1 id=&#34;the-host-os-network-interface&#34;&gt;The host OS network interface&lt;/h1&gt;

&lt;h1 id=&#34;dns-settings-here-12-10-resets-resolv-conf-on-reboot&#34;&gt;DNS settings here,  12.10 resets resolv.conf on reboot.&lt;/h1&gt;

&lt;p&gt;auto ovsbr0p1
iface ovsbr0p1 inet static
address 192.168.50.10
netmask 255.255.255.0
gateway 192.168.50.1
dns-nameservers 192.168.50.1
dns-search example.com
{% endcodeblock %}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Configure the openvswitch network&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Now we need to configure the network on the openvswitch.     We need to define the bridge, connect it to the uplink interface and create a port for the host OS.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;note: if you&amp;rsquo;re doing this via SSH it will probably break your session&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;{% codeblock lang:bash %}
ovs-vsctl add-br ovsbr0
ovs-vsctl add-port ovsbr0 eth0
ovs-vsctl add-port ovsbr0 ovsbr0p1 &amp;ndash; set interface ovsbr0p1 type=internal
reboot
{% endcodeblock %}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Modify network service sleep times&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;That took forever to boot.    We can fix that by modifying sleeps in /etc/init/failsafe.conf and reboot again to make sure it helped.&lt;/p&gt;

&lt;p&gt;Change :&lt;/p&gt;

&lt;p&gt;{% codeblock %}
$PLYMOUTH message &amp;ndash;text=&amp;ldquo;Waiting for network configuration&amp;hellip;&amp;rdquo; || :
sleep 40
$PLYMOUTH message &amp;ndash;text=&amp;ldquo;Waiting up to 60 more seconds for network configuration&amp;hellip;&amp;rdquo; || :
sleep 59
$PLYMOUTH message &amp;ndash;text=&amp;ldquo;Booting system without full network configuration&amp;hellip;&amp;rdquo; || :
{% endcodeblock %}&lt;/p&gt;

&lt;p&gt;To :&lt;/p&gt;

&lt;p&gt;{% codeblock %}
$PLYMOUTH message &amp;ndash;text=&amp;ldquo;Waiting for network configuration&amp;hellip;&amp;rdquo; || :
sleep 1
$PLYMOUTH message &amp;ndash;text=&amp;ldquo;Waiting up to 60 more seconds for network configuration&amp;hellip;&amp;rdquo; || :
sleep 1
$PLYMOUTH message &amp;ndash;text=&amp;ldquo;Booting system without full network configuration&amp;hellip;&amp;rdquo; || :
{% endcodeblock %}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;LVM configure&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;re going to also use LVM to for the KVM virtual machines to use as storage.    I have a pair of 500g disks in a software raid1 which I&amp;rsquo;ll use for this.&lt;/p&gt;

&lt;p&gt;{% codeblock lang:bash %}
pvcreate /dev/md0
vgcreate data-disk /dev/md0
lvcreate -L 10G -n ISO data-disk
mkfs.ext4 /dev/data-disk/ISO
mkdir -p /data-disk/ISO
echo &amp;ldquo;/dev/data-disk/ISO /data-disk/ISO defaults    ext4    0 0&amp;rdquo; &amp;gt;&amp;gt; /etc/fstab
mount -a
{% endcodeblock %}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Create VM&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Now we can go ahead and create our first VM.   I&amp;rsquo;ve already downloaded the Ubuntu ISO to /data-disk/ISO&lt;/p&gt;

&lt;p&gt;&lt;em&gt;note: virt-install does not support setting a virtualport type of openvswitch yet .. so we have to trick it&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;{% codeblock lang:bash %}
lvcreate -L 8G -n VM-UbuntuTest data-disk
virt-install &amp;ndash;name UbuntuTest &amp;ndash;hvm &amp;ndash;noautoconsole &amp;ndash;ram 1024 &lt;br /&gt;
&amp;ndash;disk path=/dev/data-disk/VM-UbuntuTest &amp;ndash;nonetworks &amp;ndash;vnc &lt;br /&gt;
&amp;ndash;os-type=linux &amp;ndash;os-variant=ubuntuquantal &lt;br /&gt;
&amp;ndash;cdrom /data-disk/ISO/ubuntu-12.10-server-amd64.iso
{% endcodeblock %}&lt;/p&gt;

&lt;p&gt;set up the networking by editing the VM&amp;rsquo;s XML and adding a network interface stanza just before the &amp;lt;/devices&amp;gt;.&lt;/p&gt;

&lt;p&gt;{% codeblock lang:bash %}
virsh edit UbuntuTest
{% endcodeblock %}&lt;/p&gt;

&lt;p&gt;{% codeblock lang:xml %}
&lt;interface type=&#39;bridge&#39;&gt;
 &lt;source bridge=&#39;ovsbr0&#39;/&gt;
 &lt;virtualport type=&#39;openvswitch&#39; /&gt;
 &lt;model type=&#39;virtio&#39;/&gt;
&lt;/interface&gt;
{% endcodeblock %}&lt;/p&gt;

&lt;p&gt;The VM will need to be reset to pick up the network change,  however that will cause it to drop the ISO mount.  We can either continue through with the OS install without networking or reset the VM and then re-attach the ISO as a CD.    I connected to it from my desktop using the VirtualMachineManager GUI to do that but you could use virsh commands if you want to stick to CLI.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Moving VMs from VMWare to KVM</title>
      <link>https://tech.paulcz.net/blog/moving-vms-from-vmware-to-kvm/</link>
      <pubDate>Sun, 09 Dec 2012 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/moving-vms-from-vmware-to-kvm/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m migrating from my old VMWare ESXi box to a new machine running Ubuntu 12.10 and KVM.    Not wanting to rebuild all of my VMs I set about trying to work out the best way to migrate the VMs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Steps to make Windows migrate without Bluescreen&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you migrate a windows box it&amp;rsquo;ll bluescreen due to the shock of so much hardware changing.   The following .reg hack will prevent this from happening by opening up access to a bunch of random system drivers.   Copy and run the .reg file in your VM before doing any further steps.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://gist.github.com/4247499&#34;&gt;c:\temp\vmdriverhack.reg&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Move your VM to shared storage&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I mounted an nfs partition to vmware from my freenas box and migrated the data across using the vmware datastore tools.   If you don&amp;rsquo;t have a NAS you could export a directory from your KVM server to achieve the same goal.&lt;/p&gt;

&lt;p&gt;To move VMs in ESXi we click on the ESXi server, pick the Configuration tab,  click on &amp;lsquo;Storage&amp;rsquo; and then right-click on the datastore and select &amp;lsquo;Browse Datastore&amp;rsquo;.   Select the VM folder you wish to move and click the move icon.&lt;/p&gt;

&lt;p&gt;{% img &lt;a href=&#34;https://lh6.googleusercontent.com/-R5x6JyT5x14/UMUkr4qNr4I/AAAAAAAAAHY/CYhOlizOLb8/s640/VMWare-move-1.png&#34;&gt;https://lh6.googleusercontent.com/-R5x6JyT5x14/UMUkr4qNr4I/AAAAAAAAAHY/CYhOlizOLb8/s640/VMWare-move-1.png&lt;/a&gt; %}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Convert your VM to qcow2 format&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Next we need to make your VM available to the KVM server.   Again I just mounted my nfs volume to it.     You could run the VM from NFS if you want,  but I want to run it locally.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
kvm-img convert -O qcow2 /mnt/freenas/vms/WindowsGuest/WindowsGuest.vmdk \
  /data-disk/VMs/WindowsGuest.qcow2
&lt;/code&gt;`&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;ll take a while to convert.   Once it&amp;rsquo;s done you can make it available to KVM.    I used the VirtualMachineManager GUI,  but you could use virsh if you want.   Simply create a new VM like you would usually except check the &amp;lsquo;Import existing disk image&amp;rsquo; option.&lt;/p&gt;

&lt;p&gt;{% img &lt;a href=&#34;https://lh6.googleusercontent.com/-dXfvXbUOE5c/UMUkwnky73I/AAAAAAAAAHg/DDjgO2CAwqI/s800/vmware-2.png&#34;&gt;https://lh6.googleusercontent.com/-dXfvXbUOE5c/UMUkwnky73I/AAAAAAAAAHg/DDjgO2CAwqI/s800/vmware-2.png&lt;/a&gt; %}&lt;/p&gt;

&lt;p&gt;Then choose your VM image and set the OS versions.&lt;/p&gt;

&lt;p&gt;{% img &lt;a href=&#34;https://lh3.googleusercontent.com/-OybiOO1KsGI/UMUk9JbhzpI/AAAAAAAAAHo/zayosuFa5co/s800/vmware-3.png&#34;&gt;https://lh3.googleusercontent.com/-OybiOO1KsGI/UMUk9JbhzpI/AAAAAAAAAHo/zayosuFa5co/s800/vmware-3.png&lt;/a&gt; %}&lt;/p&gt;

&lt;p&gt;If Windows then choose &amp;lsquo;edit VM before starting it and select the realtek network adaptor.&lt;/p&gt;

&lt;p&gt;{% img &lt;a href=&#34;https://lh4.googleusercontent.com/-MHBkRrhkCI4/UMUlQJ_6ZcI/AAAAAAAAAHw/-Dj9UYNszgU/s800/VMWare-4.png&#34;&gt;https://lh4.googleusercontent.com/-MHBkRrhkCI4/UMUlQJ_6ZcI/AAAAAAAAAHw/-Dj9UYNszgU/s800/VMWare-4.png&lt;/a&gt; %}&lt;/p&gt;

&lt;p&gt;Once created the VM should start up fine.   If the VM is running Windows you&amp;rsquo;ll probably need to re-activate it as the hardware change will make it think you&amp;rsquo;ve pirated it.&lt;/p&gt;

&lt;p&gt;{% img &lt;a href=&#34;https://lh5.googleusercontent.com/-iJuM6CXhWVo/UMUlnT4qjHI/AAAAAAAAAH4/mahRyoBn4ZM/s800/vmware-5.png&#34;&gt;https://lh5.googleusercontent.com/-iJuM6CXhWVo/UMUlnT4qjHI/AAAAAAAAAH4/mahRyoBn4ZM/s800/vmware-5.png&lt;/a&gt; %}&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Moving from a self hosted Wordpress blog to Blogger</title>
      <link>https://tech.paulcz.net/blog/moving-from-a-self-hosted-wordpress-blog-to-blogger/</link>
      <pubDate>Sun, 21 Oct 2012 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/moving-from-a-self-hosted-wordpress-blog-to-blogger/</guid>
      <description>&lt;p&gt;I have a self hosted &lt;a href=&#34;http://xesla.ro&#34;&gt;wordpress food blog&lt;/a&gt; hosted on an old silly domain that I&amp;rsquo;ve wanted to move away from for a while.   I also want to stop paying hosting fees some time soon.   Since I&amp;rsquo;m already looking at moving a lot of my day-to-day activities to the &amp;lsquo;cloud&amp;rsquo;  it made sense to pick a blogging platform that ties into a major cloud hub.   Google&amp;rsquo;s Blogger was the obvious choice as I&amp;rsquo;m already using the Google Apps platform for my new domain &lt;code&gt;paulcz.net&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Creating a blog on blogger is dead simple.   I went ahead and created two: &lt;a href=&#34;http://food.paulcz.net&#34;&gt;food.paulcz.net&lt;/a&gt; for the new food blog and &lt;a href=&#34;http://tech.paulcz.net&#34;&gt;tech.paulcz.net&lt;/a&gt; to start journalling random tech things.     Transfering the blog content itself  is quite simple,  however, doing it in a way as to preserve links between posts, from other sites, and teaching the search engines how to find your new site requires a little more trickery.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m assuming you already have some technical know-how &amp;hellip;  this isn&amp;rsquo;t for the Octogenarians or Luddites in the audience.     You&amp;rsquo;ll need access to a linux host and have a basic understanding of editing files in vim and navigating the linux  command prompt as well as be able to navigate the Wordpress  and Blogger interfaces.&lt;/p&gt;

&lt;p&gt;These examples  use my own blog so where you see &lt;a href=&#34;http://food.paulcz.net&#34;&gt;food.paulcz.net&lt;/a&gt; please modify the text to your own blog URL if you want to use them.   Some of the scripting is a bit silly and could be cleaner but they&amp;rsquo;re throwaway scripts and I didn&amp;rsquo;t want to spend too much time on them.&lt;/p&gt;

&lt;h2 id=&#34;step-1-perform-initial-import-export-of-blog&#34;&gt;Step 1.   Perform Initial Import/Export of blog.&lt;/h2&gt;

&lt;p&gt;Log into your wordpress blog and export the blog to an XMLfile ( &lt;code&gt;wordpress_export.xml&lt;/code&gt; ).     You should be able to do this by going to &lt;code&gt;Dashboard -&amp;gt; Tools -&amp;gt; Export -&amp;gt; Download Export file&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Browse  to &lt;a href=&#34;http://wordpress2blogger.appspot.com/&#34;&gt;http://wordpress2blogger.appspot.com/&lt;/a&gt;.   This site will convert the file from Wordpress format to Blogger format.   Upload and Convert your file, saving it to &lt;code&gt;blogger_export.xml&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Log into blogger and create a new blog.   if you&amp;rsquo;re using your own domain set up the address in basic settings and then go to &lt;code&gt;Other Settings -&amp;gt; Import blog&lt;/code&gt;  and select the &lt;code&gt;blogger-export.xml&lt;/code&gt; file.   You&amp;rsquo;ll also need to publish all the posts ( they don&amp;rsquo;t seem to publish by default ).  You can do this  50 at a  time from the Posts section.&lt;/p&gt;

&lt;p&gt;Now if you have a small blog, or don&amp;rsquo;t have any intrasite links you&amp;rsquo;re basically done here.   However A lot of my posts have links back to other posts and these are not converted.   This means that there&amp;rsquo;s tons of links on my new blog posting back to my old blog.&lt;/p&gt;

&lt;p&gt;Thankfully I don&amp;rsquo;t upload images to the blog, rather link to them on my picasa/google+ albums.   This means I don&amp;rsquo;t have to deal with some wacky image stuff.&lt;/p&gt;

&lt;h2 id=&#34;step-2-modify-links-to-point-to-new-site&#34;&gt;Step 2.   Modify links to point to new site.&lt;/h2&gt;

&lt;p&gt;The first thing to do is get a list of all blog post links on your new Blogger site.   This is pretty easy,  you can do it via an RSS feed and some perl magic.&lt;/p&gt;

&lt;p&gt;Browse to your new blog like so:   &lt;a href=&#34;http://food.paulcz.net/feeds/posts/default?start-index=1&amp;amp;max-results=999&#34;&gt;http://food.paulcz.net/feeds/posts/default?start-index=1&amp;amp;max-results=999&lt;/a&gt;.   This  will give you a page showing your entire blog ( assuming you have less than 999 posts ).    Save this file from your browser as default.xhtml.&lt;/p&gt;

&lt;p&gt;now create a perl script called &lt;code&gt;urls.pl&lt;/code&gt; &amp;hellip;    notice the URL ( with \ escaped characters ) .. you&amp;rsquo;ll need to modify this to match your new blogger url.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-perl&#34;&gt;#!/usr/bin/perl
open (TXT,&amp;quot;&amp;lt; default.xhtml&amp;quot;);
while (&amp;lt;TXT&amp;gt;) {
 chomp;
 /(http:\/\/food\.paulcz\.net\/\d\d\d\d.*?html)/;
 print $1 . &amp;quot;\n&amp;quot;;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then run the following:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;perl urls.pl | uniq &amp;gt; blogger_urls.txt&#39;
grep &amp;quot;&amp;lt;link&amp;gt;&amp;quot; wordpress_export.xml | sed &amp;quot;s/&amp;lt;link&amp;gt;//&amp;quot; | sed &amp;quot;s/&amp;lt;\/link&amp;gt;//&amp;quot; \
 | perl -e &#39;print reverse &amp;lt;&amp;gt;&#39; &amp;gt; wordpress_posts.txt
cp blogger_export.xml blogger_munge.xml
vim -O wordpress_posts.txt blogger_urls.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This  will extract a unique list of URLs  of posts from both your new blogger and old wordpress sites. Chances are they won&amp;rsquo;t match up exactly and some hand editing will be required.  That&amp;rsquo;s okay the  last line  above will open the  files side-by-side in vim  to allow you  to clean this  up.     Hand edit each side  to ensure that both URLs on the same line match the same a posts.&lt;/p&gt;

&lt;p&gt;Once that is done we can write  and run some more perl to create a shell script  that will modify the  &lt;code&gt;blogger_munge.xml&lt;/code&gt; file ( copied from the original &lt;code&gt;blogger_export.xml&lt;/code&gt; above ) replacing all your old links.&lt;/p&gt;

&lt;p&gt;Create the following perl script named &lt;code&gt;create_sed.pl&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-perl&#34;&gt;#!/usr/bin/perl
my @combined;
open(TXT, &amp;quot;&amp;lt; wordpress_posts.txt&amp;quot;);
my @wordpress = &amp;lt;TXT&amp;gt;;
close TXT;
open(TXT, &amp;quot;&amp;lt; blogger_urls&amp;quot;);
my @blogger = &amp;lt;TXT&amp;gt;;
chomp(@wordpress);
chomp(@blogger);
$size = $#blogger;
for ( $count=0;$count&amp;lt;=$size;$count++ ) {
 $string = $wordpress[$count] . &amp;quot;|&amp;quot; . $blogger[$count];
 push(@combined, $string);
}

foreach ( @combined ) {
 s/\//\\\//g;
 s/\./\\./g;
 s/\|/\//;
 print &amp;quot;sed -i \&#39;s/$_/g\&#39; blogger-munge.xml\n&amp;quot;;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now run &lt;code&gt;create_sed.pl&lt;/code&gt;, and the resultant &lt;code&gt;munge.sh&lt;/code&gt; script.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;perl create_sed.pl  &amp;gt; munge.sh
sh munge.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;now you have a blogger_munge.xml file that links back to its own articles.    There may be some residual links  back to your old  site.   If you&amp;rsquo;re a perfectionist you  could hand edit  the XML to fix this,  but I&amp;rsquo;d say its now good  enough.&lt;/p&gt;

&lt;h2 id=&#34;step-3-re-create-your-blogger-blog&#34;&gt;Step 3. Re-Create your blogger blog.&lt;/h2&gt;

&lt;p&gt;Now you&amp;rsquo;ll want to delete your initial blogger blog and recreate it.   Remember to set your URL again, then import the &lt;code&gt;blogger_munge.xml&lt;/code&gt; file and republish all the posts.&lt;/p&gt;

&lt;h2 id=&#34;step-4-set-redirects-from-your-old-blog-space&#34;&gt;Step 4.  Set redirects from your old blog space.&lt;/h2&gt;

&lt;p&gt;Since we have the matching pairs of URLs  it becomes quite simple to set up some permanent redirects from your apache config, or even better from inside a &lt;code&gt;.htaccess&lt;/code&gt; file.  Setting a permanent redirect not only redirects the user to the correct place,  but in theory also informs a search engine ( on its next pass over your blog )  to update its database to the new location.&lt;/p&gt;

&lt;p&gt;Links from around the web pointing at your old domain will continue to work for as long as the redirects work.    At some point if you want to retire your old domain this will break those links&amp;hellip;   But there&amp;rsquo;s not a great deal you can do about that.&lt;/p&gt;

&lt;p&gt;save the following perl snippet as &lt;code&gt;rewrite.pl&lt;/code&gt;.  remember to rewrite any strings specific to my domains to match your own:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-perl&#34;&gt;#!/usr/bin/perl
my @combined;
my @wordpress;
open(TXT, &amp;quot;&amp;lt; wordpress_posts.txt&amp;quot;);
while (&amp;lt;TXT&amp;gt;) {
 chomp;
 s/\/$//;  # remove trailing /
 my @temp = split &amp;quot;/&amp;quot;;
 $last = pop @temp;
 push(@wordpress, $last);
}
close TXT;
open(TXT, &amp;quot;&amp;lt; blogger_urls&amp;quot;);
my @blogger = &amp;lt;TXT&amp;gt;;
chomp(@blogger);
$size = $#blogger;
for ( $count=0;$count&amp;lt;=$size;$count++ ) {
 $string = $wordpress[$count] . &amp;quot;|&amp;quot; . $blogger[$count];
 push(@combined, $string);
}
print &amp;quot;rewriteEngine on\n&amp;quot;;
foreach ( @combined ) {
 s/\|/ /;
 s/^http:\/\/*.?\///;
 print &amp;quot;rewriteRule $_ [R=permanent,L]\n&amp;quot;;
}
print &amp;quot;rewriteRule ^.*\$ http://food.paulcz.net [R=permanent,L]\n&amp;quot;; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Run the script and output to a file.   The contents of this file can be added to a .htaccess file and will immediately start redirecting your traffic to the correct post on your new blog.   Any hits that don&amp;rsquo;t match a mapped blog post will get redirected to the main page of your blog.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;perl rewrite.pl &amp;gt; rewrite.txt 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Inspect the file and if it looks correct you can append it to your &lt;code&gt;.htaccess&lt;/code&gt; file.   If you have other redirect rules already in your &lt;code&gt;.htaccess&lt;/code&gt; file you&amp;rsquo;ll need to remove them or comment them out.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat rewrite.txt &amp;gt;&amp;gt; /var/www/html/.htaccess
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There&amp;rsquo;s no need to restart apache for a changed .htaccess file,  so you can immediately test that the redirects are working &amp;hellip;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://xesla.ro&#34;&gt;http://xesla.ro&lt;/a&gt; &amp;ndash;&amp;gt; &lt;a href=&#34;http://food.paulcz.net/&#34;&gt;http://food.paulcz.net/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://xesla.ro/wordpress/cooking/prickly-pear-syrup&#34;&gt;http://xesla.ro/&amp;hellip;/prickly-pear-syrup&lt;/a&gt; &amp;ndash;&amp;gt; &lt;a href=&#34;http://food.paulcz.net/2012/08/prickly-pear-syrup.html&#34;&gt;http://food.paulcz.net/&amp;hellip;/prickly-pear-syrup.html&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Perfect!  we&amp;rsquo;re working.&lt;/p&gt;

&lt;h2 id=&#34;step-5-more-things-to-do&#34;&gt;Step 5.   More Things to do &amp;hellip; ?&lt;/h2&gt;

&lt;p&gt;This has taken care of almost everything I care about.  However I do want to be able to cancel my webhost subscription so I&amp;rsquo;ll need to try and find a cloud type service to perform the redirects for me.    &lt;a href=&#34;www.heroku.com&#34;&gt;Heroku&lt;/a&gt; is probably the place I&amp;rsquo;ll go for that.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>