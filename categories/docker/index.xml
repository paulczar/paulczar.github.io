<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Docker on Paul Czarkowski</title>
    <link>http://tech.paulcz.net/categories/docker/</link>
    <description>Recent content in Docker on Paul Czarkowski</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 10 Jan 2016 10:22:22 -0600</lastBuildDate>
    <atom:link href="http://tech.paulcz.net/categories/docker/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Flexible Private Docker Registry Infrastructure</title>
      <link>http://tech.paulcz.net/2016/01/flexible-docker-registry-infrastructure/</link>
      <pubDate>Sun, 10 Jan 2016 10:22:22 -0600</pubDate>
      
      <guid>http://tech.paulcz.net/2016/01/flexible-docker-registry-infrastructure/</guid>
      <description>

&lt;p&gt;Previously I showed how to run a &lt;a href=&#34;http://tech.paulcz.net/2016/01/deploying-a-secure-docker-registry/&#34;&gt;basic secure Docker Registry&lt;/a&gt;.  I am now going to expand on this to show you something that you might use in production as part of your CI/CD infrastructure.&lt;/p&gt;

&lt;p&gt;The beauty of running Docker is that you &lt;em&gt;can&lt;/em&gt; push an image from a developer&amp;rsquo;s laptop all the way into production which helps ensure that what you see in development and your various test/qa/stage environments are exactly the same as what you run in production.&lt;/p&gt;

&lt;p&gt;So they tell you anyway. The reality is that you don&amp;rsquo;t ever want to push an image built on a developer&amp;rsquo;s machine into production as you can&amp;rsquo;t be sure what is in it.  Instead you want to have a trusted build server build images from a &lt;code&gt;Dockerfile&lt;/code&gt; in your git repository and have it promoted through your environments from there.&lt;/p&gt;

&lt;p&gt;To ensure the integrity of your images you&amp;rsquo;ll want to run a Docker Registry that can be reached by all of your servers (and potentially people), but can only be written to by your build server (and/or an administrative user).&lt;/p&gt;

&lt;p&gt;You could run your &lt;a href=&#34;https://www.docker.com/docker-registry&#34;&gt;Docker Registry&lt;/a&gt; behind a &lt;a href=&#34;https://docs.docker.com/registry/recipes/&#34;&gt;complicated reverse proxy&lt;/a&gt; and create rules about who can GET/POST/etc through to the &lt;a href=&#34;https://www.docker.com/docker-registry&#34;&gt;Docker Registry&lt;/a&gt; however we can use the magic of &amp;ldquo;&lt;a href=&#34;https://github.com/panicsteve/cloud-to-butt&#34;&gt;The Cloud&lt;/a&gt;&amp;rdquo; to reduce the complexity and thus the need for a reverse proxy.&lt;/p&gt;

&lt;p&gt;You will want to use either the &lt;a href=&#34;https://wiki.openstack.org/wiki/Swift&#34;&gt;Openstack Swift&lt;/a&gt; or the &lt;a href=&#34;https://aws.amazon.com/s3/&#34;&gt;Amazon S3&lt;/a&gt; object storage driver for the &lt;a href=&#34;https://www.docker.com/docker-registry&#34;&gt;Docker Registry&lt;/a&gt;. I will demonstrate using Swift, but using S3 should be very similar.&lt;/p&gt;

&lt;p&gt;You will of course want to also build all of these servers with Configuration Management including the commands to actually run the &lt;a href=&#34;https://www.docker.com/docker-registry&#34;&gt;Docker Registry&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;build-server-s:aceca7a5aff8ee4a89a691ff67eb065d&#34;&gt;Build Server(s)&lt;/h2&gt;

&lt;p&gt;For your build server(s) you&amp;rsquo;ll want to be running an OS with Docker installed on it. I use the &lt;a href=&#34;https://hub.docker.com/_/jenkins/&#34;&gt;Jenkins&lt;/a&gt; Docker image on &lt;a href=&#34;http://coreos.com/&#34;&gt;CoreOS&lt;/a&gt; for both my Jenkins Master and Slaves, however this is just personal preference.&lt;/p&gt;

&lt;p&gt;On each server you want to run a &lt;a href=&#34;https://www.docker.com/docker-registry&#34;&gt;Docker Registry&lt;/a&gt; with your Swift credentials passed through to it. Since we&amp;rsquo;re only accessing this via &lt;code&gt;127.0.0.1&lt;/code&gt; we do not need to secure it with TLS or authentication.&lt;/p&gt;

&lt;p&gt;Run the following on each build server to run the Registry backed by Swift, replacing the OpenStack credentials with your own:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;build01$ docker run -d \
              -p 127.0.0.1:5000:5000 \
              --name registry \
              --restart always \
              -e REGISTRY_STORAGE=swift \
              -e REGISTRY_STORAGE_SWIFT_USERNAME=${OS_USERNAME} \
              -e REGISTRY_STORAGE_SWIFT_PASSWORD=${OS_PASSWORD} \
              -e REGISTRY_STORAGE_SWIFT_TENANT=${OS_TENANT} \
              -e REGISTRY_STORAGE_SWIFT_AUTHURL=${OS_AUTH_URL} \
              -e REGISTRY_STORAGE_SWIFT_CONTAINER=docker-registry \
              registry:2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Push an image to make sure it worked:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;build01$ docker pull alpine
Using default tag: latest
latest: Pulling from library/alpine
Digest: sha256:78a756d480bcbc35db6dcc05b08228a39b32c2b2c7e02336a2dcaa196547a41d
Status: Downloaded newer image for alpine:latest
$ docker tag alpine 127.0.0.1:5000/alpine
$ docker push 127.0.0.1:5000/alpine
The push refers to a repository [127.0.0.1:5000/alpine] (len: 1)
74e49af2062e: Pushed 
latest: digest: sha256:a96155be113bb2b4b82ebbc11cf1b511726c5b41617a70e0772f8180afc72fa5 size: 1369
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you have more that one build server try to pull the image from one of the others, since we&amp;rsquo;re backing the &lt;a href=&#34;https://www.docker.com/docker-registry&#34;&gt;Docker Registry&lt;/a&gt; with an object store they should retrieve it just fine:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;build02$ docker pull 127.0.0.1:5000/alpine
Using default tag: latest
latest: Pulling from alpine

340b2f9a2643: Already exists 
Digest: sha256:a96155be113bb2b4b82ebbc11cf1b511726c5b41617a70e0772f8180afc72fa5
Status: Downloaded newer image for 127.0.0.1:5000/alpine:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;regular-server-s:aceca7a5aff8ee4a89a691ff67eb065d&#34;&gt;Regular Server(s)&lt;/h2&gt;

&lt;p&gt;We have a couple of options here.  You can run a &lt;a href=&#34;https://www.docker.com/docker-registry&#34;&gt;Docker Registry&lt;/a&gt; on each server listening only on localhost, or you can run one or more of them on their own servers that will listen on an IP and be secured with TLS.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll cover the former use case, for the latter use case you can adapt the instructions found &lt;a href=&#34;http://tech.paulcz.net/2016/01/deploying-a-secure-docker-registry/&#34;&gt;at my previous blog post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The important step in either case is to start the Registry as read-only so that regular servers cannot alter the contents of the Registry.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://www.docker.com/docker-registry&#34;&gt;Docker Registry&lt;/a&gt; is fairly light-weight when the files are in external storage and thus will use a neglible amount of your system resources and provides the advantages and security of running the registry on localhost and not needed to set &lt;code&gt;--insecure-registry&lt;/code&gt; settings or worrying about TLS certs for the docker daemon.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d \
      -p 127.0.0.1:5000:5000 \
      --name registry \
      --restart always \
      -e REGISTRY_STORAGE_MAINTENANCE_READONLY=&#39;enabled: true&#39; \
      -e REGISTRY_STORAGE=swift \
      -e REGISTRY_STORAGE_SWIFT_USERNAME=${OS_USERNAME} \
      -e REGISTRY_STORAGE_SWIFT_PASSWORD=${OS_PASSWORD} \
      -e REGISTRY_STORAGE_SWIFT_TENANT=${OS_TENANT} \
      -e REGISTRY_STORAGE_SWIFT_AUTHURL=${OS_AUTH_URL} \
      -e REGISTRY_STORAGE_SWIFT_CONTAINER=docker-registry \
      registry:2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With &lt;code&gt;REGISTRY_STORAGE_MAINTENANCE_READONLY=&#39;enabled: true&lt;/code&gt; set, when we try to push to the registry it should fail:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker push 127.0.0.1:5000/alpine
The push refers to a repository [127.0.0.1:5000/alpine] (len: 1)
f4fddc471ec2: Preparing 
Error parsing HTTP response: invalid character &#39;M&#39; looking for beginning of value: &amp;quot;Method not allowed\n&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;user-access-to-registry:aceca7a5aff8ee4a89a691ff67eb065d&#34;&gt;User Access to Registry:&lt;/h2&gt;

&lt;p&gt;If you want to provide access to regular users and don&amp;rsquo;t mind maintaining the password files locally you can adapt my &lt;a href=&#34;http://tech.paulcz.net/2016/01/deploying-a-secure-docker-registry/&#34;&gt;basic secure Docker Registry&lt;/a&gt; blog post to use the object storage backend.&lt;/p&gt;

&lt;p&gt;Assuming you&amp;rsquo;ve followed the instructions provided to create the TLS certificates you can run two &lt;a href=&#34;https://www.docker.com/docker-registry&#34;&gt;Docker Registry&lt;/a&gt;s each pointing at a different &lt;code&gt;htpasswd&lt;/code&gt; file.&lt;/p&gt;

&lt;p&gt;These can run on the same server, or on seperate servers.  They can also be run on multiple servers that are load balanced via an external load balancer or via round-robin-dns for high availability.&lt;/p&gt;

&lt;h3 id=&#34;read-only-users:aceca7a5aff8ee4a89a691ff67eb065d&#34;&gt;Read only Users&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d \
      -p 443:5000 \
      --name registry \
      --restart always \
      -v /opt/registry \
      -e REGISTRY_STORAGE_MAINTENANCE_READONLY=&#39;enabled: true&#39; \
      -e REGISTRY_STORAGE=swift \
      -e REGISTRY_STORAGE_SWIFT_USERNAME=${OS_USERNAME} \
      -e REGISTRY_STORAGE_SWIFT_PASSWORD=${OS_PASSWORD} \
      -e REGISTRY_STORAGE_SWIFT_TENANT=${OS_TENANT} \
      -e REGISTRY_STORAGE_SWIFT_AUTHURL=${OS_AUTH_URL} \
      -e REGISTRY_STORAGE_SWIFT_CONTAINER=docker-registry \
      -e REGISTRY_AUTH=htpasswd \
      -e &amp;quot;REGISTRY_AUTH_HTPASSWD_REALM=Admin Registry Realm&amp;quot; \
      -e REGISTRY_AUTH_HTPASSWD_PATH=/opt/registry/auth/admin.htpasswd \
      -e REGISTRY_HTTP_SECRET=qerldsljckjqr \
      -e REGISTRY_HTTP_TLS_CERTIFICATE=/opt/registry/ssl/cert.pem \
      -e REGISTRY_HTTP_TLS_KEY=/opt/registry/ssl/key.pem \
      registry:2
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;admin-read-write:aceca7a5aff8ee4a89a691ff67eb065d&#34;&gt;Admin Read/Write&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d \
      -p 444:5000 \
      --name registry \
      --restart always \
      -v /opt/registry \
      -e REGISTRY_STORAGE=swift \
      -e REGISTRY_STORAGE_SWIFT_USERNAME=${OS_USERNAME} \
      -e REGISTRY_STORAGE_SWIFT_PASSWORD=${OS_PASSWORD} \
      -e REGISTRY_STORAGE_SWIFT_TENANT=${OS_TENANT} \      
      -e REGISTRY_STORAGE_SWIFT_AUTHURL=${OS_AUTH_URL} \
      -e REGISTRY_STORAGE_SWIFT_CONTAINER=docker-registry \
      -e REGISTRY_AUTH=htpasswd \
      -e &amp;quot;REGISTRY_AUTH_HTPASSWD_REALM=Read Only Registry Realm&amp;quot; \
      -e REGISTRY_AUTH_HTPASSWD_PATH=/opt/registry/auth/users.htpasswd \
      -e REGISTRY_HTTP_SECRET=hlyrehbrvgszd \
      -e REGISTRY_HTTP_TLS_CERTIFICATE=/opt/registry/ssl/cert.pem \
      -e REGISTRY_HTTP_TLS_KEY=/opt/registry/ssl/key.pem \
      registry:2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Before pushing or pull images to these registries you&amp;rsquo;ll need to log in using &lt;code&gt;docker login myregistrydomain.com:443&lt;/code&gt; or &lt;code&gt;docker login myregistrydomain.com:444&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;By using external storage for the Registry we have increased our ability to run a resiliant Docker Registry with no single points of failure. All of the servers access the registry itself via localhost which means they have almost no reliance on external systems (except for a very robust object storage platform) and no need for complicated authentication systems.&lt;/p&gt;

&lt;p&gt;We also provide access to both Admin (read/write) and Regular (read-only) users via &lt;code&gt;htpasswd&lt;/code&gt; files and &lt;code&gt;TLS&lt;/code&gt; certificates/encryption which can be managed by Configuration Management.&lt;/p&gt;

&lt;p&gt;It goes without saying that you should further lock down all of these services with network based access restrictions in the form of Firewall/IPTables/Security-Groups so that only certain trusted networks can access any of the public endpoints we have created.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deploying a Simple and Secure Docker Registry</title>
      <link>http://tech.paulcz.net/2016/01/deploying-a-secure-docker-registry/</link>
      <pubDate>Sun, 10 Jan 2016 05:22:22 -0600</pubDate>
      
      <guid>http://tech.paulcz.net/2016/01/deploying-a-secure-docker-registry/</guid>
      <description>

&lt;p&gt;There comes a time in everybody&amp;rsquo;s life where they realize they have to run their own &lt;a href=&#34;https://www.docker.com/docker-registry&#34;&gt;Docker Registry&lt;/a&gt;. Unfortunately there&amp;rsquo;s not a lot of good information on how to run one. &lt;a href=&#34;http://docker.com&#34;&gt;Docker&lt;/a&gt;&amp;rsquo;s documentation is pretty good, but is verbose and across a lot of different pages which means having half a dozen tabs open and searching for the right information.&lt;/p&gt;

&lt;p&gt;While it&amp;rsquo;s pretty common to run the &lt;a href=&#34;https://www.docker.com/docker-registry&#34;&gt;Docker Registry&lt;/a&gt; itself with little to no security settings and fronting it with NGINX or Apache to provide this security I wanted to show how it can be done with just the &lt;a href=&#34;http://docker.com&#34;&gt;Docker&lt;/a&gt; Registry. If you need to do really clever stuff like authenticate against LDAP then you&amp;rsquo;ll want to go down the reverse proxy road.&lt;/p&gt;

&lt;p&gt;This example will demonstrate using just the &lt;a href=&#34;https://www.docker.com/docker-registry&#34;&gt;Docker Registry&lt;/a&gt; itself with both TLS certificate backed encryption and Certificate based endpoint authorization.&lt;/p&gt;

&lt;p&gt;For simplicity it will assume a single registry running on the local filesystem and will avoid using OS specific init (systemd/upstart/etc) systems by focusing just on the docker commands themselves.  This should work on any system capable of running &lt;a href=&#34;http://docker.com&#34;&gt;Docker&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;preparation:6cef620af73a5d9314be6797578405bb&#34;&gt;Preparation&lt;/h2&gt;

&lt;p&gt;Boot a server that has &lt;a href=&#34;http://docker.com&#34;&gt;Docker&lt;/a&gt; installed. For an OS with &lt;a href=&#34;http://docker.com&#34;&gt;Docker&lt;/a&gt; already installed I recommend &lt;a href=&#34;http://coreos.com/&#34;&gt;CoreOS&lt;/a&gt;. However you could just as easily boot Ubuntu or CentOS and run &lt;code&gt;curl -sSL get.docker.com | sudo bash&lt;/code&gt; if you&amp;rsquo;re into that sort of thing.&lt;/p&gt;

&lt;p&gt;SSH into the server and ensure &lt;a href=&#34;http://docker.com&#34;&gt;Docker&lt;/a&gt; is working:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh core@xx.xx.xx.xx
$ docker info
Containers: 0
Images: 0
Server Version: 1.9.1
Storage Driver: overlay
 Backing Filesystem: extfs
Execution Driver: native-0.2
Logging Driver: json-file
Kernel Version: 4.3.3-coreos
Operating System: CoreOS 899.1.0
CPUs: 1
Total Memory: 997.4 MiB
Name: core-01
ID: C5XV:CZ3H:EAO4:ATJ3:ARSO:UOGD:XH3X:UKLZ:V3FO:2LRF:6E3X:CV5K
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;create-certificates:6cef620af73a5d9314be6797578405bb&#34;&gt;Create Certificates&lt;/h2&gt;

&lt;p&gt;To keep this as simple as possible I will demonstrate using the &lt;a href=&#34;https://github.com/paulczar/omgwtfssl&#34;&gt;paulczar/omgwtfssl&lt;/a&gt; image to create certificates. If you would rather create them manually via the &lt;code&gt;openssl&lt;/code&gt; cli see my blog post on &lt;a href=&#34;http://tech.paulcz.net/2016/01/secure-docker-with-tls/&#34;&gt;Securing Docker with TLS&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We need to create a place on the filesystem to store the data for the registry as well as certificates and config data:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo mkdir -p /opt/registry/{data,ssl,config}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can create the certificates, add any IPs and DNS that you might address your registry with including that of any loadbalancer or floating IP that you might have:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run --rm \
  -v /opt/registry/ssl:/certs \
  -e SSL_IP=172.17.8.101 \
  -e SSL_DNS=registry.local \
  paulczar/omgwtfssl
----------------------------
| OMGWTFSSL Cert Generator |
----------------------------

--&amp;gt; Certificate Authority
====&amp;gt; Generating new CA key ca-key.pem
Generating RSA private key, 2048 bit long modulus
................+++
.................................+++
e is 65537 (0x10001)
====&amp;gt; Generating new CA Certificate ca.pem
====&amp;gt; Generating new config file openssl.cnf
====&amp;gt; Generating new SSL KEY key.pem
Generating RSA private key, 2048 bit long modulus
..........................................................+++
.............................................+++
e is 65537 (0x10001)
====&amp;gt; Generating new SSL CSR key.csr
====&amp;gt; Generating new SSL CERT cert.pem
Signature ok
subject=/CN=example.com
Getting CA Private Key

core@core-01 ~ $ ls /opt/registry/ssl/
ca-key.pem  ca.pem  ca.srl  cert.pem  key.csr  key.pem  openssl.cnf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our next step is to create a config file &lt;code&gt;/opt/registry/config/registry.env&lt;/code&gt; which will contain a list of Environment Variables that will be passed into the container:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;For this example I&amp;rsquo;m using the same CA certificate for clients as I did for the server, in reality it should probably be a different CA.&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# location of registry data
REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY=/opt/registry/data

# location of TLK key/cert
REGISTRY_HTTP_TLS_KEY=/opt/registry/ssl/key.pem
REGISTRY_HTTP_TLS_CERTIFICATE=/opt/registry/ssl/cert.pem

# location of CA of trusted clients
REGISTRY_HTTP_TLS_CLIENTCAS_0=/opt/registry/ssl/ca.pem
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All that is left to do now is start the registry container, bind mount in the &lt;code&gt;/opt/registry&lt;/code&gt; directory, pass in the config file, and expose port &lt;code&gt;443&lt;/code&gt; to the internal registry port:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d --name registry \
  -v /opt/registry:/opt/registry \
  -p 443:5000 --restart always \
  --env-file /opt/registry/config/registry.env \
  registry:2
Unable to find image &#39;registry:2&#39; locally
2: Pulling from library/registry
Digest: sha256:a842b52833778977f7b4466b90cc829e0f9aae725aebe3e32a5a6c407acd2a03
Status: Downloaded newer image for registry:2
d0106555b2d0aa30691c75c50b279e6a8bd485aa4ba2f203773e971988253169  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can check that we can access it from the server itself by tagging and pushing the &lt;code&gt;alpine&lt;/code&gt; image to it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker pull alpine
Using default tag: latest
latest: Pulling from library/alpine
Digest: sha256:78a756d480bcbc35db6dcc05b08228a39b32c2b2c7e02336a2dcaa196547a41d
Status: Downloaded newer image for alpine:latest
$ docker tag alpine 127.0.0.1/alpine
$ docker push 127.0.0.1/alpine
The push refers to a repository [127.0.0.1/alpine] (len: 1)
74e49af2062e: Pushed 
latest: digest: sha256:a96155be113bb2b4b82ebbc11cf1b511726c5b41617a70e0772f8180afc72fa5 size: 1369
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To check the security settings worked we&amp;rsquo;ll try to access the docker registry from a remote host:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Anywhere you see &lt;code&gt;172.17.8.101&lt;/code&gt; you will want to replace it with the IP or hostname of your docker registry.&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker pull 172.17.8.101/alpine
Using default tag: latest
Error response from daemon: unable to ping registry endpoint https://172.17.8.101/v0/
v2 ping attempt failed with error: Get https://172.17.8.101/v2/: x509: certificate signed by unknown authority
 v1 ping attempt failed with error: Get https://172.17.8.101/v1/_ping: x509: certificate signed by unknown authority
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;On the server we can see this failure in the docker logs:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker logs registry
2016/01/10 16:18:47 http: TLS handshake error from 172.17.8.1:44096: remote error: bad certificate
2016/01/10 16:18:47 http: TLS handshake error from 172.17.8.1:44098: remote error: bad certificate
2016/01/10 16:18:47 http: TLS handshake error from 172.17.8.1:44099: remote error: bad certificate
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are two things causing this failure. The first is that the remote server does not trust the client because it cannot provide the trusted CA certificate as specified in &lt;code&gt;REGISTRY_HTTP_TLS_CLIENTCAS_0&lt;/code&gt;. The second reason for failure is that the client doesn&amp;rsquo;t trust the &lt;code&gt;CA&lt;/code&gt; of the server.&lt;/p&gt;

&lt;p&gt;If we didn&amp;rsquo;t have &lt;code&gt;REGISTRY_HTTP_TLS_CLIENTCAS_0&lt;/code&gt; set we could simply add &lt;code&gt;--insecure-registry 172.17.8.101&lt;/code&gt; to &lt;code&gt;DOCKER_OPTS&lt;/code&gt; in &lt;code&gt;/etc/default/docker&lt;/code&gt;, however since we do have this set we&amp;rsquo;ll want to take the &lt;code&gt;CA.pem&lt;/code&gt; and save it as &lt;code&gt;/etc/docker/certs.d/172.17.8.101/ca.crt&lt;/code&gt; on the remote machine that you want to trust the registry server.&lt;/p&gt;

&lt;p&gt;I do this with the following commands, you may need to do it differently based on how your server is set up for access:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo mkdir -p /etc/docker/certs.d/172.17.8.101
$ sudo scp core@172.17.8.101:/opt/docker/registry/ca.pem \
    /etc/docker/certs.d/172.17.8.101/ca.crt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we have established trust in both directions we can try to access the docker registry again:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker pull 172.17.8.101/alpine
Using default tag: latest
latest: Pulling from alpine

340b2f9a2643: Already exists 
Digest: sha256:a96155be113bb2b4b82ebbc11cf1b511726c5b41617a70e0772f8180afc72fa5
Status: Downloaded newer image for 172.17.8.101/alpine:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Success!   We know have a &lt;a href=&#34;https://www.docker.com/docker-registry&#34;&gt;Docker Registry&lt;/a&gt; that is secured both with Encryption and an authorization based on each client having a specific CA certificate.  This setup is ideal for providing secure access to a private registry for remote servers.&lt;/p&gt;

&lt;p&gt;If you want to do this in a more automated fashion you can look at the various configuration management communities such as &lt;a href=&#34;https://supermarket.chef.io/cookbooks/docker_registry&#34;&gt;chef&lt;/a&gt; for examples.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Securing Docker with TLS certificates</title>
      <link>http://tech.paulcz.net/2016/01/secure-docker-with-tls/</link>
      <pubDate>Sun, 03 Jan 2016 14:44:30 -0600</pubDate>
      
      <guid>http://tech.paulcz.net/2016/01/secure-docker-with-tls/</guid>
      <description>

&lt;p&gt;By default Docker (and by extension Docker Swarm) has no authentication or authorization on its API, relying instead on the filesystem security of its unix socket &lt;code&gt;/var/run/docker.sock&lt;/code&gt; which by default is only accessible by the root user.&lt;/p&gt;

&lt;p&gt;This is fine for the basic use case of the default behavior of only accessing the Docker API on the local machine via the socket as the root user. However if you wish to use the Docker API over TCP then you&amp;rsquo;ll want to secure it so that you don&amp;rsquo;t give out root access to anyone that happens to poke you on the TCP port.&lt;/p&gt;

&lt;p&gt;Docker supports using TLS certificates (both on the server and the client) to provide proof of identity. When set up correctly it will only allow clients/servers with a certificate signed by a specific CA to talk to eachother. While not providing fine grained access permissions it does at least allow us to listen on a TCP socket and restrict access with a bonus of also providing encryption.&lt;/p&gt;

&lt;p&gt;Here I will detail what is required to secure Docker (and in turn Docker Swarm) running on a &lt;a href=&#34;http://coreos.com/&#34;&gt;CoreOS&lt;/a&gt; server. I will assume you already have a &lt;a href=&#34;http://coreos.com/&#34;&gt;CoreOS&lt;/a&gt; server running as described in my Docker Swarm &lt;a href=&#34;http://tech.paulcz.net/2016/01/running-ha-docker-swarm/&#34;&gt;post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you are only interested in securing Docker itself and not Docker Swarm then this should apply to any server with Docker installed that uses systemd.  Even on systems without systemd it should provide enough details to secure Docker.&lt;/p&gt;

&lt;h2 id=&#34;creating-certificates:ee7ba636086981df3b8f20678b008bf1&#34;&gt;Creating Certificates&lt;/h2&gt;

&lt;p&gt;I will offer two methods to create the certificates, the first by using &lt;code&gt;openssl&lt;/code&gt; to create a CA and then sign a key/cert pair, the second by using the &lt;a href=&#34;https://hub.docker.com/r/paulczar/omgwtfssl/&#34;&gt;paulczar/omgwtfssl&lt;/a&gt; Docker Image which automates the certificate creation process.&lt;/p&gt;

&lt;p&gt;Either way you&amp;rsquo;ll want to start off by creating directories for both the server and client certificate sets:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo mkdir -p /etc/docker/ssl
$ mkdir -p ~/.docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;For this example we&amp;rsquo;re creating the keys and certificates on the server itself, ideally you would do this on your laptop or via configuration management and never store the CA key on a public server.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;openssl:ee7ba636086981df3b8f20678b008bf1&#34;&gt;OpenSSL&lt;/h3&gt;

&lt;p&gt;First run &lt;code&gt;openssl&lt;/code&gt; to create and sign a CA key and certificate and copy the CA certificate into &lt;code&gt;/etc/docker/ssl&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ openssl genrsa -out ~/.docker/ca-key.pem 2048
.+++
..........................................................................................................+++
e is 65537 (0x10001)

$ openssl req -x509 -new -nodes -key ~/.docker/ca-key.pem \
    -days 10000 -out ~/.docker/ca.pem -subj &#39;/CN=docker-CA&#39;

$ ls ~/.docker/
ca-key.pem  ca.pem

$ sudo cp ~/.docker/ca.pem /etc/docker/ssl
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next we&amp;rsquo;ll need an openssl configuration file for the Docker client &lt;code&gt;~/.docker/openssl.cnf&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
extendedKeyUsage = serverAuth, clientAuth
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Followed by a configuration file for the Docker server &lt;code&gt;/etc/docker/ssl/openssl.cnf&lt;/code&gt;.  Add any DNS or IPs that you might use to access the Docker Server with, this is critical as the Golang SSL libraries are very strict:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
extendedKeyUsage = serverAuth, clientAuth
subjectAltName = @alt_names

[alt_names]
DNS.1 = docker.local
IP.1 = 172.17.8.101
IP.2 = 127.0.0.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next create and sign a certificate for the client:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ openssl genrsa -out ~/.docker/key.pem 2048
....................................+++
.............+++
e is 65537 (0x10001)

$ openssl req -new -key ~/.docker/key.pem -out ~/.docker/cert.csr \
    -subj &#39;/CN=docker-client&#39; -config ~/.docker/openssl.cnf

$ openssl x509 -req -in ~/.docker/cert.csr -CA ~/.docker/ca.pem \
    -CAkey ~/.docker/ca-key.pem -CAcreateserial \
    -out ~/.docker/cert.pem -days 365 -extensions v3_req \
    -extfile ~/.docker/openssl.cnf
Signature ok
subject=/CN=docker-client
Getting CA Private Key
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then do the same for the server:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo openssl genrsa -out /etc/docker/ssl/key.pem 2048
................................................................................+++
....................................+++
e is 65537 (0x10001)

$ sudo openssl req -new -key /etc/docker/ssl/key.pem \
    -out /etc/docker/ssl/cert.csr \
    -subj &#39;/CN=docker-server&#39; -config /etc/docker/ssl/openssl.cnf

$ sudo openssl x509 -req -in /etc/docker/ssl/cert.csr -CA ~/.docker/ca.pem \
    -CAkey ~/.docker/ca-key.pem -CAcreateserial \
    -out /etc/docker/ssl/cert.pem -days 365 -extensions v3_req \
    -extfile /etc/docker/ssl/openssl.cnf
Signature ok
subject=/CN=docker-client
Getting CA Private Key
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;omgwtfssl:ee7ba636086981df3b8f20678b008bf1&#34;&gt;OMGWTFSSL&lt;/h3&gt;

&lt;p&gt;If you want to skip manually creating the certificates you can use the &lt;a href=&#34;https://hub.docker.com/r/paulczar/omgwtfssl/&#34;&gt;paulczar/omgwtfssl&lt;/a&gt; image which is a small (&amp;lt; 10mb) Docker image built specifically for creating certificates for situations like this.&lt;/p&gt;

&lt;p&gt;First we&amp;rsquo;ll create our client certs and use a docker volume binding to put the CA and certs into &lt;code&gt;~/.docker&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run --rm -v $(pwd)/.docker:/certs \
    paulczar/omgwtfssl
----------------------------
| OMGWTFSSL Cert Generator |
----------------------------

--&amp;gt; Certificate Authority
====&amp;gt; Using existing CA Key ca-key.pem
====&amp;gt; Using existing CA Certificate ca.pem
====&amp;gt; Generating new config file openssl.cnf
====&amp;gt; Generating new SSL KEY key.pem
Generating RSA private key, 2048 bit long modulus
.............+++
..........+++
e is 65537 (0x10001)
====&amp;gt; Generating new SSL CSR key.csr
====&amp;gt; Generating new SSL CERT cert.pem
Signature ok
subject=/CN=example.com
Getting CA Private Key
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next we&amp;rsquo;ll take ownership of them back from root (because of the docker volume binding) and then create the server certificates using the same CA using a second volume binding to &lt;code&gt;/etc/docker/ssl&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Since this is a server certificate we need to pass the IP and DNS that the server may respond to via the -e command line arguments.&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo cp ~/.docker/ca.pem /etc/docker/ssl/ca.pem
$ chown -R $USER ~/.docker
$ docker run --rm -v /etc/docker/ssl:/server \
    -v $(pwd)/.docker:/certs \
    -e SSL_IP=127.0.0.1,172.17.8.101 \
    -e SSL_DNS=docker.local -e SSL_KEY=/server/key.pem \
    -e SSL_CERT=/server/cert.pem paulczar/omgwtfssl
----------------------------
| OMGWTFSSL Cert Generator |
----------------------------

--&amp;gt; Certificate Authority
====&amp;gt; Using existing CA Key ca-key.pem
====&amp;gt; Using existing CA Certificate ca.pem
====&amp;gt; Generating new config file openssl.cnf
====&amp;gt; Generating new SSL KEY /server/key.pem
Generating RSA private key, 2048 bit long modulus
.................................+++
..................+++
e is 65537 (0x10001)
====&amp;gt; Generating new SSL CSR key.csr
====&amp;gt; Generating new SSL CERT /server/cert.pem
Signature ok
subject=/CN=example.com
Getting CA Private Key
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;using-the-tls-certificates-with-docker:ee7ba636086981df3b8f20678b008bf1&#34;&gt;Using the TLS certificates with Docker&lt;/h2&gt;

&lt;p&gt;Now we have our TLS certificates created and in the correct locations you need to tell Docker to use the TLS certificate and also verify the client.  You do this by creating a drop in systemd unit to modify the existing Docker systemd unit.&lt;/p&gt;

&lt;p&gt;Create the file &lt;code&gt;custom.conf&lt;/code&gt; in &lt;code&gt;/etc/systemd/system/docker.service.d/&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;If you want to restrict local users from using the docker unix socket remove the second -H command line option, if you already have a custom drop in unit you can add the -H and &amp;ndash;tls* arguments to it.&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Service]
Environment=&amp;quot;DOCKER_OPTS=-H=0.0.0.0:2376 -H unix:///var/run/docker.sock --tlsverify --tlscacert=/etc/docker/ssl/ca.pem --tlscert=/etc/docker/ssl/cert.pem --tlskey=/etc/docker/ssl/key.pem&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Reload systemd and the Docker service:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo systemctl daemon-reload
$ sudo systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now when you try to access Docker via the TCP port you should get a TLS error:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H tcp://127.0.0.1:2376 info
Get http://127.0.0.1:2376/v1.21/containers/json: malformed HTTP response &amp;quot;\x15\x03\x01\x00\x02\x02&amp;quot;.
* Are you trying to connect to a TLS-enabled daemon without TLS?
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is because the Docker client does not know to use TLS to communicate with the server.  We can set some environment variables to enable TLS for the client and use the client key we created:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export DOCKER_HOST=tcp://127.0.0.1:2376
$ export DOCKER_TLS_VERIFY=1
$ export DOCKER_CERT_PATH=~/.docker
$ docker info
docker info
Containers: 0
Images: 0
Server Version: 1.9.1
Storage Driver: overlay
 Backing Filesystem: extfs
Execution Driver: native-0.2
Logging Driver: json-file
Kernel Version: 4.3.3-coreos
Operating System: CoreOS 899.1.0
CPUs: 1
Total Memory: 997.4 MiB
Name: core-01
ID: RGVQ:VDUC:Z5LU:IE7I:J6UJ:TFBJ:SSCO:EWG2:QKAW:5FY6:EIAV:MROK
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;using-the-tls-certificates-with-docker-swarm:ee7ba636086981df3b8f20678b008bf1&#34;&gt;Using the TLS certificates with Docker Swarm&lt;/h2&gt;

&lt;p&gt;To secure Docker Swarm using these TLS certificates you will need to create TLS certificate/key pairs for each server using the same CA.&lt;/p&gt;

&lt;p&gt;to add some arguments to the &lt;code&gt;docker run&lt;/code&gt; command that you start Swarm Manager with the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d --name swarm-manager \
    -v /etc/docker/ssl:/etc/docker/ssl \
    --net=host swarm:latest manage \
    --tlsverify \
    --tlscacert=/etc/docker/ssl/ca.pem \
    --tlscert=/etc/docker/ssl/cert.pem \
    --tlskey=/etc/docker/ssl/key.pem \
    etcd://127.0.0.1:2379
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which you can then access using the docker client:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export DOCKER_HOST=tcp://127.0.0.1:2375
$ export DOCKER_TLS_VERIFY=1
$ export DOCKER_CERT_PATH=~/.docker

$ docker info
Containers: 6
Images: 5
Role: primary
Strategy: spread
Filters: health, port, dependency, affinity, constraint
Nodes: 3
 core-01: 172.17.8.101:2376
  └ Status: Healthy
  └ Containers: 2
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 1.023 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=4.3.3-coreos, operatingsystem=CoreOS 899.1.0, storagedriver=overlay
 core-02: 172.17.8.102:2376
  └ Status: Healthy
  └ Containers: 2
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 1.023 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=4.3.3-coreos, operatingsystem=CoreOS 899.1.0, storagedriver=overlay
 core-03: 172.17.8.103:2376
  └ Status: Healthy
  └ Containers: 2
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 1.023 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=4.3.3-coreos, operatingsystem=CoreOS 899.1.0, storagedriver=overlay
CPUs: 3
Total Memory: 3.068 GiB
Name: core-01
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Deploying a HA Docker Swarm Cluster</title>
      <link>http://tech.paulcz.net/2016/01/running-ha-docker-swarm/</link>
      <pubDate>Sat, 02 Jan 2016 14:44:30 -0600</pubDate>
      
      <guid>http://tech.paulcz.net/2016/01/running-ha-docker-swarm/</guid>
      <description>

&lt;p&gt;Given Docker&amp;rsquo;s propensity for creating easy to use tools it shouldn&amp;rsquo;t come as a surprise that Docker Swarm is one of the easier to understand and run of the &amp;ldquo;Docker Clustering&amp;rdquo; options currently out there. I recently built some &lt;a href=&#34;http://terraform.io&#34;&gt;Terraform&lt;/a&gt; configs for deploying a &lt;a href=&#34;https://github.com/openstack/osops-tools-contrib/tree/master/terraform/dockerswarm-coreos&#34;&gt;Highly Available Docker Swarm cluster on Openstack&lt;/a&gt; and learned a fair bit about Swarm in the process.&lt;/p&gt;

&lt;p&gt;This guide is meant to be a platform agnostic howto on installing and running a Highly Available Docker Swarm to show you the ideas and concepts that may not be as easy to understand from just reading some config management code.&lt;/p&gt;

&lt;h2 id=&#34;coreos:f092910207480b0f11ab4bcafecee8ad&#34;&gt;CoreOS&lt;/h2&gt;

&lt;p&gt;The reason for using &lt;a href=&#34;http://coreos.com&#34;&gt;CoreOS&lt;/a&gt; here is that to make Swarm run in High Availability mode as well as being able to support docker networking between hosts we need to use service discovery.  We can choose to use &lt;code&gt;etcd&lt;/code&gt;, &lt;code&gt;consul&lt;/code&gt;, or &lt;code&gt;zookeeper&lt;/code&gt; here, CoreOS comes with &lt;code&gt;etcd&lt;/code&gt; thus makes it an excellent choice for running Docker Swarm.&lt;/p&gt;

&lt;p&gt;You will need three servers capable of running &lt;a href=&#34;http://coreos.com&#34;&gt;CoreOS&lt;/a&gt;.  See the &amp;ldquo;Try Out CoreOS&amp;rdquo; section of their website for various installation methods for different infrastructure. For this guide I will use the official &lt;a href=&#34;https://github.com/coreos/coreos-vagrant&#34;&gt;CoreOS Vagrant Example&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;skip the rest of this section if you install CoreOS for a different platform&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Clone down the Vagrant example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/coreos/coreos-vagrant.git vagrant-docker-swarm 
Cloning into &#39;vagrant-docker-swarm&#39;...
remote: Counting objects: 411, done.
remote: Total 411 (delta 0), reused 0 (delta 0), pack-reused 411
Receiving objects: 100% (411/411), 100.33 KiB | 0 bytes/s, done.
Resolving deltas: 100% (181/181), done.
Checking connectivity... done.
cd vagrant-docker-swarm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Edit the &lt;code&gt;Vagrantfile&lt;/code&gt; to set &lt;code&gt;$num_instances = 3&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;on Unix-like systems you can do this easily with sed&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sed -i &#39;s/\$num_instances = 1/\$num_instances = 3/&#39; Vagrantfile
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Get a new etcd discovery-url:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;if you are on a windows box and don&amp;rsquo;t have curl you can paste the url into a web browser to get the discovery-url&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl https://discovery.etcd.io/new\?size\=3
https://discovery.etcd.io/6a9c62105f04dac40a29b90fbed322ef
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a cloud-init file called &lt;code&gt;user-data&lt;/code&gt; in the base of the repo using the discovery-url from above:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#cloud-config

coreos:
  etcd2:
    discovery: https://discovery.etcd.io/888fd1e440faf680a7abb3fd934da6fd
    advertise-client-urls: http://$public_ipv4:2379
    initial-advertise-peer-urls: http://$public_ipv4:2380
    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
    listen-peer-urls: http://$public_ipv4:2380,http://$public_ipv4:7001
  units:
    - name: etcd2.service
      command: start

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Start up the CoreOS VMs and log into the first one to check everything worked ok:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vagrant up
Bringing machine &#39;core-01&#39; up with &#39;virtualbox&#39; provider...
Bringing machine &#39;core-02&#39; up with &#39;virtualbox&#39; provider...
Bringing machine &#39;core-03&#39; up with &#39;virtualbox&#39; provider...
...
$ vagrant ssh core-01
$ etcdctl member list
3c5901a3db54efa3: name=f1bae7bba7714ed7b4585c6b1256ddb2 peerURLs=http://172.17.8.101:2380 clientURLs=http://172.17.8.101:2379
9eeb141350af8439: name=5c8e57890d114d7d9d7aef662033a6e0 peerURLs=http://172.17.8.103:2380 clientURLs=http://172.17.8.103:2379
ebcc652087dfe6e8: name=de426249d3b34e23a5706d99b4900665 peerURLs=http://172.17.8.102:2380 clientURLs=http://172.17.8.102:2379
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;docker-swarm:f092910207480b0f11ab4bcafecee8ad&#34;&gt;Docker Swarm&lt;/h2&gt;

&lt;p&gt;Now that we have several CoreOS servers with a working etcd cluster we can move on to setting up Docker Swarm.&lt;/p&gt;

&lt;p&gt;We need to modify docker to listen on tcp port &lt;code&gt;2376&lt;/code&gt; as well as registering itself to service discovery (which will allow us to set up overlay networking later on).  We do this by creating a file &lt;code&gt;custom.conf&lt;/code&gt; in &lt;code&gt;/etc/systemd/system/docker.service.d/&lt;/code&gt; on each server.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;if not using vagrant change &lt;code&gt;eth1&lt;/code&gt; to match the primary interface for your server&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Service]
Environment=&amp;quot;DOCKER_OPTS=-H=0.0.0.0:2376 -H unix:///var/run/docker.sock --cluster-advertise eth1:2376 --cluster-store etcd://127.0.0.1:2379&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then need to reload the &lt;code&gt;systemctl&lt;/code&gt; daemon and then restart docker for these changes to take effect.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo systemctl daemon-reload
sudo systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check that you can access docker via tcp on one of your hosts:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H tcp://172.17.8.101:2376 info
Containers: 0
Images: 0
Engine Version: 1.9.1
Storage Driver: overlay
 Backing Filesystem: extfs
Execution Driver: native-0.2
Logging Driver: json-file
Kernel Version: 4.3.3-coreos
Operating System: CoreOS 899.1.0
CPUs: 1
Total Memory: 997.4 MiB
Name: core-01
ID: BK64:WF3J:5JU6:VYLI:YJSO:CAQH:HPYM:MPTG:FMTA:VLE3:HSMP:F4VQ
Cluster store: etcd://127.0.0.1:2379/docker

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;rsquo;re now ready to run Docker Swarm itself. There are two extra components to running Docker Swarm, a Swarm Agent and a Swarm Manager.&lt;/p&gt;

&lt;p&gt;The Swarm Agent watches the local Docker service via it&amp;rsquo;s TCP port and registers it into service discovery (etcd in our case).  We will run this on each server like so:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;set the &amp;ndash;addr= argument to match the primary IP of each node&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d --name swarm-agent \
    --net=host swarm:latest \
        join --addr=172.17.8.101:2376 \
        etcd://127.0.0.1:2379
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The Swarm Manager watches service discovery and exposes a TCP port (2375) which when accessed by a Docker client will perform actions and schedule containers across the Swarm cluster.&lt;/p&gt;

&lt;p&gt;To ensure High Availability of our cluster we&amp;rsquo;ll run a Swarm Manager on each server:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d --name swarm-manager 
    --net=host swarm:latest manage \
    etcd://127.0.0.1:2379
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Assuming everything went smoothly we can now access the swarm cluster via the Swarm Managers TCP port on any of the servers:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H tcp://172.17.8.101:2375 info
Containers: 6
Images: 5
Role: primary
Strategy: spread
Filters: health, port, dependency, affinity, constraint
Nodes: 3
 core-01: 172.17.8.101:2376
  └ Status: Healthy
  └ Containers: 2
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 1.023 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=4.3.3-coreos, operatingsystem=CoreOS 899.1.0, storagedriver=overlay
 core-02: 172.17.8.102:2376
  └ Status: Healthy
  └ Containers: 2
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 1.023 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=4.3.3-coreos, operatingsystem=CoreOS 899.1.0, storagedriver=overlay
 core-03: 172.17.8.103:2376
  └ Status: Healthy
  └ Containers: 2
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 1.023 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=4.3.3-coreos, operatingsystem=CoreOS 899.1.0, storagedriver=overlay
CPUs: 3
Total Memory: 3.068 GiB
Name: core-01
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our next step is to create an overlay network using the &lt;code&gt;docker network&lt;/code&gt; command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H tcp://172.17.8.101:2375 network create --driver overlay my-net
614913b275dee43a63b48d08b4f5e52f7c0e531d70c63eeb8bb35624470da0c4

$ docker -H tcp://172.17.8.101:2375 network ls                            
NETWORK ID          NAME                DRIVER
86ecb0cf32c6        core-02/none        null                
c7a291ed8366        core-01/host        host                
3747364c5961        core-03/none        null                
8245d6d3ac67        core-02/host        host                
614913b275de        my-net              overlay             
61ead145e9dd        core-01/bridge      bridge              
c9457c4f4588        core-03/bridge      bridge              
b8a6c75cb3b9        core-03/host        host                
bdc4d5ccd778        core-02/bridge      bridge              
66afdc892361        core-01/none        null
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally we&amp;rsquo;ll create a Container on one host and then check that it is accessible from another:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;replace the node==XXXX argument with the hostname of one of your hosts, make sure to use a different node for each docker command&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -it --name=web --net=my-net \
    -H tcp://172.17.8.101:2375 \
    --env=&amp;quot;constraint:node==core-01&amp;quot; nginx
e0fe18c946a5692806608f939d4d6f31c670e3f42bf3942a77142bed2095983e

$ docker run -it --rm --net=my-net \
    -H tcp://172.17.8.101:2375 \
    --env=&amp;quot;constraint:node==core02&amp;quot; busybox wget -O- http://web
Connecting to web (10.0.0.2:80)
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you&amp;rsquo;ve been following along you have successfully deployed a Highly Available Docker Swarm cluster.  From here you could use a load balancer to load balance the Swarm Manager port (2375) or even use Round Robin DNS.&lt;/p&gt;

&lt;p&gt;You may have notice there is no authentication or authorization on this and anybody with a Docker binary and TCP access to your hosts could spin up docker containers. This is fairly easily fixed by using Docker&amp;rsquo;s TLS cert based authorization.&lt;/p&gt;

&lt;p&gt;To read how to secure both Docker and Docker Swarm with TLS read the followup post &lt;a href=&#34;http://tech.paulcz.net/2016/01/secure-docker-with-tls/&#34;&gt;Secure Docker with TLS&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multi Process Docker Images Done Right</title>
      <link>http://tech.paulcz.net/2014/12/multi-process-docker-images-done-right/</link>
      <pubDate>Mon, 22 Dec 2014 21:31:03 -0600</pubDate>
      
      <guid>http://tech.paulcz.net/2014/12/multi-process-docker-images-done-right/</guid>
      <description>

&lt;h2 id=&#34;for-some-values-of-right:1c79ecc8ccdca915b8029a73add2dd3b&#34;&gt;For some values of &amp;lsquo;right&amp;rsquo;&lt;/h2&gt;

&lt;p&gt;Almost since &lt;a href=&#34;http://docker.com&#34;&gt;Docker&lt;/a&gt; was first introduced to the world there has been a fairly strong push to keeping containers to be single process.   This makes a lot of sense and definitely plays into the &lt;a href=&#34;http://12factor.net&#34;&gt;12 Factor&lt;/a&gt; way of thinking where all application output should be pushed to &lt;code&gt;stdout&lt;/code&gt; and docker itself with tools like &lt;a href=&#34;https://github.com/progrium/logspout&#34;&gt;logspout&lt;/a&gt; now has fairly strong tooling to deal with those logs.&lt;/p&gt;

&lt;p&gt;Sometimes however it just makes sense to run more than one process in a container,  a perfect example would be running &lt;a href=&#34;https://github.com/kelseyhightower/confd&#34;&gt;confd&lt;/a&gt; as well as your application in order to modify the application&amp;rsquo;s config file based on changes in service discovery systems like &lt;a href=&#34;https://github.com/coreos/etcd&#34;&gt;etcd&lt;/a&gt;.   The &lt;a href=&#34;https://docs.docker.com/articles/ambassador_pattern_linking/&#34;&gt;ambassador&lt;/a&gt; container way of working can achieve similar things, but I&amp;rsquo;m not sure that running two containers with a process each to run your application is any better than running one container with two processes.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re going run multiple processes you have a few options to do it.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Start the container with the first process adnd then use the new &lt;code&gt;docker exec&lt;/code&gt; command to start the second.&lt;/li&gt;
&lt;li&gt;Start them in sequence in a &lt;code&gt;bash&lt;/code&gt; script and background all but the last process with a &lt;code&gt;&amp;amp;&lt;/code&gt; at the end of the line.&lt;/li&gt;
&lt;li&gt;Use a Process Supervisor such as Supervisord or Runit.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I haven&amp;rsquo;t really messed around with the first option, maybe it could work out, but you&amp;rsquo;d lose the logs from the second process as it would need to output via the first process&amp;rsquo; &lt;code&gt;stdout&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;the-bash-script:1c79ecc8ccdca915b8029a73add2dd3b&#34;&gt;The Bash Script&lt;/h2&gt;

&lt;p&gt;Up until recently the way I have been running multiple processes is via the &lt;code&gt;bash&lt;/code&gt; script method, but it feels really clumsy and fragile and while it works I&amp;rsquo;ve never been particularly fond of it.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s an snippet from such a script from my &lt;a href=&#34;https://github.com/paulczar/docker-elk_confd&#34;&gt;docker-elk_confd&lt;/a&gt; project which builds out the [ELK]() stack using values in &lt;code&gt;etcd&lt;/code&gt; to orchestrate clustering and configuration via &lt;code&gt;confd&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo Starting ${APP_NAME}

confd -node $ETCD -config-file /app/confd.toml -confdir /app &amp;amp;
/opt/elasticsearch/bin/elasticsearch -p /app/elasticsearch.pid &amp;amp;

# while the port is listening, publish to etcd
while [[ ! -z $(netstat -lnt | awk &amp;quot;\$6 == \&amp;quot;LISTEN\&amp;quot; &amp;amp;&amp;amp; \$4 ~ \&amp;quot;.$PUBLISH\&amp;quot; &amp;amp;&amp;amp; \$1 ~ \&amp;quot;$PROTO.?\&amp;quot;&amp;quot;) ]] ; do
  publish_to_etcd
  sleep 5 # sleep for half the TTL
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see I&amp;rsquo;ve started two processes &lt;code&gt;elasticsearch&lt;/code&gt; and &lt;code&gt;confd&lt;/code&gt; both backgrounded and then I finish with a loop which publishes data to etcd every 5 seconds until the &lt;code&gt;elasticsearch&lt;/code&gt; process quits listening on its published tcp port.  This works, but it leaves me feeling a bit icky.&lt;/p&gt;

&lt;h2 id=&#34;process-supervisor:1c79ecc8ccdca915b8029a73add2dd3b&#34;&gt;Process Supervisor&lt;/h2&gt;

&lt;p&gt;I have used various supervisors in containers before but never really liked the experience as I could never get all the logs out to &lt;code&gt;stdout&lt;/code&gt; and using the standard docker logging mechanisms so I&amp;rsquo;ve always gone back to the &lt;code&gt;bash&lt;/code&gt; script method.  Recently while working on the ELK project mentioned above I decided to give using a process supervisor another chance.&lt;/p&gt;

&lt;p&gt;My primary measure of success for using a supervisor going forward was to come up with a way to push all output to the supervisor&amp;rsquo;s stdout so that I can use the regular docker logging.&lt;/p&gt;

&lt;p&gt;I decided to try with &lt;a href=&#34;http://supervisord.org&#34;&gt;supervisor&lt;/a&gt; as a starting point because it is a fairly small install and has an easily templatable config.   At about the same time I was looking at this I found a &lt;a href=&#34;http://supervisord.org&#34;&gt;blog post&lt;/a&gt; ( I believe it was linked in a recent Docker Weekly ) that talked about using &lt;code&gt;supervisor&lt;/code&gt; in docker containers.  They had even (sortof) solved the logging problem,  however the logging was appended with debug lines and made it messy and difficult to read.  I figured there had to be a cleaner way.&lt;/p&gt;

&lt;p&gt;Reading through the documentation I saw that you can specify a file to log each supervised process to.   I just needed a way to hijack that config item to write to supervisor&amp;rsquo;s stdout instead.   Turns out that&amp;rsquo;s quite easy as there&amp;rsquo;s a special device &lt;code&gt;/dev/stdout&lt;/code&gt; which links to &lt;code&gt;/dev/self/fd/1&lt;/code&gt; which is the &lt;code&gt;stdout&lt;/code&gt; for the running application.   I quickly threw together a test and it did indeed pipe the logs from the process through &lt;code&gt;stdout&lt;/code&gt; of supervisor.&lt;/p&gt;

&lt;p&gt;I end up with a &lt;code&gt;/etc/supervisord.conf&lt;/code&gt; ( which is written out by confd before supervisor is started ) file that looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[supervisord]
logfile=/dev/null
pidfile=/var/run/supervisord.pid
nodaemon=true

[program:publish_etcd]
command=/app/bin/publish_etcd
redirect_stderr=true
stdout_logfile=/dev/stdout
stdout_logfile_maxbytes=0
auto_start=true
autorestart=true

[program:confd]
command=confd -node %(ENV_ETCD)s -config-file /app/confd.toml -confdir /app
redirect_stderr=true
stdout_logfile=/dev/stdout
stdout_logfile_maxbytes=0
auto_start=true
autorestart=true

[program:elasticsearch]
command=/opt/elasticsearch/bin/elasticsearch
redirect_stderr=true
stdout_logfile=/dev/stdout
stdout_logfile_maxbytes=0
auto_start=true
autorestart=true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and my boot script that docker runs the following to launch my app:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo Starting ${APP_NAME}
/usr/bin/supervisord -c /etc/supervisor/supervisord.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All output from &lt;code&gt;Elasticsearch&lt;/code&gt;, &lt;code&gt;confd&lt;/code&gt;, &lt;code&gt;supervisord&lt;/code&gt; now output via the docker logging systems so that I can see what is going on by running:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker logs elasticsearch
docker logs -f 7270755ce94c03dda930fbdedeee7722dddf6fdbbf8902aaee52c9f94f2147ca
2014-12-23T04:46:02Z 7270755ce94c confd[37]: INFO /opt/elasticsearch/config/elasticsearch.yml has md5sum 08a09998560b7b786eca1e594b004ddc should be d83b49b485b5acad2666aa03b1ee90a0
2014-12-23T04:46:02Z 7270755ce94c confd[37]: INFO Target config /opt/elasticsearch/config/elasticsearch.yml out of sync
2014-12-23T04:46:02Z 7270755ce94c confd[37]: INFO Target config /opt/elasticsearch/config/elasticsearch.yml has been updated
2014-12-23T04:46:02Z 7270755ce94c confd[37]: INFO /etc/supervisor/supervisord.conf has mode -rw-r--r-- should be -rwxr-xr-x
2014-12-23T04:46:02Z 7270755ce94c confd[37]: INFO /etc/supervisor/supervisord.conf has md5sum 99dc7e8a1178ede9ae9794aaecbca436 should be ad9bc3735991d133a09f4fc665e2305f
2014-12-23T04:46:02Z 7270755ce94c confd[37]: INFO Target config /etc/supervisor/supervisord.conf out of sync
2014-12-23T04:46:02Z 7270755ce94c confd[37]: INFO Target config /etc/supervisor/supervisord.conf has been updated
Starting elasticsearch
2014-12-23 04:46:02,245 CRIT Supervisor running as root (no user in config file)
2014-12-23 04:46:02,251 INFO supervisord started with pid 51
2014-12-23 04:46:03,255 INFO spawned: &#39;publish_etcd&#39; with pid 54
2014-12-23 04:46:03,258 INFO spawned: &#39;elasticsearch&#39; with pid 55
2014-12-23 04:46:03,260 INFO spawned: &#39;confd&#39; with pid 56
==&amp;gt; sleeping for 20 seconds, then testing if elasticsearch is up.
[2014-12-23 04:46:04,146][INFO ][node                     ] [Sultan] version[1.4.2], pid[55], build[927caff/2014-12-16T14:11:12Z]
[2014-12-23 04:46:04,149][INFO ][node                     ] [Sultan] initializing ...
[2014-12-23 04:46:04,156][INFO ][plugins                  ] [Sultan] loaded [], sites []
2014-12-23 04:46:05,158 INFO success: publish_etcd entered RUNNING state, process has stayed up for &amp;gt; than 1 seconds (startsecs)
2014-12-23 04:46:05,159 INFO success: elasticsearch entered RUNNING state, process has stayed up for &amp;gt; than 1 seconds (startsecs)
2014-12-23 04:46:05,161 INFO success: confd entered RUNNING state, process has stayed up for &amp;gt; than 1 seconds (startsecs)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One last thing that I should mention.  the &lt;code&gt;publish_etcd&lt;/code&gt; talk in the supervisor config is running a script that contains the &lt;code&gt;while&lt;/code&gt; loop to make sure that &lt;code&gt;elasticsearch&lt;/code&gt; is listening on the approriate port, If that loop is broken it means that&lt;code&gt;elasticsearch&lt;/code&gt; is not responding and it sends a kill signal to &lt;code&gt;supervisor&lt;/code&gt; which then causes the container to shoot itself in the head because the rest of the  processes running are useless without &lt;code&gt;elasticsearch&lt;/code&gt; running.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>EZBake - A new way to converge docker containers with chef</title>
      <link>http://tech.paulcz.net/2014/05/ezbake-a-new-way-to-converge-docker-containers-with-chef/</link>
      <pubDate>Tue, 13 May 2014 00:00:00 +0000</pubDate>
      
      <guid>http://tech.paulcz.net/2014/05/ezbake-a-new-way-to-converge-docker-containers-with-chef/</guid>
      <description>&lt;p&gt;&lt;code&gt;EZ Bake&lt;/code&gt; came from an idea I had while watching the &lt;a href=&#34;https://twitter.com/hangops&#34;&gt;HangOps&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=clLFKIeSADo&amp;amp;feature=youtu.be&#34;&gt;episode 2014-04-11&lt;/a&gt; in which they were talking about &lt;code&gt;Docker&lt;/code&gt; and Config Management being complementary rather than adversary.&lt;/p&gt;

&lt;p&gt;I have expermented with using &lt;code&gt;Chef&lt;/code&gt; and &lt;code&gt;Docker&lt;/code&gt; together in the &lt;a href=&#34;http://tech.paulcz.net/2013/09/creating-immutable-servers-with-chef-and-docker-dot-io.html&#34;&gt;past&lt;/a&gt; but wanted to tackle the problem from a slightly different angle.  I&amp;rsquo;ve recently been working on some PAAS stuff, both &lt;a href=&#34;http://deis.io&#34;&gt;Deis&lt;/a&gt; and &lt;a href=&#34;http://solum.io&#34;&gt;Solum&lt;/a&gt; these both utilize the tooling from &lt;a href=&#34;https://github.com/flynn/flynn&#34;&gt;Flynn&lt;/a&gt; which builds heroku style &lt;code&gt;buildpacks&lt;/code&gt; in &lt;code&gt;Docker&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;EZ Bake takes chef recipes designed for &lt;code&gt;chef-solo&lt;/code&gt; ( but could easily be extended to do the same for &lt;code&gt;chef-zero&lt;/code&gt;, or &lt;code&gt;chef-client&lt;/code&gt; with a server) in a tarball via &lt;code&gt;stdin&lt;/code&gt; and converges a docker node using that recipe.&lt;/p&gt;

&lt;p&gt;This methodology seems a little weird at first,  but it gives you the ability to ship your Chef cookbooks as self-contained tarballs, or even more interestingly use the &lt;code&gt;git archive&lt;/code&gt; command from your git repository to do this automatically and then pipe that directly to the &lt;code&gt;docker run&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;In order to recognize and run your cookbook ( or repo ) it needs to contain the following files: &lt;code&gt;Berksfile&lt;/code&gt;, &lt;code&gt;solo.json&lt;/code&gt;, &lt;code&gt;solo.rb&lt;/code&gt; in the root of your cookbook.   There is some provision for providing different locations for these via environment variables.   This is pre-ChefDK and will probably become easier with ChefDK.&lt;/p&gt;

&lt;p&gt;I have provided an example in the ezbake repo that will install Java7 in the container.&lt;/p&gt;

&lt;p&gt;This example shows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Converging a container using a local chef recipe&lt;/li&gt;
&lt;li&gt;Committing the container to an image on completion&lt;/li&gt;
&lt;li&gt;Removing the build container&lt;/li&gt;
&lt;li&gt;Running the new image&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;$ git clone paulczar/ezbake
$ cd ezbake/examples
$ ID=$(tar cf - . | sudo docker run -i -a stdin paulczar/ezbake) \
  &amp;amp;&amp;amp; sudo docker attach $ID \
  &amp;amp;&amp;amp; sudo docker commit $ID java7 
  &amp;amp;&amp;amp; sudo docker rm $ID

Running Berkshelf to collect your cookbooks:
Installing java (1.22.0) from site: &#39;http://cookbooks.opscode.com/api/v1/cookbooks&#39;
Converging your container:
[2014-04-12T22:10:24+00:00] INFO: Forking chef instance to converge...
....
[2014-04-12T22:16:52+00:00] INFO: Chef Run complete in 154.563192281 seconds
[2014-04-12T22:16:52+00:00] INFO: Running report handlers
[2014-04-12T22:16:52+00:00] INFO: Report handlers complete

$ sudo docker run -t java7 java -version
java version &amp;quot;1.7.0_51&amp;quot;
Java(TM) SE Runtime Environment (build 1.7.0_51-b13)
Java HotSpot(TM) 64-Bit Server VM (build 24.51-b03, mixed mode)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This could easily be built into a CI pipeline.   a git webhook could call jenkins which would clone the repo and then use a command like  &lt;code&gt;git archive master | docker run -i -a stdin paulczar/ezbake&lt;/code&gt; to converge a container from it.&lt;/p&gt;

&lt;p&gt;It could also very easily be used in &lt;code&gt;Deis&lt;/code&gt; or &lt;code&gt;Solum&lt;/code&gt; as an alternative to a Heroku buildpack.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Running DEIS.IO on Rackspace Cloud</title>
      <link>http://tech.paulcz.net/2014/02/running-deis-io-on-rackspace-cloud/</link>
      <pubDate>Sun, 23 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>http://tech.paulcz.net/2014/02/running-deis-io-on-rackspace-cloud/</guid>
      <description>

&lt;p&gt;I recently did a presentation at the Cloud Austin meetup titled &lt;a href=&#34;http://tech.paulcz.net/presentation-cloud-austin-deis/#/&#34;&gt;Docking with Unicorns&lt;/a&gt; about new PAAS on the block &lt;a href=&#34;http://deis.io&#34;&gt;DEIS&lt;/a&gt;.   Building out DEIS is quite easy,  make more easy by some tight integration they have with Rackspace Cloud.    If you&amp;rsquo;re interested in what deis is go through my slides linked above, and the documentation on their website.    If you want to build out an environment to kick the tires a bit,  then click &amp;lsquo;Read on&amp;rsquo; below and follow me down the rabbit hole.&lt;/p&gt;

&lt;h2 id=&#34;chef-setup:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Chef setup&lt;/h2&gt;

&lt;p&gt;Chef offers a free hosted service for up to five servers.  That&amp;rsquo;s plenty for this exercise so go to the &lt;a href=&#34;https://www.getchef.com/account&#34;&gt;registration page&lt;/a&gt; and create yourself a user.  At some point it will prompt you to generate and save a key, do that and download it.&lt;/p&gt;

&lt;p&gt;Once you have signed up you can download a knife config file and generate a validation key from the &lt;a href=&#34;https://manage.opscode.com/organizations&#34;&gt;Organizations&lt;/a&gt; page.  We can save those down and then move them to a local working directory.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh5.googleusercontent.com/-3R-Z-bRi_s0/UwpipiLhhWI/AAAAAAAAN0Q/W6q_Rb7NFy8/w1240-h663-no/opscode-org-page.png&#34; alt=&#34;chef org setup&#34; /&gt;
&lt;/p&gt;

&lt;h3 id=&#34;prepare-working-environment:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Prepare Working Environment&lt;/h3&gt;

&lt;p&gt;Create a &lt;code&gt;~/paas&lt;/code&gt; working directory and configure your local chef tools like this ( change the Download location to match the files you downloaded above ) :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mkdir -p ~/paas/.chef
$ cd ~/paas
$ mv ~/Downloads/&amp;lt;username&amp;gt;.pem .chef/
$ mv ~/Downloads/knife.rb .chef/
$ mv ~/Downloads/&amp;lt;username&amp;gt;-validator.pem .chef/

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;clone-the-deis-repository:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Clone the Deis Repository&lt;/h3&gt;

&lt;p&gt;Clone the deis project into your paas working directory:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd ~/paas
$ git clone https://github.com/opdemand/deis.git
Cloning into &#39;deis&#39;...
remote: Reusing existing pack: 5651, done.
Receiving objects: 100% (5651/5651), 2.16 MiB | 1.37 MiB/s, done.
remote: Total 5651 (delta 0), reused 0 (delta 0)
Resolving deltas: 100% (3131/3131), done.
Checking connectivity... done

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;install-pre-reqs:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Install Pre-reqs&lt;/h3&gt;

&lt;p&gt;Assuming you have a working &lt;code&gt;Ruby 1.9.3+&lt;/code&gt; and the &lt;code&gt;bundler&lt;/code&gt; gem installed you should be able to use the &lt;code&gt;Gemfile&lt;/code&gt; from the deis project to ensure you have all the necessary tools:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd ~/paas/deis
$ bundle install
bundle install
Fetching gem metadata from https://rubygems.org/.......
Fetching additional metadata from https://rubygems.org/..
Using i18n (0.6.9)
Using multi_json (1.8.4)
Using activesupport (3.2.16)
Using addressable (2.3.5)
...
Using bundler (1.5.2)
Your bundle is complete!
Use `bundle show [gemname]` to see where a bundled gem is installed.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;I had some errors installing the eventmachine gem and had to follow &lt;a href=&#34;https://github.com/gitlabhq/gitlabhq/issues/1051#issuecomment-9176547&#34;&gt;this fix&lt;/a&gt; to get bundle install to work&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;test-chef-connectivity:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Test Chef Connectivity&lt;/h3&gt;

&lt;p&gt;To make sure we configured chef correctly and installed knife as part of the bundle we can run a quick knife command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec knife client list
&amp;lt;USERNAME&amp;gt;-validator
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;create-an-environment-for-deis:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Create an Environment for Deis&lt;/h3&gt;

&lt;p&gt;Deis is currently hardcoded to use the &lt;code&gt;_default&lt;/code&gt; chef environment.    There is a current &lt;a href=&#34;https://github.com/opdemand/deis/issues/523&#34;&gt;issue&lt;/a&gt; on their github to resolve this.   Once that is done I&amp;rsquo;ll update these instructions to create a &lt;code&gt;deis&lt;/code&gt; environment.&lt;/p&gt;

&lt;h3 id=&#34;upload-the-deis-cookbooks:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Upload the Deis Cookbooks&lt;/h3&gt;

&lt;p&gt;If that went well we can upload our cookbooks:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd ~/paas/deis
$ bundle exec berks install
Installing apt (2.3.8) from site: &#39;http://cookbooks.opscode.com/api/v1/cookbooks&#39;
Installing docker (0.31.0) from site: &#39;http://cookbooks.opscode.com/api/v1/cookbooks&#39;
Installing rsyslog (1.10.2) from site: &#39;http://cookbooks.opscode.com/api/v1/cookbooks&#39;
Installing sudo (2.3.0) from site: &#39;http://cookbooks.opscode.com/api/v1/cookbooks&#39;
...
$ bundle exec berks upload
Using apt (2.3.8)
Using docker (0.31.0)
Using rsyslog (1.10.2)
Using sudo (2.3.0)
Installing deis (0.5.1) from git: &#39;https://github.com/opdemand/deis-cookbook.git&#39; with branch: &#39;master&#39; at ref: &#39;6361706a1d3245d2a061ed55f5dd4b7cb60d5e5c&#39;
Using git (2.7.0)
...
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;create-deis-databags:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Create Deis Databags&lt;/h3&gt;

&lt;p&gt;Deis uses some databags to help manage application state.  We can create them like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec knife data bag create deis-formations
Created data_bag[deis-formations]
$ bundle exec knife data bag create deis-apps
Created data_bag[deis-apps]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;prepare-infrastructure:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Prepare Infrastructure&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;m using Rackspace cloud servers for this as I have the (&lt;a href=&#34;http://developer.rackspace.com/blog/developer-love-welcome-to-the-rackspace-cloud-developer-discount.html)[Rackspace&#34;&gt;http://developer.rackspace.com/blog/developer-love-welcome-to-the-rackspace-cloud-developer-discount.html)[Rackspace&lt;/a&gt; Developer Discount] which is enough discount to host this for free.&lt;/p&gt;

&lt;p&gt;Since Deis will want your rackspace credentials to configure worker nodes I recomment creating a user under (&lt;a href=&#34;https://mycloud.rackspace.com/account#users/create)[User&#34;&gt;https://mycloud.rackspace.com/account#users/create)[User&lt;/a&gt; Management] in your account to use for this.&lt;/p&gt;

&lt;h3 id=&#34;create-a-cloud-load-balancer:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Create a Cloud Load Balancer&lt;/h3&gt;

&lt;p&gt;Log into mycloud.rackspace.com and click on the (&lt;a href=&#34;https://mycloud.rackspace.com/load_balancers)[Load&#34;&gt;https://mycloud.rackspace.com/load_balancers)[Load&lt;/a&gt; Balancers] button.  Select the Dallas Region (DFW) and hit &lt;code&gt;Create Load Balancer&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Set the Name to &lt;code&gt;deis&lt;/code&gt; and check the region is set to &lt;code&gt;Dallas (DFW)&lt;/code&gt; and hit &lt;code&gt;Create Load Balancer&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://lh4.googleusercontent.com/-E4cZvoKWlYU/Uwpiqr9xOKI/AAAAAAAAN0o/P3vGqPC8A98/w793-h592-no/rackspace-create-lb.png&#34; alt=&#34;creating load balancer&#34; /&gt;
&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Take note of the public IP of the Load Balancer, we&amp;rsquo;ll need it later.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://lh4.googleusercontent.com/-ORvf6nzEduU/Uwpiqk5eP0I/AAAAAAAAN0k/WZ-NaJn3eJg/w770-h567-no/rackspace-lb.png&#34; alt=&#34;load balancer created&#34; /&gt;
&lt;/p&gt;

&lt;h3 id=&#34;wildcard-dns:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Wildcard DNS&lt;/h3&gt;

&lt;p&gt;Deis&amp;rsquo; proxy layer requires you to set up Wildcard DNS to point to your proxy layer.  There are many ways to achieve this here are two options:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Rackspace Cloud DNS can host wildcard DNS entries, if you already have DNS hosted by rackspace using Cloud DNS simply add an A record for &lt;code&gt;*.deis&lt;/code&gt; under your domain and point it to the IP of your load balancer.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The (&lt;a href=&#34;http://xip.io)[xip.io&#34;&gt;http://xip.io)[xip.io&lt;/a&gt;] domain does wildcard DNS based on your IP.  We can use this with our Cloud Load Balancer to load balance our applications.   My Load Balancer has a public IP of &lt;code&gt;50.56.167.26&lt;/code&gt; therefore my wildcard domain will be &lt;code&gt;50.56.167.26.xip.io&lt;/code&gt;.   Remember this.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;configure-knife-for-rackspace:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Configure Knife for Rackspace&lt;/h3&gt;

&lt;p&gt;The bundle install above already installed the rackspace knife plugin so we just need to add some details to &lt;code&gt;.chef/knife.rb&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat &amp;lt;&amp;lt;&#39;EOF&#39; &amp;gt;&amp;gt; $HOME/.chef/knife.rb
knife[:rackspace_api_username] = &amp;quot;#{ENV[&#39;OS_USERNAME&#39;]}&amp;quot;
knife[:rackspace_api_key]      = &amp;quot;#{ENV[&#39;OS_PASSWORD&#39;]}&amp;quot;
knife[:rackspace_version]      = &#39;v2&#39;
knife[:rackspace_region]       = :dfw
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;install-rackspace-nova-client:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Install Rackspace Nova Client&lt;/h3&gt;

&lt;p&gt;We also need the Nova client:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo pip install rackspace-novaclient
$ cat &amp;lt;&amp;lt;&#39;EOF&#39; &amp;gt;&amp;gt; ~/paas/.chef/openrc
export OS_AUTH_URL=https://identity.api.rackspacecloud.com/v2.0/
export OS_AUTH_SYSTEM=rackspace
export OS_REGION_NAME=DFW
export OS_USERNAME=&amp;lt;RACKSPACE_USERNAME&amp;gt;
export NOVA_RAX_AUTH=1
export OS_PASSWORD=&amp;lt;RACKSPACE_API_KEY&amp;gt;
export OS_NO_CACHE=1
export OS_TENANT_NAME=&amp;lt;RACKSPACE_USERNAME&amp;gt;
EOF
$ source ~/paas/.chef/openrc
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;test-rackspace-connectivity:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Test Rackspace Connectivity&lt;/h3&gt;

&lt;p&gt;Make sure you can connect to Rackspace with Knife:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec knife rackspace server list
Instance ID  Name  Public IP  Private IP  Flavor  Image  State
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Make sure you can connect to Rackspace with nova:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ nova list
+--------------------------------------+-----------------+--------+------------+-------------+----------------------------------------------------------------------------------------+
| ID                                   | Name            | Status | Task State | Power State | Networks                                                                               |
+--------------------------------------+-----------------+--------+------------+-------------+----------------------------------------------------------------------------------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;build-base-images-for-controller-and-nodes:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Build base images for Controller and Nodes.&lt;/h2&gt;

&lt;p&gt;This isn&amp;rsquo;t strictly necessary,  but will help build your nodes quicker on subsequent builds.&lt;/p&gt;

&lt;h3 id=&#34;launce-a-new-instance:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Launce a new instance:&lt;/h3&gt;

&lt;p&gt;If we create a base image and pre-install some software we&amp;rsquo;ll get a faster booting system for auto-provisioning:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec knife rackspace server create \
  --image &#39;80fbcb55-b206-41f9-9bc2-2dd7aac6c061&#39; \
  --node-name &#39;deis-base-image&#39; \
  --flavor &#39;performance1-1&#39;
...
...
Instance ID: 56760bf1-b977-405e-9348-f70b15a14b87
Host ID: 97da00a12312a7e455bda70c6dfab8833953e2a03b081aeedfd68152
Name: deis-base-image
Flavor: 1 GB Performance
Image: Ubuntu 12.04 
Metadata: []
Public DNS Name: 23-253-69-98.xip.io
Public IP Address: 23.253.69.98
Private IP Address: 10.208.101.31
Password: **************
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Take note of the &lt;code&gt;Instance ID&lt;/code&gt;, &lt;code&gt;Public IP Address&lt;/code&gt; and &lt;code&gt;Password&lt;/code&gt;.  We&amp;rsquo;ll need them later.&lt;/p&gt;

&lt;h3 id=&#34;add-users-keys-to-instance:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Add users / keys to instance&lt;/h3&gt;

&lt;p&gt;We&amp;rsquo;re going to add our ssh key as well as a local &lt;code&gt;deis-ops&lt;/code&gt; user to the image to make it easier to manage and troubleshoot later:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ DEIS_IP=&amp;lt;IP_OF_SERVER&amp;gt;
$ ssh-copy-id root@$DEIS_IP
root@162.242.144.193&#39;s password: 
Number of key(s) added: 1
Now try logging into the machine, with:   &amp;quot;ssh &#39;root@162.242.144.193&#39;&amp;quot;
and check to make sure that only the key(s) you wanted were added.
$ ssh root@$DEIS_IP
Welcome to Ubuntu 12.04.3 LTS (GNU/Linux 3.2.0-55-virtual x86_64)

 * Documentation:  https://help.ubuntu.com/

  System information as of Sun Feb 23 18:34:40 UTC 2014

  System load:  0.08              Processes:           60
  Usage of /:   5.5% of 19.68GB   Users logged in:     0
  Memory usage: 6%                IP address for eth0: 162.242.144.193
  Swap usage:   0%                IP address for eth1: 10.208.135.114

  Graph this data and manage this system at https://landscape.canonical.com/

Last login: Sun Feb 23 18:33:02 2014 from cpe-24-27-47-27.austin.res.rr.com
root@deis-base-image:~# useradd --comment &#39;deis ops user&#39; --home-dir &#39;/home/deis-ops&#39; \
  --shell &#39;/bin/bash&#39; --create-home deis-ops
root@deis-base-image:~# mkdir -p /home/deis-ops/.ssh &amp;amp;&amp;amp; \
   cp /root/.ssh/authorized_keys /home/deis-ops/.ssh/authorized_keys &amp;amp;&amp;amp; \
  chown -R deis-ops:deis-ops /home/deis-ops &amp;amp;&amp;amp; \
  chmod 0700 /home/deis-ops/.ssh &amp;amp;&amp;amp; \
  chmod 0600 /home/deis-ops/.ssh/authorized_keys &amp;amp;&amp;amp; \
  echo &#39;deis-ops ALL=(ALL) NOPASSWD:ALL&#39; &amp;gt; /etc/sudoers.d/deis-ops &amp;amp;&amp;amp; \
  chmod 0440 /etc/sudoers.d/deis-ops
root@deis-base-image:~# exit
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check that you can log in with these new creds:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh deis-ops@$DEIS_IP
deis$ sudo bash
root@deis$ exit
deis$ exit
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;finish-preparing-node-image:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Finish preparing node image&lt;/h3&gt;

&lt;p&gt;Next we&amp;rsquo;re going to update the kernel and prepare the base node image.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh deis-ops@$DEIS_IP &#39;sudo apt-get update&#39;
$ scp contrib/rackspace/*.sh deis-ops@$DEIS_IP:~/
$ ssh deis-ops@$DEIS_IP &#39;sudo ~/prepare-node-image.sh&#39;
$ ssh deis-ops@$DEIS_IP &#39;sudo apt-get install -yq linux-image-generic-lts-raring linux-headers-generic-lts-raring&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;create-an-image-from-this-server:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Create an image from this server&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ nova image-create deis-base-image deis-node-image
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After a few minutes you should see this response to running &lt;code&gt;nova image-list&lt;/code&gt;, if you&amp;rsquo;re impatient like me wrap your command with a &lt;code&gt;watch&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ watch &#39;nova image-list | grep deis&#39;
| df958d26-6515-4dd9-a449-920e74ea93a2 | deis-base-image                                              | ACTIVE | 0fc7f68b-176d-49a9-82ff-2d5893d32acd |

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once the image is active we can move onto the next steps.&lt;/p&gt;

&lt;h3 id=&#34;prepare-controller-image:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Prepare controller image&lt;/h3&gt;

&lt;p&gt;Next we want to prepare the VM for the controller image:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh deis-ops@$DEIS_IP &#39;sudo ~/prepare-controller-image.sh&#39;
$ ssh deis-ops@$DEIS_IP &#39;sudo apt-get install -yq linux-image-generic-lts-raring linux-headers-generic-lts-raring&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;create-an-image-from-this-server-1:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Create an image from this server&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ nova image-create deis-base-image deis-base-image
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After a few minutes you should see this response to running &lt;code&gt;nova image-list&lt;/code&gt;, if you&amp;rsquo;re impatient like me wrap your command with a &lt;code&gt;watch&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ watch &#39;nova image-list | grep deis-node&#39;
| f2236fa6-1e2d-4746-ac87-a3dd6b2de811 | deis-node-image                                              | ACTIVE | 633d5d88-54b3-463c-80fe-c119f4eb33a3 |

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;delete-the-instance:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Delete the instance&lt;/h3&gt;

&lt;p&gt;No need to keep the instance around and keep paying for it once you have the image:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec knife rackspace server list | grep deis  
42899699-68e7-4785-9f49-e0050f86249a  deis-base-image  162.242.144.193  10.208.135.114  performance1-1  80fbcb55-b206-41f9-9bc2-2dd7aac6c061  active
$ bundle exec knife rackspace server delete 42899699-68e7-4785-9f49-e0050f86249a --purge
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;create-the-deis-controller-server:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Create the Deis Controller server&lt;/h2&gt;

&lt;h3 id=&#34;launch-the-server:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Launch the Server&lt;/h3&gt;

&lt;p&gt;Launch the server from the image you created earlier:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ nova image-list | grep  deis-base-image
| a58c9895-6349-442a-bba7-99611900209d | deis-base-image
$ knife rackspace server create \
  --image a58c9895-6349-442a-bba7-99611900209d \
  --rackspace-metadata &amp;quot;{\&amp;quot;Name\&amp;quot;: \&amp;quot;deis-controller\&amp;quot;}&amp;quot; \
  --rackspace-disk-config MANUAL \
  --server-name deis-controller \
  --node-name deis-controller \
  --flavor &#39;performance1-2&#39;
Instance ID: bb713170-9322-424a-8837-863a4b396705
Name: deis-controller
Flavor: 2 GB Performance
Image: deis-base-image
...
Public IP Address: 23.253.104.13
Private IP Address: 10.208.132.190
Password: CQwDU4m97nvF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Take note of the &lt;code&gt;Instance ID&lt;/code&gt; and &lt;code&gt;Public IP Address&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If you have an easy to manage domain add an A record for &lt;code&gt;deis&lt;/code&gt; to it for the Public IP address.  If not
add an entry to your hosts file ( or do both! I did ):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo sh -c &amp;quot;echo &#39;&amp;lt;IP_OF_SERVER&amp;gt; deis&#39; &amp;gt;&amp;gt; /etc/hosts&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;modify-chef-admin-group:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Modify Chef Admin Group&lt;/h3&gt;

&lt;p&gt;On the Chef management website click (&lt;a href=&#34;https://manage.opscode.com/groups/admins/edit)[Groups&#34;&gt;https://manage.opscode.com/groups/admins/edit)[Groups&lt;/a&gt;] and add the &lt;code&gt;deis-controller&lt;/code&gt; client and your validator client to the &lt;code&gt;admins&lt;/code&gt; group.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh5.googleusercontent.com/-oSqB1Tdnn4c/UwpioPAXpJI/AAAAAAAANz4/xa8BdmRuTzQ/w579-h580-no/chef-admins.png&#34; alt=&#34;chef admins group&#34; /&gt;
&lt;/p&gt;

&lt;h3 id=&#34;converge-the-deis-controller-server:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Converge the Deis Controller Server&lt;/h3&gt;

&lt;p&gt;Edit the &lt;code&gt;deis-controller&lt;/code&gt; node via this command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ EDITOR=vi knife node edit deis-controller
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;make it look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;name&amp;quot;: &amp;quot;deis-controller&amp;quot;,
  &amp;quot;chef_environment&amp;quot;: &amp;quot;_default&amp;quot;,
  &amp;quot;normal&amp;quot;: {
    &amp;quot;tags&amp;quot;: [

    ]
  },
  &amp;quot;run_list&amp;quot;: [
    &amp;quot;recipe[deis::controller]&amp;quot;
  ]
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;then converge the node by running chef client on it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh deis-ops@deis sudo chef-client
[2014-02-23T19:25:32+00:00] INFO: Forking chef instance to converge...
[2014-02-23T19:25:32+00:00] INFO: *** Chef 11.6.2 ***
[2014-02-23T19:25:33+00:00] INFO: Run List is [recipe[deis::controller]]
[2014-02-23T19:25:33+00:00] INFO: Run List expands to [deis::controller]
[2014-02-23T19:25:33+00:00] INFO: Starting Chef Run for deis-controller
[2014-02-23T19:25:33+00:00] INFO: Running start handlers
[2014-02-23T19:25:33+00:00] INFO: Start handlers complete.
...
$
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;testing-deis:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Testing Deis&lt;/h2&gt;

&lt;h3 id=&#34;install-the-deis-client-with-pip:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Install the Deis Client with pip&lt;/h3&gt;

&lt;p&gt;The Deis client is written in python and can be installed by &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo pip install deis  
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;register-admin-user:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Register Admin User&lt;/h3&gt;

&lt;p&gt;First user to register becomes the Admin:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ deis register http://deis:8000
username: admin
password: 
password (confirm): 
email: admin@example.com
Registered admin
Logged in as admin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Push your public key to deis:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ deis keys:add ~/.ssh/id_rsa.pub 
Uploading SSH_KEY to Deis...done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;check the web server is serving content by browsing to (&lt;a href=&#34;http://deis)[http://deis&#34;&gt;http://deis)[http://deis&lt;/a&gt;] and entering your admin credentials.&lt;/p&gt;

&lt;h3 id=&#34;teach-deis-your-provider-credentials:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Teach Deis your provider credentials&lt;/h3&gt;

&lt;p&gt;Deis will automatically provision worker nodes if you teach it your credentials.&lt;/p&gt;

&lt;p&gt;We already have our Rackspace credentials saved to &lt;code&gt;~/paas/.chef/openrc&lt;/code&gt; but Deis wants them named differently:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export RACKSPACE_USERNAME=$OS_USERNAME
$ export RACKSPACE_API_KEY=$OS_PASSWORD
$ deis providers:discover
No EC2 credentials discovered.
Discovered Rackspace credentials: ****************
Import Rackspace credentials? (y/n) : y
Uploading Rackspace credentials... done
No DigitalOcean credentials discovered.
No Vagrant VMs discovered.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;deploy-formations-layers:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Deploy Formations &amp;amp; Layers&lt;/h2&gt;

&lt;h3 id=&#34;formation:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Formation&lt;/h3&gt;

&lt;p&gt;Formations are collections of infrastructure for serving applications.   We&amp;rsquo;ll call our first Formation &lt;code&gt;dev&lt;/code&gt; for development.&lt;/p&gt;

&lt;p&gt;Create formation (using the wildcard domain from our cloud load balancer created earlier in the &lt;code&gt;--domain&lt;/code&gt; argument):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ deis formations:create dev --domain=50.56.167.26.xip.io
Creating formation... done, created dev
See `deis help layers:create` to begin building your formation
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;layers:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Layers&lt;/h3&gt;

&lt;p&gt;Layers are a heterogenerous collection of nodes that perform one of two function:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Proxy - Directs traffic to the appropriate container running the application.&lt;/li&gt;
&lt;li&gt;Runtime - Runs the containers that hold the applications.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We&amp;rsquo;re going to create a layer called &lt;code&gt;nodes&lt;/code&gt; that will perform both the proxy and runtime functions:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ deis layers:create dev nodes rackspace-dfw --proxy=y --runtime=y
Creating nodes layer... done in 4s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;note&lt;/em&gt; There&amp;rsquo;s currently a &lt;a href=&#34;https://github.com/opdemand/deis/issues/541&#34;&gt;bug&lt;/a&gt; that causes the first creation of a layer to fail.  if that happens run the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;deis formations:create dev --domain=50.56.167.26.xip.io
Creating formation... done, created dev

See `deis help layers:create` to begin building your formation
$ deis layers:create dev nodes rackspace-dfw --proxy=y --runtime=y
Creating nodes layer... 500 INTERNAL SERVER ERROR
&amp;lt;h1&amp;gt;Server Error (500)&amp;lt;/h1&amp;gt;
$ deis layers:destroy dev nodes
Destroying nodes layer... done in 0s
$ deis layers:create dev nodes rackspace-dfw --proxy=y --runtime=y
Creating nodes layer... done in 2s

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;build-nodes:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Build Nodes&lt;/h3&gt;

&lt;p&gt;Next we tell deis to spin up two Cloud Servers which will become members of the &lt;code&gt;nodes&lt;/code&gt; layer.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ deis nodes:scale dev nodes=2
Scaling nodes... but first, coffee!
done in 345s
Use `deis create --formation=dev` to create an application
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This can sometimes take longer than the &lt;code&gt;deis&lt;/code&gt; cli timeout.   Don&amp;rsquo;t fear,  just wait a bit longer, this could be a great time to explore the &lt;code&gt;deis&lt;/code&gt; cli by running &lt;code&gt;deis help&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;update-cloud-load-balancer:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Update Cloud Load Balancer&lt;/h2&gt;

&lt;p&gt;Add these two nodes to the (&lt;a href=&#34;https://mycloud.rackspace.com/load_balancers)[Cloud&#34;&gt;https://mycloud.rackspace.com/load_balancers)[Cloud&lt;/a&gt; Load Balancer] we created earlier.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh6.googleusercontent.com/-yaJfxoyDk4M/UwpioEndiOI/AAAAAAAANz0/aXannmisdbE/w903-h407-no/cloud-servers-list.png&#34; alt=&#34;cloud server list&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;This is simple to do through the GUI:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Click on your load balancer and under &lt;code&gt;Nodes&lt;/code&gt; click the &lt;code&gt;Add Cloud Servers&lt;/code&gt; button.&lt;/li&gt;
&lt;li&gt;Check the box beside the two &lt;code&gt;dev-nodes&lt;/code&gt; servers and click &lt;code&gt;Add Selected Servers&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://lh6.googleusercontent.com/-zm6sB7l7YVk/Uwpin4BNJPI/AAAAAAAANzw/b-_J2ieyIuE/w773-h476-no/cloud-lb-nodes.png&#34; alt=&#34;cloud lb servers&#34; /&gt;
&lt;/p&gt;

&lt;h2 id=&#34;deploy-an-application:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Deploy an Application&lt;/h2&gt;

&lt;p&gt;So great, you have a PaaS, but what do you do now?  Deploy some apps of course!&lt;/p&gt;

&lt;h3 id=&#34;nodejs-example-app:59de91c1dc1797a6214d8b570db9750b&#34;&gt;NodeJS Example App&lt;/h3&gt;

&lt;p&gt;Download the NodeJS example application so like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mkdir -p ~/paas/apps
$ cd ~paas/apps
$ git clone https://github.com/opdemand/example-nodejs-express.git
$ cd example-nodejs-express
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;create-an-application-in-deis:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Create an Application in Deis&lt;/h3&gt;

&lt;p&gt;Use the Deis command line tool to create a new application:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ deis create      
Creating application... done, created exotic-sandwich
Git remote deis added
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;push-your-application-to-deis:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Push your Application to Deis&lt;/h3&gt;

&lt;p&gt;This will push, deploy and Launch the app.  The first one will take some time as deis has to download some docker images,  subsequent apps will be much faster:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git push deis master                     
git push deis master
Counting objects: 184, done.
Delta compression using up to 4 threads.
Compressing objects: 100% (89/89), done.
Writing objects: 100% (184/184), 28.77 KiB | 0 bytes/s, done.
Total 184 (delta 103), reused 165 (delta 92)
-----&amp;gt; Node.js app detected
-----&amp;gt; Requested node range: 0.10.x
-----&amp;gt; Resolved node version: 0.10.26
-----&amp;gt; Downloading and installing node
-----&amp;gt; Installing dependencies
       npm WARN package.json example-nodejs-express@0.0.1 No repository field.
       npm http GET https://registry.npmjs.org/express
       npm http 200 https://registry.npmjs.org/express
...
-----&amp;gt; Caching node_modules directory for future builds
-----&amp;gt; Cleaning up node-gyp and npm artifacts
-----&amp;gt; Building runtime environment
-----&amp;gt; Discovering process types
       Procfile declares types -&amp;gt; web
-----&amp;gt; Compiled slug size is 5.5M
-----&amp;gt; Building Docker image
Uploading context 5.698 MB
Uploading context 
Step 0 : FROM deis/slugrunner
 ---&amp;gt; bb0a27915014
Step 1 : RUN mkdir -p /app
 ---&amp;gt; Running in 1ae5cdeaad9a
 ---&amp;gt; 6e6467466d48
Step 2 : ADD slug.tgz /app
 ---&amp;gt; 191a4345b1e4
Step 3 : ENTRYPOINT [&amp;quot;/runner/init&amp;quot;]
 ---&amp;gt; Running in d322512d5865
 ---&amp;gt; 2866cf3e37c9
Successfully built 2866cf3e37c9
-----&amp;gt; Pushing image to private registry
       Launching... done, v2

-----&amp;gt; exotic-sandwich deployed to Deis
       http://exotic-sandwich.50.56.167.26.xip.io

       To learn more, use `deis help` or visit http://deis.io

To ssh://git@deis:2222/exotic-sandwich.git
 * [new branch]      master -&amp;gt; master

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;did-it-work:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Did it work ?&lt;/h2&gt;

&lt;p&gt;Open your web browser to the URL in the output of the previous command.  In my case this was &lt;code&gt;http://exotic-sandwich.50.56.167.26.xip.io&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If everything worked the text in the browser window should read &lt;code&gt;Powered by Deis&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh6.googleusercontent.com/-cxuysxM_oM8/UwpipfiKFMI/AAAAAAAAN0U/M7T9dC6xJ-E/w446-h171-no/deis-app-1.png&#34; alt=&#34;deis app&#34; /&gt;
&lt;/p&gt;

&lt;h2 id=&#34;configure-and-scale-your-application:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Configure and Scale your application&lt;/h2&gt;

&lt;p&gt;We can set config parameters for our apps by running &lt;code&gt;deis config&lt;/code&gt;.   The example app we&amp;rsquo;re using has a config paramater &amp;lsquo;POWERED_BY&amp;rsquo; so we can set that by running:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ deis config:set POWERED_BY=&#39;DEIS and Rackspace&#39;
=== exotic-sandwich
POWERED_BY: DEIS and Rackspace
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://lh6.googleusercontent.com/-J5AcNytZLOQ/UwpipEdpeBI/AAAAAAAAN0E/WXWC08rxsBU/w507-h157-no/deis-app-2.png&#34; alt=&#34;deis app2&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;Expecting visitors?  Let&amp;rsquo;s scale your app to 5 nodes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ deis scale web=5
Scaling containers... but first, coffee!
done in 54s

=== exotic-sandwich Containers

--- web: `node server.js`
web.1 up 2014-02-23T20:22:07.241Z (dev-nodes-2)
web.2 up 2014-02-23T20:28:21.778Z (dev-nodes-1)
web.3 up 2014-02-23T20:28:21.788Z (dev-nodes-2)
web.4 up 2014-02-23T20:28:21.799Z (dev-nodes-1)
web.5 up 2014-02-23T20:28:21.810Z (dev-nodes-2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can see what your app is doing by running &lt;code&gt;deis info&lt;/code&gt; and &lt;code&gt;deis logs&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ deis info
=== exotic-sandwich Application
{
  &amp;quot;updated&amp;quot;: &amp;quot;2014-02-23T20:28:21.812Z&amp;quot;, 
  &amp;quot;uuid&amp;quot;: &amp;quot;ef618db6-f5a8-4cab-a7d9-d01e78036e3a&amp;quot;, 
  &amp;quot;created&amp;quot;: &amp;quot;2014-02-23T20:16:51.931Z&amp;quot;, 
  &amp;quot;formation&amp;quot;: &amp;quot;dev&amp;quot;, 
  &amp;quot;owner&amp;quot;: &amp;quot;admin&amp;quot;, 
  &amp;quot;id&amp;quot;: &amp;quot;exotic-sandwich&amp;quot;, 
  &amp;quot;containers&amp;quot;: &amp;quot;{\&amp;quot;web\&amp;quot;: 5}&amp;quot;
}

=== exotic-sandwich Containers

--- web: `node server.js`
web.1 up 2014-02-23T20:22:07.241Z (dev-nodes-2)
web.2 up 2014-02-23T20:28:21.778Z (dev-nodes-1)
web.3 up 2014-02-23T20:28:21.788Z (dev-nodes-2)
web.4 up 2014-02-23T20:28:21.799Z (dev-nodes-1)
web.5 up 2014-02-23T20:28:21.810Z (dev-nodes-2)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;$ deis logs
Feb 23 20:22:57 dev-nodes exotic-sandwich[web.1]: Server listening on port 10001 in development mode
Feb 23 20:25:38 dev-nodes exotic-sandwich[web.1]: Server listening on port 10001 in development mode
Feb 23 20:26:49 dev-nodes exotic-sandwich[web.1]: Server listening on port 10001 in development mode
Feb 23 20:28:28 dev-nodes exotic-sandwich[web.3]: Server listening on port 10003 in development mode
Feb 23 20:28:29 dev-nodes exotic-sandwich[web.5]: Server listening on port 10005 in development mode
Feb 23 20:29:11 dev-nodes exotic-sandwich[web.2]: Server listening on port 10002 in development mode
Feb 23 20:29:12 dev-nodes exotic-sandwich[web.4]: Server listening on port 10004 in development mode
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Congratulations!  You&amp;rsquo;ve successfully built out your own cost effective PAAS and deployed your first application to it.&lt;/p&gt;

&lt;p&gt;Speaking of costs &amp;hellip;  How much would this cost to run per month ?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Cloud Load Balancer - $10.95 / month&lt;/li&gt;
&lt;li&gt;Deis Controller - $57.60 / month&lt;/li&gt;
&lt;li&gt;Deis Nodes (x2) - $115.20 / month&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Total:  $183.75 / month.&lt;/p&gt;

&lt;p&gt;You could run all of this on a single server without a load balancer,  which means it would be just $57.60/month, which with the &lt;a href=&#34;http://developer.rackspace.com/devtrial/&#34;&gt;Rackspace Developer Discount&lt;/a&gt; would reduce down to just $7.60/month.&lt;/p&gt;

&lt;h1 id=&#34;epilogue:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Epilogue&lt;/h1&gt;

&lt;h2 id=&#34;cleanup:59de91c1dc1797a6214d8b570db9750b&#34;&gt;Cleanup&lt;/h2&gt;

&lt;p&gt;Destroy your app:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ deis destroy

 !    WARNING: Potentially Destructive Action
 !    This command will destroy the application: exotic-sandwich
 !    To proceed, type &amp;quot;exotic-sandwich&amp;quot; or re-run this command with --confirm=exotic-sandwich

&amp;gt; exotic-sandwich
Destroying exotic-sandwich... done in 21s
Git remote deis removed
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;list your servers:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec knife rackspace server list
Instance ID                           Name             Public IP       Private IP      Flavor          Image                                 State 
7c43ecb9-1ba3-454c-a5f4-637b56961d68  dev-nodes        23.253.102.184  10.208.135.137  performance1-2  2d59cbce-92fa-412b-8a5e-6eb426ce7dc9  active
f89c4b25-6486-422a-907a-16b3b3223a5e  dev-nodes        23.253.102.158  10.208.137.18   performance1-2  2d59cbce-92fa-412b-8a5e-6eb426ce7dc9  active
bb713170-9322-424a-8837-863a4b396705  deis-controller  23.253.104.13   10.208.132.190  performance1-2  a58c9895-6349-442a-bba7-99611900209d  active
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Delete your servers by running the following command for each:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec knife rackspace server delete 7c43ecb9-1ba3-454c-a5f4-637b56961d68 --purge
Instance ID: 7c43ecb9-1ba3-454c-a5f4-637b56961d68
Host ID: e0da0172f321babe99aec9686c7b99ac7fa5ff8fa1ada934f5fae842
Name: dev-nodes
Flavor: 2 GB Performance
Image: deis-node-image
Public IP Address: 23.253.102.184
Private IP Address: 10.208.135.137

Do you really want to delete this server? (Y/N) y
[WARNING] Error Parsing response json - Yajl::ParseError
WARNING: Deleted server 7c43ecb9-1ba3-454c-a5f4-637b56961d68
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Clean up your chef:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec knife data bag delete deis-apps
$ bundle exec knife data bag delete deis-formations
$ bundle exec knife client delete dev-nodes-1
$ bundle exec knife client delete dev-nodes-2
$ bundle exec knife node delete dev-nodes-1
$ bundle exec knife node delete dev-nodes-2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Delete your glance images:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ nova image-delete deis-base-image
$ nova image-delete deis-node-image
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally delete your Cloud Load Balancer from the &lt;a href=&#34;https://mycloud.rackspace.com/load_balancers&#34;&gt;Rackspace UI&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Managing docker services with this one easy trick</title>
      <link>http://tech.paulcz.net/2013/10/managing-docker-services-with-this-one-easy-trick/</link>
      <pubDate>Sun, 20 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>http://tech.paulcz.net/2013/10/managing-docker-services-with-this-one-easy-trick/</guid>
      <description>

&lt;p&gt;I have been having a lot of internal debate about the idea of running more than one service in a docker container.   A Docker container is built to run a single process in the foreground and to live for only as long as that process is running.  This is great in a utopian world where servers are immutable and sysadmins drink tiki drinks on the beach,  however it doesn&amp;rsquo;t always translate well to the real world.&lt;/p&gt;

&lt;p&gt;Examples where you might want to be able to run multiple servers span from the simple use case of running &lt;code&gt;sshd&lt;/code&gt; as well as your application to running a web app such as &lt;code&gt;wordpress&lt;/code&gt; where you might want both &lt;code&gt;apache&lt;/code&gt; and &lt;code&gt;mysql&lt;/code&gt; running in the same container.&lt;/p&gt;

&lt;p&gt;Wrapping your applications in a supervisor daemon such as &lt;code&gt;runit&lt;/code&gt; seems like a perfect fit for this.  All you need to do is install &lt;code&gt;runit&lt;/code&gt; as part of your &lt;code&gt;dockerfile&lt;/code&gt; and then create appropriate service directories for the apps you want to run in the container.    I was doing some testing of this when I realized a quirk of &lt;code&gt;runit&lt;/code&gt; which I could exploit for evil.&lt;/p&gt;

&lt;p&gt;To start or stop a service with &lt;code&gt;runit&lt;/code&gt; is simply a matter of creating or deleting a symlink in a service directory,   so in theory if you could expose that directory to the server hosting the container you could exploit that to start and stop services from outside of the container.  &lt;code&gt;Docker&lt;/code&gt; volume mapping allows exactly this!&lt;/p&gt;

&lt;p&gt;Below you will find examples of running three services (logstash,elasticsearch,kibana) that make up the &lt;code&gt;logstash&lt;/code&gt; suite.&lt;/p&gt;

&lt;h2 id=&#34;start-by-cloning-the-demo-git-repository-and-run-demo-sh:dab52676160fc987cea1712f60e5a17c&#34;&gt;Start by cloning the demo git repository and run demo.sh&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/paulczar/docker-runit-demo.git
$ cd docker-runit-demo
$ ./demo.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;demo-sh-script:dab52676160fc987cea1712f60e5a17c&#34;&gt;demo.sh script&lt;/h3&gt;

&lt;h4 id=&#34;step-1-build-the-container:dab52676160fc987cea1712f60e5a17c&#34;&gt;Step 1:  Build the container&lt;/h4&gt;

&lt;p&gt;The script uses the below &lt;code&gt;Dockerfile&lt;/code&gt; to build the base container that we&amp;rsquo;ll be running.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Installs runit for service management
#
# Author: Paul Czarkowski
# Date: 10/20/2013

FROM paulczar/jre7
MAINTAINER Paul Czarkowski &amp;quot;paul@paulcz.net&amp;quot;

RUN apt-get update

RUN apt-get -y install curl wget git nginx
RUN apt-get -y install runit || echo

CMD [&amp;quot;/usr/sbin/runsvdir-start&amp;quot;]

&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;step-2-install-the-applications:dab52676160fc987cea1712f60e5a17c&#34;&gt;Step 2: Install the applications&lt;/h4&gt;

&lt;p&gt;This will take a few minutes the first time as it needs to download &lt;code&gt;logstash&lt;/code&gt;, &lt;code&gt;kibana&lt;/code&gt;, and &lt;code&gt;elasticsearch&lt;/code&gt; and stage them in a local &lt;code&gt;./opt&lt;/code&gt;directory.&lt;/p&gt;

&lt;h4 id=&#34;step-3-start-the-docker-container:dab52676160fc987cea1712f60e5a17c&#34;&gt;Step 3: Start the Docker container&lt;/h4&gt;

&lt;p&gt;Starts the &lt;code&gt;Docker&lt;/code&gt; container with the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -d -p 8080:80 -p 5014:514 -p 9200:9200 \
  -v $BASE/opt:/opt \
  -v $BASE/sv:/etc/sv \
  -v $BASE/init:/etc/init \
  -v $BASE/service:/etc/service \
  demo/runit
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The container should be up and running&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker ps
ID                  IMAGE               COMMAND                CREATED             STATUS              PORTS
eb495ad92ba0        demo/runit:latest   /usr/sbin/runsvdir-s   4 seconds ago       Up 3 seconds        5014-&amp;gt;514, 8080-&amp;gt;80, 9200-&amp;gt;9200   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However there aren&amp;rsquo;t any services running!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl localhost:8080
curl: (56) Recv failure: Connection reset by peer
$ curl localhost:9200
curl: (56) Recv failure: Connection reset by peer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can start the services with the following commands&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd service
$ ln -s ../sv/elasticsearch
$ ln -s ../sv/logstash
$ ln -s ../sv/kibana
cd ..
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now see the services are running, test the ports and send some data to logstash.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl localhost:8080      
&amp;lt;!DOCTYPE html&amp;gt;&amp;lt;!--[if IE 8]&amp;gt;&amp;lt;html class=&amp;quot;no-js lt-ie9&amp;quot; lang=&amp;quot;en&amp;quot;&amp;gt;&amp;lt;![endif]--&amp;gt;&amp;lt;!--[if gt IE 8]&amp;gt;&amp;lt;!--&amp;gt;&amp;lt;html class=&amp;quot;no-js&amp;quot; lang=&amp;quot;en&amp;quot;&amp;gt;
...
curl localhost:9200
{
  &amp;quot;ok&amp;quot; : true,
  &amp;quot;status&amp;quot; : 200,
...
$tail -100 /var/log/syslog | nc localhost 5014
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Stop a service ?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ rm service/elasticsearch
$ rm service/logstash
$ rm service/kibana
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;bonus-round-logs:dab52676160fc987cea1712f60e5a17c&#34;&gt;Bonus Round: Logs!&lt;/h2&gt;

&lt;p&gt;The beautify of doing this is that we&amp;rsquo;re actually logging the application output to a mounted volume.   This means we now have access to their logs from the host machine.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ tail opt/logstash/logs/current
$ tail opt/elasticsearch-0.90.5/logs/current
$ tail opt/kibana/logs/access.log
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;cleanup:dab52676160fc987cea1712f60e5a17c&#34;&gt;Cleanup&lt;/h2&gt;

&lt;p&gt;Unfortunately any files created inside the docker instance are owned by root ( an artifact of docker daemon running as root ).   If you&amp;rsquo;re in The following script will clean out any such files after you&amp;rsquo;ve stopped the docker container.&lt;/p&gt;

&lt;p&gt;It will delete any files/dirs inside your current directory that are owned by root.  Obviously it can be very dangerous to run &amp;hellip; so be careful where you run it from!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo find . -uid 0   -exec rm -rfv {} \;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Creating immutable servers with chef and docker.io</title>
      <link>http://tech.paulcz.net/2013/09/creating-immutable-servers-with-chef-and-docker-dot-io/</link>
      <pubDate>Sat, 07 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>http://tech.paulcz.net/2013/09/creating-immutable-servers-with-chef-and-docker-dot-io/</guid>
      <description>

&lt;p&gt;Building applications in a &lt;a href=&#34;http://docker.io&#34;&gt;docker.io&lt;/a&gt; Dockerfile is relatively simple,  but sometimes you want to just install the application exactly as you would normally via already built chef cookbooks.   Turns out this is actually pretty simple.&lt;/p&gt;

&lt;p&gt;The first thing you&amp;rsquo;ll need to do is build a container with chef-client and berkshelf installed.   You can grab the one I&amp;rsquo;ve built by running &lt;code&gt;docker pull paulczar/chef-solo&lt;/code&gt; or build one youself from a &lt;code&gt;Dockerfile&lt;/code&gt; that looks a little something like the following&amp;hellip;&lt;/p&gt;

&lt;h3 id=&#34;creating-a-docker-io-container-with-chef-and-berkshelf:8e5e7d2d9bff571e6328148c619ee8a0&#34;&gt;Creating a docker.io container with chef and berkshelf&lt;/h3&gt;

&lt;p&gt;``` ruby Dockerfile&lt;/p&gt;

&lt;h1 id=&#34;docker-version-0-5-3:8e5e7d2d9bff571e6328148c619ee8a0&#34;&gt;DOCKER-VERSION 0.5.3&lt;/h1&gt;

&lt;p&gt;FROM ubuntu:12.10
MAINTAINER Paul Czarkowski &amp;ldquo;paul@paulcz.net&amp;rdquo;&lt;/p&gt;

&lt;p&gt;RUN apt-get -y update
RUN apt-get -y install curl build-essential libxml2-dev libxslt-dev git
RUN curl -L &lt;a href=&#34;https://www.opscode.com/chef/install.sh&#34;&gt;https://www.opscode.com/chef/install.sh&lt;/a&gt; | bash
RUN echo &amp;ldquo;gem: &amp;ndash;no-ri &amp;ndash;no-rdoc&amp;rdquo; &amp;gt; ~/.gemrc
RUN /opt/chef/embedded/bin/gem install berkshelf&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
_you&#39;ll notice I&#39;m using the embedded chef ruby to install the berkshelf gem,  this is a handy shortcut to avoid messing around with random ruby versions from your distributions packaging._

run `$ docker build -t paulczar/chef-solo .` to build a usable docker container from the above `Dockerfile`.

### Using chef-solo and berkshelf to build an application in a docker.io container ###

My [example application](https://github.com/paulczar/docker-chef-solo) will install `Kibana3` to your docker container.   I&#39;ll step through how it works below.

#### Chef-Solo ####

To run `chef-solo` successfully we require two files.   `solo.rb` to set up file locations, and `solo.json&#39; to set up the json / run list required for your application.

``` ruby chef.rb
root = File.absolute_path(File.dirname(__FILE__))

file_cache_path root
cookbook_path root + &#39;/cookbooks&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;``` json chef.json
{
  &amp;ldquo;kibana&amp;rdquo;: {
    &amp;ldquo;webserver_listen&amp;rdquo;: &amp;ldquo;0.0.0.0&amp;rdquo;
  },
  &amp;ldquo;run_list&amp;rdquo;: [
    &amp;ldquo;recipe[kibana::default]&amp;rdquo;
  ]
}&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
#### Berkshelf ####

To run `berkshelf` we need to build a Berksfile which contains a list of all the chef cookbooks required for the applocation.   Berkshelf will download these cookbooks to a local directory which will be usable by chef-solo.

``` ruby Berksfile
site :opscode

cookbook &#39;build-essential&#39;
cookbook &#39;apache2&#39;
cookbook &#39;git&#39;
cookbook &#39;kibana&#39;, github: &#39;lusis/chef-kibana&#39;
cookbook &#39;nginx&#39; , github: &#39;opscode-cookbooks/nginx&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;You can see some of the cookbooks are being pulled from the opscode repository,  whereas others are being pulled directly from github.&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&#34;dockerfile:8e5e7d2d9bff571e6328148c619ee8a0&#34;&gt;Dockerfile&lt;/h4&gt;

&lt;p&gt;All that&amp;rsquo;s left now is to create a Dockerfile that will bring it all together.&lt;/p&gt;

&lt;p&gt;``` ruby Dockerfile&lt;/p&gt;

&lt;h1 id=&#34;docker-version-0-5-3-1:8e5e7d2d9bff571e6328148c619ee8a0&#34;&gt;DOCKER-VERSION 0.5.3&lt;/h1&gt;

&lt;p&gt;FROM paulczar/chef-client
MAINTAINER Paul Czarkowski &amp;ldquo;paul@paulcz.net&amp;rdquo;&lt;/p&gt;

&lt;p&gt;RUN apt-get -y update
ADD . /chef
RUN cd /chef &amp;amp;&amp;amp; /opt/chef/embedded/bin/berks install &amp;ndash;path /chef/cookbooks
RUN chef-solo -c /chef/solo.rb -j /chef/solo.json
RUN echo &amp;ldquo;daemon off;&amp;rdquo; &amp;gt;&amp;gt; /etc/nginx/nginx.conf&lt;/p&gt;

&lt;p&gt;CMD [&amp;ldquo;nginx&amp;rdquo;]
```&lt;/p&gt;

&lt;p&gt;Run &lt;code&gt;$ docker build -t demo/kibana3 .&lt;/code&gt; to build your application.&lt;/p&gt;

&lt;p&gt;It will add the local files ( &lt;code&gt;solo.rb&lt;/code&gt;, &lt;code&gt;solo.json&lt;/code&gt;, &lt;code&gt;Berksfile&lt;/code&gt; ) to /chef in the server and then call berkshelf to download the cookbooks and chef-solo to install your application.   Finally it will give &lt;code&gt;nginx&lt;/code&gt; a directive to run in the foreground so that we don&amp;rsquo;t have to do any sneaky prcess control to get it to work with the way &lt;code&gt;docker.io&lt;/code&gt; runs processes.&lt;/p&gt;

&lt;p&gt;To run the resultant &lt;code&gt;docker.io&lt;/code&gt; container you simply need to run &lt;code&gt;$ docker run -d -p 80 demo/kibana3&lt;/code&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>