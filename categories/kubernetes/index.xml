<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kubernetes on Paul Czarkowski</title>
    <link>https://tech.paulcz.net/categories/kubernetes/</link>
    <description>Recent content in kubernetes on Paul Czarkowski</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 22 Feb 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://tech.paulcz.net/categories/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Creating Self Signed Certificates on Kubernetes</title>
      <link>https://tech.paulcz.net/blog/creating-self-signed-certs-on-kubernetes/</link>
      <pubDate>Sat, 22 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/creating-self-signed-certs-on-kubernetes/</guid>
      <description>Welcome to 2020. Creating self signed TLS certificates is still hard. Five (5) years ago I created a project on github called omgwtfssl which is a fairly simple bash script wrapping a bunch of openssl commands to create certificates.
I&amp;rsquo;ve been using it ever since and kind of forgot about the pain of creating certificates.
Skip the words and jump to the examples Creating self signed certificates with cert-manager, Creating multiple certificates from the same self signed CA with cert-manager.</description>
      <content>

&lt;p&gt;Welcome to 2020. Creating self signed TLS certificates is still hard. Five (5) years ago I created a project on github called &lt;a href=&#34;https://github.com/paulczar/omgwtfssl&#34;&gt;omgwtfssl&lt;/a&gt; which is a fairly simple bash script wrapping a bunch of &lt;code&gt;openssl&lt;/code&gt; commands to create certificates.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve been using it ever since and kind of forgot about the pain of creating certificates.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Skip the words and jump to the examples &lt;a href=&#34;#creating-self-signed-certificates-with-cert-manager&#34;&gt;Creating self signed certificates with cert-manager&lt;/a&gt;, &lt;a href=&#34;#creating-multiple-certificates-from-the-same-self-signed-ca-with-cert-manager&#34;&gt;Creating multiple certificates from the same self signed CA with cert-manager&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;With the advent of &lt;a href=&#34;https://letsencrypt.org/&#34;&gt;letsencrypt&lt;/a&gt; and later the Kubernetes &lt;a href=&#34;https://cert-manager.io/&#34;&gt;cert-manager&lt;/a&gt; controller we can make real signed certificates with a quick flourish of some &lt;strong&gt;YAML&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve been happily chugging along with this combination of &lt;code&gt;cert-manager&lt;/code&gt; &lt;a href=&#34;https://cert-manager.io/&#34;&gt;cert-manager&lt;/a&gt; for real certificates, and &lt;a href=&#34;https://github.com/paulczar/omgwtfssl&#34;&gt;omgwtfssl&lt;/a&gt; for self signed (despite the fact that the name is &lt;a href=&#34;https://gitlab.com/gitlab-org/charts/gitlab/issues/584&#34;&gt;inappropriate and unprofessional.&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;We should try to find a replacement for omgwtfssl, which is currently used to generate self-signed certificates. The name is inappropriate and unprofessional.&amp;rdquo;  - &lt;a href=&#34;https://gitlab.com/gitlab-org/charts/gitlab/issues/584&#34;&gt;gitlab&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;As amusing as &lt;code&gt;docker run paulczar/omgwtfssl&lt;/code&gt; is to type (I giggle every time), its a bit weird to tell people to create certificates locally then add them to their Kubernetes manifests or Helm charts. So I finally decided to sit down and figure out how to create them sensibly with &lt;a href=&#34;https://cert-manager.io/&#34;&gt;cert-manager&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;create-a-kubernetes-in-docker-cluster&#34;&gt;Create a Kubernetes in Docker Cluster&lt;/h2&gt;

&lt;p&gt;You&amp;rsquo;ll need a Kubernetes cluster, we&amp;rsquo;re not doing anything too resource intensive so a &lt;a href=&#34;https://kind.sigs.k8s.io/docs/user/quick-start/&#34;&gt;kind&lt;/a&gt; cluster should be fine.&lt;/p&gt;

&lt;p&gt;Create &lt;a href=&#34;https://kind.sigs.k8s.io/docs/user/quick-start/&#34;&gt;kind&lt;/a&gt; cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kind create cluster
export KUBECONFIG=&amp;quot;$(kind get kubeconfig-path --name=&amp;quot;kind&amp;quot;)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Test the cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl cluster-info
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;creating-self-signed-certificates-with-cert-manager&#34;&gt;Creating self signed certificates with cert-manager&lt;/h2&gt;

&lt;p&gt;Install &lt;a href=&#34;https://cert-manager.io/&#34;&gt;cert-manager&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl create namespace cert-manager
kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.13.1/cert-manager.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;If you receive a validation error relating to the x-kubernetes-preserve-unknown-fields add &lt;code&gt;--validate&lt;/code&gt; to the above command and run again.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Create a namespace to work in:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl create namespace sandbox
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create an Issuer:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: you can create a ClusterIssuer instead if you want to be able to request certificates from any namespace.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl apply -n sandbox -f &amp;lt;(echo &amp;quot;
apiVersion: cert-manager.io/v1alpha2
kind: Issuer
metadata:
  name: selfsigned-issuer
spec:
  selfSigned: {}
&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a self signed certificate:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This creates a wildcard certificate that could be used for
  any services in the sandbox namespace.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl apply -n sandbox -f &amp;lt;(echo &#39;
apiVersion: cert-manager.io/v1alpha2
kind: Certificate
metadata:
  name: first-tls
spec:
  secretName: first-tls
  dnsNames:
  - &amp;quot;*.sandbox.svc.cluster.local&amp;quot;
  - &amp;quot;*.sandbox&amp;quot;
  issuerRef:
    name: selfsigned-issuer
&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Validate the secret is created&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Check the certificate resource:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl -n sandbox get certificate
  NAME        READY   SECRET      AGE
  first-tls   True    first-tls   9s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check the subsequent secret:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl -n sandbox get secret first-tls
NAME        TYPE                DATA   AGE
first-tls   kubernetes.io/tls   3      73s
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;This secret contains three keys &lt;code&gt;ca.crt&lt;/code&gt;, &lt;code&gt;tls.crt&lt;/code&gt;, &lt;code&gt;tls.key&lt;/code&gt;. You can run &lt;code&gt;kubectl -n sandbox get secret first-tls -o yaml&lt;/code&gt; to see the whole thing.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Test that the certificate is valid:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;openssl x509 -in &amp;lt;(kubectl -n sandbox get secret \
  first-tls -o jsonpath=&#39;{.data.tls\.crt}&#39; | base64 -d) \
  -text -noout
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;If you scan through the output you should find &lt;code&gt;X509v3 Subject Alternative Name: DNS:*.first.svc.cluster.local, DNS:*.first&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Congratulations. You&amp;rsquo;ve just created your first self signed certificate with Kubernetes. While it involves more typing than &lt;code&gt;docker run paulczar/omgwtfssl&lt;/code&gt; it is much more useful for Kubernetes enthusiasts to have the cluster generate them for you.&lt;/p&gt;

&lt;p&gt;However, what if you want to use TLS certificates signed by the same CA for performing client/server authentication? Never fear we can do that too.&lt;/p&gt;

&lt;h2 id=&#34;creating-multiple-certificates-from-the-same-self-signed-ca-with-cert-manager&#34;&gt;Creating multiple certificates from the same self signed CA with cert-manager&lt;/h2&gt;

&lt;p&gt;Install &lt;a href=&#34;https://cert-manager.io/&#34;&gt;cert-manager&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Skip this step if you already installed cert-manager from the first example.&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl create namespace cert-manager
kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.13.1/cert-manager.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;If you receive a validation error relating to the x-kubernetes-preserve-unknown-fields add &lt;code&gt;--validate&lt;/code&gt; to the above command and run again.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Create a namespace to work in:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl create namespace sandbox2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create an Issuer:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: you can create a ClusterIssuer instead if you want to be able to request certificates from any namespace.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl apply -n sandbox2 -f &amp;lt;(echo &amp;quot;
apiVersion: cert-manager.io/v1alpha2
kind: Issuer
metadata:
  name: selfsigned-issuer
spec:
  selfSigned: {}
&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a CA Certificate:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;note &lt;code&gt;isCA&lt;/code&gt; is set to true in the body of the &lt;code&gt;spec&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl apply -n sandbox2 -f &amp;lt;(echo &#39;
apiVersion: cert-manager.io/v1alpha2
kind: Certificate
metadata:
  name: sandbox2-ca
spec:
  secretName: sandbox2-ca-tls
  commonName: sandbox2.svc.cluster.local
  usages:
    - server auth
    - client auth
  isCA: true
  issuerRef:
    name: selfsigned-issuer
&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check the certificate and secret were created:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl -n sandbox2 get certificate sandbox2-ca
NAME          READY   SECRET            AGE
sandbox2-ca   True    sandbox2-ca-tls   15s

$ kubectl -n sandbox2 get secret sandbox2-ca-tls
NAME              TYPE                DATA   AGE
sandbox2-ca-tls   kubernetes.io/tls   3      22s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a second Issuer using the secret name from the &lt;code&gt;sandbox2-ca&lt;/code&gt; secret:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;In order to sign multiple certificates from the same CA we need to create an Issuer resource from secret created by the CA.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl apply -n sandbox2 -f &amp;lt;(echo &#39;
apiVersion: cert-manager.io/v1alpha2
kind: Issuer
metadata:
  name: sandbox2-ca-issuer
spec:
  ca:
    secretName: sandbox2-ca-tls&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a TLS Certificate from the new CA Issuer:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;We can add &lt;code&gt;usages&lt;/code&gt; to the certificate &lt;code&gt;spec&lt;/code&gt; to ensure that the certificates can be used for client/server authentication.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl apply -n sandbox2 -f &amp;lt;(echo &#39;
apiVersion: cert-manager.io/v1alpha2
kind: Certificate
metadata:
  name: sandbox2-server
spec:
  secretName: sandbox2-server-tls
  isCA: false
  usages:
    - server auth
    - client auth
  dnsNames:
  - &amp;quot;server.sandbox2.svc.cluster.local&amp;quot;
  - &amp;quot;server&amp;quot;
  issuerRef:
    name: sandbox2-ca-issuer
&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a second TLS Certificate from the new CA Issuer:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl apply -n sandbox2 -f &amp;lt;(echo &#39;
apiVersion: cert-manager.io/v1alpha2
kind: Certificate
metadata:
  name: sandbox2-client
spec:
  secretName: sandbox2-client-tls
  isCA: false
  usages:
    - server auth
    - client auth
  dnsNames:
  - &amp;quot;client.sandbox2.svc.cluster.local&amp;quot;
  - &amp;quot;client&amp;quot;
  issuerRef:
    name: sandbox2-ca-issuer
&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check that all three certificates are created:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl -n sandbox2 get certificate
NAME              READY   SECRET                AGE
sandbox2-ca       True    sandbox2-ca-tls       7m34s
sandbox2-client   True    sandbox2-client-tls   7s
sandbox2-server   True    sandbox2-server-tls   16s

$ kubectl -n sandbox2 get secret
NAME                  TYPE                                  DATA   AGE
sandbox2-ca-tls       kubernetes.io/tls                     3      8m14s
sandbox2-client-tls   kubernetes.io/tls                     3      48s
sandbox2-server-tls   kubernetes.io/tls                     3      57s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Validate the certificates against the CA:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ openssl verify -CAfile \
&amp;lt;(kubectl -n sandbox2 get secret sandbox2-ca-tls \
  -o jsonpath=&#39;{.data.ca\.crt}&#39; | base64 -d) \
&amp;lt;(kubectl -n sandbox2 get secret sandbox2-server-tls \
  -o jsonpath=&#39;{.data.tls\.crt}&#39; | base64 -d)
/proc/self/fd/18: OK

$ openssl verify -CAfile \
&amp;lt;(kubectl -n sandbox2 get secret sandbox2-ca-tls \
  -o jsonpath=&#39;{.data.ca\.crt}&#39; | base64 -d) \
&amp;lt;(kubectl -n sandbox2 get secret sandbox2-client-tls \
  -o jsonpath=&#39;{.data.tls\.crt}&#39; | base64 -d)
/proc/self/fd/18: OK
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Validate the Client / Server authentication&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Run an &lt;code&gt;openssl&lt;/code&gt; server as a background process:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;touch test.txt

openssl s_server \
  -cert &amp;lt;(kubectl -n sandbox2 get secret sandbox2-server-tls -o jsonpath=&#39;{.data.tls\.crt}&#39; | base64 -d) \
  -key &amp;lt;(kubectl -n sandbox2 get secret sandbox2-server-tls -o jsonpath=&#39;{.data.tls\.key}&#39; | base64 -d) \
  -CAfile &amp;lt;(kubectl -n sandbox2 get secret sandbox2-ca-tls -o jsonpath=&#39;{.data.ca\.crt}&#39; | base64 -d) \
  -WWW -port 12345  \
  -verify_return_error -Verify 1 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Run an &lt;code&gt;openssl&lt;/code&gt; client test:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;look for &lt;code&gt;HTTP/1.0 200 ok&lt;/code&gt; in the client output.&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;echo -e &#39;GET /test.txt HTTP/1.1\r\n\r\n&#39; | \
  openssl s_client \
  -cert &amp;lt;(kubectl -n sandbox2 get secret sandbox2-client-tls -o jsonpath=&#39;{.data.tls\.crt}&#39; | base64 -d) \
  -key &amp;lt;(kubectl -n sandbox2 get secret sandbox2-client-tls -o jsonpath=&#39;{.data.tls\.key}&#39; | base64 -d) \
  -CAfile &amp;lt;(kubectl -n sandbox2 get secret sandbox2-client-tls -o jsonpath=&#39;{.data.ca\.crt}&#39; | base64 -d) \
  -connect localhost:12345 -quiet
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;stop the background process:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kill %1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Congratulations, you&amp;rsquo;ve now created a pair of certificates signed by the same CA that can be used for client/server authentication.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Creating self signed certificates is now officially easy. You can use &lt;a href=&#34;https://github.com/paulczar/omgwtfssl&#34;&gt;omgwtfssl&lt;/a&gt; locally, or &lt;a href=&#34;https://cert-manager.io/&#34;&gt;cert-manager&lt;/a&gt; in your Kubernetes cluster. Either way you get cheap and easy self signed certificates for testing. Obviously you should use real certificates in production, in which case you would still be able to use &lt;a href=&#34;https://cert-manager.io/&#34;&gt;cert-manager&lt;/a&gt;.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Creating a Helm Chart Repository - Part 1</title>
      <link>https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-1/</link>
      <pubDate>Wed, 03 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-1/</guid>
      <description>Note: If this topic has peaked your interest, you can join me for a Webinar on August 15 where I&amp;rsquo;ll dive deep into Cloud Native Operations with Kubernetes and CI/CD Pipelines.
 Introduction Welcome to a three part blog series on Creating a Helm Chart Repository. In part 1 I will demonstrate creating a Helm chart repository using GitHub and GitHub Pages. In part 2 I will add Automation to automatically update the repository, and in part 3 I will add testing for changes to the charts themselves.</description>
      <content>

&lt;blockquote&gt;
&lt;p&gt;Note: If this topic has peaked your interest, you can join me for a Webinar on August 15 where I&amp;rsquo;ll dive deep into &lt;a href=&#34;https://content.pivotal.io/webinars/aug-15-cloud-native-operations-with-kubernetes-and-ci-cd-webinar?utm_campaign=cno-k8s-ci-cd-q319&amp;amp;utm_source=blog&amp;amp;utm_medium=website&#34;&gt;Cloud Native Operations with Kubernetes and CI/CD Pipelines&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Welcome to a three part blog series on Creating a &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Chart Repository. In &lt;strong&gt;part 1&lt;/strong&gt; I will demonstrate creating a &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; chart repository using &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; and &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Pages. In &lt;a href=&#34;https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-2&#34;&gt;part 2&lt;/a&gt; I will add Automation to automatically update the repository, and in &lt;a href=&#34;https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-3&#34;&gt;part 3&lt;/a&gt; I will add testing for changes to the charts themselves.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If you&amp;rsquo;re into Videos, I walked JJ through starting with Helm from scratch all the way to creating a Helm Repo and CI/CD.  &lt;div class=&#34;embed video-player&#34;&gt;
&lt;iframe class=&#34;youtube-player&#34; type=&#34;text/html&#34; width=&#34;640&#34; height=&#34;385&#34; src=&#34;http://www.youtube.com/embed/xn63krHJNKI&#34; allowfullscreen frameborder=&#34;0&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Helm is the defacto tool for packaging, sharing, and running Kubernetes Manifests. I&amp;rsquo;m going to assume you know the basics of &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; and have used it before. There&amp;rsquo;s plenty of great introductory topics around.&lt;/p&gt;

&lt;p&gt;You can host and share &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Charts (packages) via a &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Repository which is effectively a static website with an &lt;code&gt;index.yaml&lt;/code&gt; providing metadata and links to the &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Packages.&lt;/p&gt;

&lt;p&gt;This makes hosting a repository perfectly suited to running in &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Pages, s3, google cloud storage, etc. I like to use &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Pages as it allows your source code and repo to live effectively in the same place.&lt;/p&gt;

&lt;p&gt;I will walk you through creating a new &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Project hosting multiple &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; charts and demonstrate how to set up Continuous Integration with CircleCI to automatically test and publish new changes to your &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Charts.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: While I would usually use &lt;a href=&#34;https://concourse-ci.org/&#34;&gt;Concourse CI&lt;/a&gt; for my CI workflows, I wanted to &lt;em&gt;only&lt;/em&gt; use managed services and I chose Circle as that is already commonly used in the Helm community.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;create-a-new-github-https-github-com-repository&#34;&gt;Create a new &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Repository&lt;/h2&gt;

&lt;p&gt;Log into &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; and &lt;a href=&#34;https://github.com/new&#34;&gt;create a new repository&lt;/a&gt; called &lt;code&gt;my-helm-charts&lt;/code&gt;. I chose to have &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; create it as with an Apache2 License.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./github-new-repo.png&#34; alt=&#34;Creating new repo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can now clone down this repository and get to work:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone git@github.com:paulczar/my-helm-charts.git
Cloning into &#39;my-helm-charts&#39;...
remote: Enumerating objects: 4, done.
remote: Counting objects: 100% (4/4), done.
remote: Compressing objects: 100% (3/3), done.
remote: Total 4 (delta 0), reused 0 (delta 0), pack-reused 0
Receiving objects: 100% (4/4), 4.52 KiB | 4.52 MiB/s, done.

$ cd my-helm-charts

$ tree
.
├── LICENSE
└── README.md

0 directories, 2 files
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can see we have a default LICENSE file and a default README.md, we can leave them alone for now. Your next step is to create a couple of &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; charts. Since this is purely for demonstration purposes they don&amp;rsquo;t have to be overly functional charts which means we can just use the default boilerplate created by &lt;code&gt;helm create&lt;/code&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;You do have &lt;a href=&#34;https://github.com/helm/helm#install&#34;&gt;helm&lt;/a&gt; installed right?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ mkdir charts

$ [Helm](https://helm.sh) create charts/app1
Creating charts/app1

$ [Helm](https://helm.sh) create charts/app2
Creating charts/app2

$ tree
.
├── charts
│   ├── app1
│   │   ├── charts
│   │   ├── Chart.yaml
│   │   ├── templates
│   │   │   ├── deployment.yaml
│   │   │   ├── _helpers.tpl
│   │   │   ├── ingress.yaml
│   │   │   ├── NOTES.txt
│   │   │   ├── service.yaml
│   │   │   └── tests
│   │   │       └── test-connection.yaml
│   │   └── values.yaml
│   └── app2
│       ├── charts
│       ├── Chart.yaml
│       ├── templates
│       │   ├── deployment.yaml
│       │   ├── _helpers.tpl
│       │   ├── ingress.yaml
│       │   ├── NOTES.txt
│       │   ├── service.yaml
│       │   └── tests
│       │       └── test-connection.yaml
│       └── values.yaml
├── LICENSE
└── README.md

9 directories, 18 files
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Push these changes to git:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;echo &amp;quot;.deploy&amp;quot; &amp;gt;&amp;gt; .gitignore

$ git add .

$ git commit -m &#39;Initial Commit&#39;
[master 2172141] Initial Commit
 18 files changed, 524 insertions(+)
...
...

$ git push origin master
Enumerating objects: 27, done.
Counting objects: 100% (27/27), done.
Delta compression using up to 4 threads
Compressing objects: 100% (24/24), done.
Writing objects: 100% (26/26), 4.72 KiB | 536.00 KiB/s, done.
Total 26 (delta 8), reused 0 (delta 0)
remote: Resolving deltas: 100% (8/8), done.
To github.com:paulczar/my-helm-charts.git
   abdcced..2172141  master -&amp;gt; master
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;publish-your-helm-https-helm-sh-repository&#34;&gt;Publish your &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Repository&lt;/h2&gt;

&lt;h3 id=&#34;prepare-github-https-github-com-pages&#34;&gt;Prepare &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Pages&lt;/h3&gt;

&lt;p&gt;We&amp;rsquo;re going to use a combination of &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Pages and releases to host our &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Repository. Therefore we need to ensure we have &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Pages enabled on the git repo and to create an empty &lt;code&gt;gh-pages&lt;/code&gt; branch.&lt;/p&gt;

&lt;p&gt;You can create an empty &lt;code&gt;gh-pages&lt;/code&gt; branch by creating an orphan branch like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git checkout --orphan gh-pages
Switched to a new branch &#39;gh-pages&#39;

$ rm -rf charts

$ git add . --all

$ git commit -m &#39;initial gh-pages&#39;
git commit -m &#39;initial gh-pages&#39;
[gh-pages a9ce382] initial gh-pages
 18 files changed, 524 deletions(-)
...
...

$ git push origin gh-pages
Enumerating objects: 3, done.
...
...
To github.com:paulczar/my-helm-charts.git
 * [new branch]      gh-pages -&amp;gt; gh-pages
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next check that &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Pages is enabled by clicking on your git repo settings in GitHub:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./github-pages.png&#34; alt=&#34;github repo settings&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: I have a custom domain set up, your URL will probably be username.github.io/my-helm-charts.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;After a few minutes you should have a default rendering on your README.md at the provided URL:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./default-gh-pages.png&#34; alt=&#34;default [GitHub](https://github.com) ages&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;introducing-chart-releaser&#34;&gt;Introducing chart-releaser&lt;/h3&gt;

&lt;p&gt;You could use a combination of &lt;code&gt;helm package&lt;/code&gt; and &lt;code&gt;helm repo&lt;/code&gt; commands to construct your &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; repository by hand, or you can simplify your life by using &lt;code&gt;chart-releaser&lt;/code&gt; which will not only create your packages, but will upload them as binaries into an appropriately versioned &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Release.&lt;/p&gt;

&lt;p&gt;Download chart-releaser for your architecture [here].&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: I&amp;rsquo;m doing this on a linux machine, so you may need to update the commands below for Mac OS.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In a new terminal download and unpackage it, moving it to an executable path:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd /tmp
$ curl -sSL https://github.com/helm/chart-releaser/releases/download/v0.2.1/chart-releaser_0.2.1_linux_amd64.tar.gz | tar xzf -

$ mv cr ~/bin/cr

$ cr help
Create [Helm](https://helm.sh) chart repositories on [GitHub](https://github.com) Pages by uploading Chart packages
and Chart metadata to [GitHub](https://github.com) Releases and creating a suitable index file

Usage:
  cr [command]

Available Commands:
  help        Help about any command
  index       Update Helm repo index.yaml for the given GitHub repo
  upload      Upload Helm chart packages to GitHub Releases
  version     Print version information

Flags:
      --config string   Config file (default is $HOME/.cr.yaml)
  -h, --help            help for cr

Use &amp;quot;cr [command] --help&amp;quot; for more information about a command.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are two commands we care about &lt;code&gt;cr index&lt;/code&gt; and &lt;code&gt;cr upload&lt;/code&gt;, the first will create an appropriate &lt;code&gt;index.yaml&lt;/code&gt; and the second will upload the packages to &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Releases. In order to do the latter you&amp;rsquo;ll need to pass it in a &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Token so that it can use the &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; APIs.&lt;/p&gt;

&lt;p&gt;In your browser go to your &lt;a href=&#34;https://github.com/settings/tokens&#34;&gt;github developer settings&lt;/a&gt; and create a new personal access token.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./personal-access-token.png&#34; alt=&#34;create personal access token&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Create an environment variable (or a &lt;code&gt;~/.cr.yaml&lt;/code&gt; config file) containing the access token:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Pro-tip: put an additional space in your command right before &lt;code&gt;export&lt;/code&gt; and it won&amp;rsquo;t be saved to your command history.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$  export CH_TOKEN=c4a4ed6ab91a246572b0c46c19c630ccadc1049
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;create-and-upload-helm-https-helm-sh-packages&#34;&gt;Create and Upload &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Packages&lt;/h3&gt;

&lt;p&gt;Your next step is to create and upload the packages:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ Helm package charts/{app1,app2} --destination .deploy
Successfully packaged chart and saved it to: .deploy/app1-0.1.0.tgz
Successfully packaged chart and saved it to: .deploy/app2-0.1.0.tgz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Run &lt;code&gt;cr upload&lt;/code&gt; to create releases and upload the packages, note if it runs correctly there&amp;rsquo;s no output.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cr upload -o paulczar -r my-helm-charts -p .deploy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check your &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; repository now has a releases page with two releases:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./github-releases&#34; alt=&#34;github releases page&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;create-and-upload-index-file-to-github-https-github-com-pages&#34;&gt;Create and upload index file to &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Pages&lt;/h3&gt;

&lt;p&gt;Checkout your &lt;code&gt;gh-pages&lt;/code&gt; branch and run &lt;code&gt;cr index&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git checkout gh-pages

$ cr index -i ./index.yaml -p .deploy --owner paulczar --repo my-helm-charts
====&amp;gt; UpdateIndexFile new index at ./index.yaml
====&amp;gt; Found app1-0.1.0.tgz
====&amp;gt; Extracting chart metadata from .deploy/app1-0.1.0.tgz
====&amp;gt; Calculating Hash for .deploy/app1-0.1.0.tgz
====&amp;gt; Found app2-0.1.0.tgz
====&amp;gt; Extracting chart metadata from .deploy/app2-0.1.0.tgz
====&amp;gt; Calculating Hash for .deploy/app2-0.1.0.tgz
--&amp;gt; Updating index ./index.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There should now be a &lt;code&gt;index.yaml&lt;/code&gt; file containing the details of your &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; packages and the path to their archive:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cat index.yaml
apiVersion: v1
entries:
  app1:
  - apiVersion: v1
    appVersion: &amp;quot;1.0&amp;quot;
    created: &amp;quot;2019-07-03T13:03:06.139332963-05:00&amp;quot;
    description: A Helm chart for Kubernetes
    digest: 48cf831b72febeac2860a0be372094250aea68a9c76147c028085c8802dd48ec
    name: app1
    urls:
    - https://github.com/paulczar/my-helm-charts/releases/download/app1-0.1.0/app1-0.1.0.tgz
    version: 0.1.0
  app2:
  - apiVersion: v1
    appVersion: &amp;quot;1.0&amp;quot;
    created: &amp;quot;2019-07-03T13:03:07.301308677-05:00&amp;quot;
    description: A Helm chart for Kubernetes
    digest: 64b00fc4804aba524201f64e78ee22ad8e61d0923424f8e24e8b70befed88141
    name: app2
    urls:
    - https://github.com/paulczar/my-helm-charts/releases/download/app2-0.1.0/app2-0.1.0.tgz
    version: 0.1.0
generated: &amp;quot;2019-07-03T13:03:05.685803874-05:00&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Commit this to git and then wait a few minutes and check that it exists in your &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Pages url:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git add index.yaml

$ git commit -m &#39;release 0.1.0&#39;
[gh-pages 696df18] release 0.1.0
 1 file changed, 23 insertions(+)
 create mode 100644 index.yaml

$ git push origin gh-pages
...
To github.com:paulczar/my-helm-charts.git
   75f1fe8..696df18  gh-pages -&amp;gt; gh-pages
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check it exists in &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Pages:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;github-pages-index-yaml.png&#34; alt=&#34;github pages index.yaml&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;test-your-new-helm-https-helm-sh-repostiory&#34;&gt;Test your new &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Repostiory&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: To do this you&amp;rsquo;ll need a Kubernetes cluster with Helm&amp;rsquo;s tiller installed, but you already know how to do that right?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;$ helm repo add my-helm-charts https://tech.paulcz.net/my-helm-charts
&amp;quot;my-helm-charts&amp;quot; has been added to your repositories

$ helm install --name test --namespace test my-helm-charts/app1
NAME:   test
LAST DEPLOYED: Wed Jul  3 13:17:32 2019
NAMESPACE: test
STATUS: DEPLOYED

RESOURCES:
==&amp;gt; v1/Deployment
NAME       READY  UP-TO-DATE  AVAILABLE  AGE
test-app1  0/1    1           0          0s

==&amp;gt; v1/Pod(related)
NAME                        READY  STATUS             RESTARTS  AGE
test-app1-7b575d95f6-zhlh2  0/1    ContainerCreating  0         0s

==&amp;gt; v1/Service
NAME       TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)  AGE
test-app1  ClusterIP  10.100.200.213  &amp;lt;none&amp;gt;       80/TCP   0s


NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace test -l &amp;quot;app.kubernetes.io/name=app1,app.kubernetes.io/instance=test&amp;quot; -o jsonpath=&amp;quot;{.items[0].metadata.name}&amp;quot;)
  echo &amp;quot;Visit http://127.0.0.1:8080 to use your application&amp;quot;
  kubectl port-forward $POD_NAME 8080:80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check that it deployed ok:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl -n test get all
NAME                             READY   STATUS    RESTARTS   AGE
pod/test-app1-7b575d95f6-zhlh2   1/1     Running   0          42m

NAME                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
service/test-app1   ClusterIP   10.100.200.213   &amp;lt;none&amp;gt;        80/TCP    42m

NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/test-app1   1/1     1            1           42m

NAME                                   DESIRED   CURRENT   READY   AGE
replicaset.apps/test-app1-7b575d95f6   1         1         1       42m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Clean up:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm delete --purge test
release &amp;quot;test&amp;quot; deleted
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;update-the-readme-md-with-instructions-on-using&#34;&gt;Update the README.md with instructions on using&lt;/h3&gt;

&lt;p&gt;switch back to your &lt;code&gt;master&lt;/code&gt; brach:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git checkout master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Edit your README.md to provide details on how to use charts from your repository:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;# My [Helm](https://helm.sh) Charts

This repository contains [Helm](https://helm.sh) charts for various projects

* [Application 1](charts/app1/)
* [Application 2](charts/app2/)

## Installing Charts from this Repository

Add the Repository to Helm:

    helm repo add my-helm-charts https://tech.paulcz.net/my-helm-charts

Install Application 1:

    helm install my-helm-charts/app1

Install Application 2:

    helm install my-helm-charts/app2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Commit the change up to GitHub:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git add README.md
$ git commit -m &#39;update readme with instructions&#39;
$ git push origin master
...
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;That&amp;rsquo;s the end of &lt;strong&gt;Part 1&lt;/strong&gt; of this three part series. In future posts I will demonstrate adding &lt;a href=&#34;https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-2&#34;&gt;automation&lt;/a&gt; and &lt;a href=&#34;https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-3&#34;&gt;testing&lt;/a&gt; to this &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; chart repository.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Creating a Helm Chart Repository - Part 2</title>
      <link>https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-2/</link>
      <pubDate>Wed, 03 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-2/</guid>
      <description>Introduction Welcome to a three part blog series on Creating a Helm Chart Repository. In part 1 of this series I demonstrated creating a Helm chart repository using GitHub and GitHub Pages. In this part 2 I will add Automation to automatically update the repository, and in part 2 I will add testing for changes to the charts themselves.
 If you&amp;rsquo;re into Videos, I walked JJ through starting with Helm from scratch all the way to creating a Helm Repo and CI/CD.</description>
      <content>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Welcome to a three part blog series on Creating a &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Chart Repository. In &lt;a href=&#34;https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-1&#34;&gt;part 1&lt;/a&gt; of this series I demonstrated creating a &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; chart repository using &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; and &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Pages. In this &lt;strong&gt;part 2&lt;/strong&gt; I will add Automation to automatically update the repository, and in &lt;a href=&#34;https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-3&#34;&gt;part 2&lt;/a&gt; I will add testing for changes to the charts themselves.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If you&amp;rsquo;re into Videos, I walked JJ through starting with Helm from scratch all the way to creating a Helm Repo and CI/CD.  &lt;div class=&#34;embed video-player&#34;&gt;
&lt;iframe class=&#34;youtube-player&#34; type=&#34;text/html&#34; width=&#34;640&#34; height=&#34;385&#34; src=&#34;http://www.youtube.com/embed/xn63krHJNKI&#34; allowfullscreen frameborder=&#34;0&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;use-circle-ci-to-automate-helm-https-helm-sh-chart-updates&#34;&gt;Use Circle CI to automate &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Chart Updates&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: While I would usually use &lt;a href=&#34;https://concourse-ci.org/&#34;&gt;Concourse CI&lt;/a&gt; for my CI workflows, I wanted to &lt;em&gt;only&lt;/em&gt; use managed services and I chose Circle as that is already commonly used in the Helm community. It would be trivial to whip up a Concourse Pipeline to do the same thing.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now that we&amp;rsquo;ve successfully created a &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Chart Repostiory using &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; and &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; pages we can move on to adding some Automation so that our Chart Repository is updated any time we push changes up to our master branch.&lt;/p&gt;

&lt;p&gt;Its pretty easy to create a new Circle CI account. You simply go to their website and hit &lt;a href=&#34;https://circleci.com/signup/&#34;&gt;sign-up&lt;/a&gt;, it will ask you to log using &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt;Oauth2 and once you&amp;rsquo;ve given it access to your repositories you are good to go.&lt;/p&gt;

&lt;p&gt;Once logged in you need to hit the &lt;strong&gt;ADD Projects&lt;/strong&gt; menu item and hit the &lt;strong&gt;set up project&lt;/strong&gt; button next to &lt;strong&gt;my-helm-charts&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;circle-add-project.png&#34; alt=&#34;add project&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can leave the defaults and just go down and click the &lt;strong&gt;Start Building&lt;/strong&gt; button.&lt;/p&gt;

&lt;p&gt;It will attempt to run and fail because you don&amp;rsquo;t have a &lt;code&gt;.circleci/config.yml&lt;/code&gt; file in your repo yet. We&amp;rsquo;ll create that soon.&lt;/p&gt;

&lt;p&gt;Before we do that though we need to create a private key for Circle CI with write access to our project. Hit the &lt;strong&gt;settings&lt;/strong&gt; button on the top right of the &lt;strong&gt;Workflows -&amp;gt; username -&amp;gt; my-helm-charts&lt;/strong&gt; screen tat looks like a little cog.&lt;/p&gt;

&lt;p&gt;From here you want to hit SSH permissions and hit &lt;strong&gt;Checkout SSH Keys&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;There should be a &lt;strong&gt;Add user key&lt;/strong&gt; section with a button that says &lt;strong&gt;Authorize with GitHub&lt;/strong&gt;, hit that button. To be extra certain it loads the same page and you need to click the &lt;strong&gt;Create and add [username] user key&lt;/strong&gt; which will create a key and pass the public key off to github.&lt;/p&gt;

&lt;p&gt;On that same &lt;strong&gt;settings&lt;/strong&gt; page you need to add some environment variables:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./circle-env-vars.png&#34; alt=&#34;circle env vars&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now its time to set up our Automation.&lt;/p&gt;

&lt;h3 id=&#34;create-circle-ci-config-for-uploading-new-packages&#34;&gt;Create Circle CI config for uploading new packages&lt;/h3&gt;

&lt;p&gt;Create a new directory &lt;code&gt;.circleci&lt;/code&gt; and a file inside that called &lt;code&gt;config.yml&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ mkdir .circleci
$ touch .circleci/config.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Write out the &lt;code&gt;config.yml&lt;/code&gt; file like so:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: this CircleCI config file creates two jobs, One to lint the shell scripts we&amp;rsquo;re about to create, the other to release charts and copy documentation into our &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; repo website. These tasks will run when code is pushed or merged into the &lt;code&gt;master&lt;/code&gt; branch.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;circleci-config-yml-https-github-com-paulczar-my-helm-charts-blob-part-2-circleci-config-yml&#34;&gt;&lt;a href=&#34;https://github.com/paulczar/my-helm-charts/blob/part-2/.circleci/config.yml&#34;&gt;.circleci/config.yml&lt;/a&gt;&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;version: 2
jobs:
  lint-scripts:
    docker:
      - image: koalaman/shellcheck-alpine
    steps:
      - checkout
      - run:
          name: lint
          command: shellcheck -x .circleci/*.sh
  release-charts:
    machine: true
    steps:
      - checkout
      - run:
          command: |
            echo &amp;quot;export GIT_REPOSITORY_URL=$CIRCLE_REPOSITORY_URL&amp;quot; &amp;gt;&amp;gt; $BASH_ENV
            echo &amp;quot;export GIT_USERNAME=$CIRCLE_PROJECT_USERNAME&amp;quot; &amp;gt;&amp;gt; $BASH_ENV
            echo &amp;quot;export GIT_REPOSITORY_NAME=$CIRCLE_PROJECT_REPONAME&amp;quot; &amp;gt;&amp;gt; $BASH_ENV
            .circleci/install_tools.sh
            .circleci/release.sh

workflows:
  version: 2
  release:
    jobs:
      - lint-scripts
      - release-charts:
          filters:
            tags:
              ignore: /.*/
            branches:
              only: master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We referenced two scripts in the &lt;code&gt;config.yml&lt;/code&gt; file, so we better create those. These scripts are a mix of ones that I have written, and have borrowed from &lt;a href=&#34;https://twitter.com/unguiculus&#34;&gt;Reinhard Nägele&lt;/a&gt; one of the main contributors to awesome tooling in the Helm Community as found &lt;a href=&#34;https://github.com/codecentric/helm-charts/blob/master/.circleci/release.sh&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;It&amp;rsquo;s no surprise that these scripts came from &lt;a href=&#34;https://twitter.com/unguiculus&#34;&gt;Reinhard Nägele&lt;/a&gt;as he is a primary maintainer of both &lt;a href=&#34;https://github.com/helm/chart-testing&#34;&gt;chart-testing&lt;/a&gt; and &lt;a href=&#34;https://github.com/helm/chart-releaser&#34;&gt;chart-releaser&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;circleci-install-tools-sh-https-github-com-paulczar-my-helm-charts-blob-part-2-circleci-install-tools-sh&#34;&gt;&lt;a href=&#34;https://github.com/paulczar/my-helm-charts/blob/part-2/.circleci/install_tools.sh&#34;&gt;.circleci/install_tools.sh&lt;/a&gt;&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/usr/bin/env bash

set -o errexit

readonly HELM_VERSION=2.13.1
readonly CHART_RELEASER_VERSION=0.1.4

echo &amp;quot;Installing Helm...&amp;quot;
curl -LO &amp;quot;https://kubernetes-helm.storage.googleapis.com/helm-v$HELM_VERSION-linux-amd64.tar.gz&amp;quot;
sudo mkdir -p &amp;quot;/usr/local/helm-v$HELM_VERSION&amp;quot;
sudo tar -xzf &amp;quot;helm-v$HELM_VERSION-linux-amd64.tar.gz&amp;quot; -C &amp;quot;/usr/local/helm-v$HELM_VERSION&amp;quot;
sudo ln -s &amp;quot;/usr/local/helm-v$HELM_VERSION/linux-amd64/helm&amp;quot; /usr/local/bin/helm
rm -f &amp;quot;helm-v$HELM_VERSION-linux-amd64.tar.gz&amp;quot;
helm init --client-only

echo &amp;quot;Installing chart-releaser...&amp;quot;
curl -LO &amp;quot;https://github.com/helm/chart-releaser/releases/download/v${CHART_RELEASER_VERSION}/chart-releaser_${CHART_RELEASER_VERSION}_Linux_x86_64.tar.gz&amp;quot;
sudo mkdir -p &amp;quot;/usr/local/chart-releaser-v$CHART_RELEASER_VERSION&amp;quot;
sudo tar -xzf &amp;quot;chart-releaser_${CHART_RELEASER_VERSION}_Linux_x86_64.tar.gz&amp;quot; -C &amp;quot;/usr/local/chart-releaser-v$CHART_RELEASER_VERSION&amp;quot;
sudo ln -s &amp;quot;/usr/local/chart-releaser-v$CHART_RELEASER_VERSION/chart-releaser&amp;quot; /usr/local/bin/chart-releaser
rm -f &amp;quot;chart-releaser_${CHART_RELEASER_VERSION}_Linux_x86_64.tar.gz&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;circleci-release-sh-https-github-com-paulczar-my-helm-charts-blob-part-2-circleci-release-sh&#34;&gt;&lt;a href=&#34;https://github.com/paulczar/my-helm-charts/blob/part-2/.circleci/release.sh&#34;&gt;.circleci/release.sh&lt;/a&gt;&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/usr/bin/env bash

set -o errexit
set -o nounset
set -o pipefail

: &amp;quot;${CH_TOKEN:?Environment variable CH_TOKEN must be set}&amp;quot;
: &amp;quot;${GIT_REPOSITORY_URL:?Environment variable GIT_REPO_URL must be set}&amp;quot;
: &amp;quot;${GIT_USERNAME:?Environment variable GIT_USERNAME must be set}&amp;quot;
: &amp;quot;${GIT_EMAIL:?Environment variable GIT_EMAIL must be set}&amp;quot;
: &amp;quot;${GIT_REPOSITORY_NAME:?Environment variable GIT_REPOSITORY_NAME must be set}&amp;quot;

readonly REPO_ROOT=&amp;quot;${REPO_ROOT:-$(git rev-parse --show-toplevel)}&amp;quot;

main() {
    pushd &amp;quot;$REPO_ROOT&amp;quot; &amp;gt; /dev/null

    echo &amp;quot;Fetching tags...&amp;quot;
    git fetch --tags

    local latest_tag
    latest_tag=$(find_latest_tag)

    local latest_tag_rev
    latest_tag_rev=$(git rev-parse --verify &amp;quot;$latest_tag&amp;quot;)
    echo &amp;quot;$latest_tag_rev $latest_tag (latest tag)&amp;quot;

    local head_rev
    head_rev=$(git rev-parse --verify HEAD)
    echo &amp;quot;$head_rev HEAD&amp;quot;

    if [[ &amp;quot;$latest_tag_rev&amp;quot; == &amp;quot;$head_rev&amp;quot; ]]; then
        echo &amp;quot;No code changes. Nothing to release.&amp;quot;
        exit
    fi

    rm -rf .deploy
    mkdir -p .deploy

    echo &amp;quot;Identifying changed charts since tag &#39;$latest_tag&#39;...&amp;quot;

    local changed_charts=()
    readarray -t changed_charts &amp;lt;&amp;lt;&amp;lt; &amp;quot;$(git diff --find-renames --name-only &amp;quot;$latest_tag_rev&amp;quot; -- charts | cut -d &#39;/&#39; -f 2 | uniq)&amp;quot;

    if [[ -n &amp;quot;${changed_charts[*]}&amp;quot; ]]; then
        for chart in &amp;quot;${changed_charts[@]}&amp;quot;; do
            echo &amp;quot;Packaging chart &#39;$chart&#39;...&amp;quot;
            package_chart &amp;quot;charts/$chart&amp;quot;
        done

        release_charts
        sleep 5
        update_index
    else
        echo &amp;quot;Nothing to do. No chart changes detected.&amp;quot;
    fi

    popd &amp;gt; /dev/null
}

find_latest_tag() {
    if ! git describe --tags --abbrev=0 2&amp;gt; /dev/null; then
        git rev-list --max-parents=0 --first-parent HEAD
    fi
}

package_chart() {
    local chart=&amp;quot;$1&amp;quot;
    Helm dependency build &amp;quot;$chart&amp;quot;
    Helm package &amp;quot;$chart&amp;quot; --destination .deploy
}

release_charts() {
    chart-releaser upload -o &amp;quot;$GIT_USERNAME&amp;quot; -r &amp;quot;$GIT_REPOSITORY_NAME&amp;quot; -p .deploy
}

update_index() {
    chart-releaser index -o &amp;quot;$GIT_USERNAME&amp;quot; -r &amp;quot;$GIT_REPOSITORY_NAME&amp;quot; -p .deploy/index.yaml

    git config user.email &amp;quot;$GIT_EMAIL&amp;quot;
    git config user.name &amp;quot;$GIT_USERNAME&amp;quot;

    for file in charts/*/*.md; do
        if [[ -e $file ]]; then
            mkdir -p &amp;quot;.deploy/docs/$(dirname &amp;quot;$file&amp;quot;)&amp;quot;
            cp --force &amp;quot;$file&amp;quot; &amp;quot;.deploy/docs/$(dirname &amp;quot;$file&amp;quot;)&amp;quot;
        fi
    done

    git checkout gh-pages
    cp --force .deploy/index.yaml index.yaml

    if [[ -e &amp;quot;.deploy/docs/charts&amp;quot; ]]; then
        mkdir -p charts
        cp --force --recursive .deploy/docs/charts/* charts/
    fi

    git checkout master -- README.md

    if ! git diff --quiet; then
        git add .
        git commit --message=&amp;quot;Update index.yaml&amp;quot; --signoff
        git push &amp;quot;$GIT_REPOSITORY_URL&amp;quot; gh-pages
    fi
}

main
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add these new files to git and push them up to the &lt;code&gt;master&lt;/code&gt; branch:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git add .

$ git status
On branch master
Your branch is up to date with &#39;origin/master&#39;.

Changes to be committed:
  (use &amp;quot;git reset HEAD &amp;lt;file&amp;gt;...&amp;quot; to unstage)

  new file:   .circleci/config.yml
  new file:   .circleci/install_tools.sh
  new file:   .circleci/release.sh

$ git commit -m &#39;add circle ci scripts&#39;

$ git push origin master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This push should kick off a Circle CI job which will hopefully pass (I usually get it wrong the first few times).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./circle-first-job.png&#34; alt=&#34;circle first job&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You&amp;rsquo;ll notice there&amp;rsquo;s a failed job, that&amp;rsquo;s because when circleci sees the releases being updated it tries to run a job for the &lt;code&gt;gh-pages&lt;/code&gt; branch that doesn&amp;rsquo;t have a circle-ci config. We can use this sweet git trick to grab the one from the master branch:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git checkout gh-pages
$ git pull origin gh-pages
$ mkdir .circleci
$ git checkout master -- .circleci/config.yml
$ git add .circleci/config.yml
$ git commit -m &#39;add circleci config&#39;
$ git push origin gh-pages
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;validate-the-release-of-new-charts&#34;&gt;Validate the release of new charts&lt;/h3&gt;

&lt;p&gt;So far we haven&amp;rsquo;t actually changed our &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Charts, so the automation hasn&amp;rsquo;t created a new release. We can change this by bumping the chart version of one of them.  Edit &lt;code&gt;./charts/app1/Chart.yaml&lt;/code&gt; and bump the version like so:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
appVersion: &amp;quot;1.0&amp;quot;
description: A Helm chart for Kubernetes
name: app1
version: 0.1.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Push this change up:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git add .
$ git commit -m &#39;update app1 chart&#39;
$ git push origin master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should see the new job show up in Circle and complete fairly quickly.&lt;/p&gt;

&lt;p&gt;Once the job has completed successfully you can check you now have a &lt;code&gt;myapp-0.1.1&lt;/code&gt; release in your &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt;repo and your &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; repository now has &lt;code&gt;myapp-0.1.1&lt;/code&gt; in its &lt;code&gt;index.yaml&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ curl http://tech.paulcz.net/my-helm-charts/index.yaml
apiVersion: v1
entries:
  app1:
  - apiVersion: v1
    appVersion: &amp;quot;1.0&amp;quot;
    created: &amp;quot;2019-07-03T23:16:21.087774995Z&amp;quot;
    description: A Helm chart for Kubernetes
    digest: 9fbf6f9d10fba82aa3b749875e137b283890136a7379efba2bbff0b645cb1c35
    name: app1
    urls:
    - https://github.com/paulczar/my-helm-charts/releases/download/app1-0.1.1/app1-0.1.1.tgz
    version: 0.1.1
  - apiVersion: v1
    appVersion: &amp;quot;1.0&amp;quot;
    created: &amp;quot;2019-07-03T23:16:21.376254864Z&amp;quot;
    description: A Helm chart for Kubernetes
    digest: 48cf831b72febeac2860a0be372094250aea68a9c76147c028085c8802dd48ec
    name: app1
    urls:
    - https://github.com/paulczar/my-helm-charts/releases/download/app1-0.1.0/app1-0.1.0.tgz
    version: 0.1.0
  app2:
  - apiVersion: v1
    appVersion: &amp;quot;1.0&amp;quot;
    created: &amp;quot;2019-07-03T23:16:21.22793015Z&amp;quot;
    description: Helm chart for Kubernetes
    digest: 64b00fc4804aba524201f64e78ee22ad8e61d0923424f8e24e8b70befed88141
    name: app2
    urls:
    - https://github.com/paulczar/my-helm-charts/releases/download/app2-0.1.0/app2-0.1.0.tgz
    version: 0.1.0
generated: &amp;quot;2019-07-03T23:16:20.624914794Z&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In &lt;a href=&#34;https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-1&#34;&gt;Part 1&lt;/a&gt; we created set of &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Charts managed in source control (GitHub) and in Part 2 we just added automation via CircleCI to automate building and deploying Chart packages to a &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Chart Repository hosted in &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; pages and &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt;releases.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&#34;https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-3&#34;&gt;Part 3&lt;/a&gt; we will add further automation to test for changes in those &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; charts and to pass them through rigorous testing before allowing them to be merged into the &lt;code&gt;master&lt;/code&gt; branch.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Creating a Helm Chart Repository - Part 3</title>
      <link>https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-3/</link>
      <pubDate>Wed, 03 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-3/</guid>
      <description>Introduction Welcome to a three part blog series on Creating a Helm Chart Repository. In part 1 of this series I demonstrated creating a Helm chart repository using GitHub and GitHub Pages. In part 2 I will add Automation to automatically update the repository, and in part 3 I will add testing for changes to the charts themselves.
 If you&amp;rsquo;re into Videos, I walked JJ through starting with Helm from scratch all the way to creating a Helm Repo and CI/CD.</description>
      <content>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Welcome to a three part blog series on Creating a &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Chart Repository. In &lt;a href=&#34;https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-1&#34;&gt;part 1&lt;/a&gt; of this series I demonstrated creating a &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; chart repository using &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; and &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Pages. In &lt;a href=&#34;https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-2&#34;&gt;part 2&lt;/a&gt; I will add Automation to automatically update the repository, and in &lt;strong&gt;part 3&lt;/strong&gt; I will add testing for changes to the charts themselves.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If you&amp;rsquo;re into Videos, I walked JJ through starting with Helm from scratch all the way to creating a Helm Repo and CI/CD.  &lt;div class=&#34;embed video-player&#34;&gt;
&lt;iframe class=&#34;youtube-player&#34; type=&#34;text/html&#34; width=&#34;640&#34; height=&#34;385&#34; src=&#34;http://www.youtube.com/embed/xn63krHJNKI&#34; allowfullscreen frameborder=&#34;0&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;use-circle-ci-to-test-helm-https-helm-sh-charts&#34;&gt;Use Circle CI to test &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Charts&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Note - You could use any other CI system here, I chose Circle as it is easy to integrate with &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; and has a free tier. If you do use a different CI system the scripts should still work, but you&amp;rsquo;ll need to rewrite a config file suitable for your CI choice.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;introducing-chart-testing&#34;&gt;Introducing Chart Testing&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; community has built a tool very imaginitively named [Chart Testing]((&lt;a href=&#34;https://github.com/helm/chart-testing&#34;&gt;https://github.com/helm/chart-testing&lt;/a&gt;) specifically for testing &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; charts. Not only is it capable of linting and performing test installs of a &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; chart, but its also designed to work within a monorepo and only test those charts that have changed.&lt;/p&gt;

&lt;p&gt;You can download and use &lt;a href=&#34;https://github.com/helm/chart-testing/releases&#34;&gt;Chart Testing&lt;/a&gt; locally, but really the power of it is using it in CI, so lets go straight to that.&lt;/p&gt;

&lt;h3 id=&#34;creat-a-chart-testing-script-and-update-circle-ci-config&#34;&gt;Creat a Chart Testing script and update Circle CI config&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: While I would usually use &lt;a href=&#34;https://concourse-ci.org/&#34;&gt;Concourse CI&lt;/a&gt; for my CI workflows, I wanted to &lt;em&gt;only&lt;/em&gt; use managed services and I chose Circle as that is already commonly used in the Helm community.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We need to add a new script, a chart-testing config file, and update the Circle CI config file.&lt;/p&gt;

&lt;h4 id=&#34;circleci-config-yaml-https-github-com-paulczar-my-helm-charts-blob-part-2-circleci-config-yaml&#34;&gt;&lt;a href=&#34;https://github.com/paulczar/my-helm-charts/blob/part-2/.circleci/config.yaml&#34;&gt;./circleci/config.yaml&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;Create two new jobs:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;These scripts and configs were heavily borrowed from &lt;a href=&#34;https://twitter.com/unguiculus&#34;&gt;Reinhard Nägele&lt;/a&gt; who is a primary maintainer of both &lt;a href=&#34;https://github.com/helm/chart-testing&#34;&gt;chart-testing&lt;/a&gt; and &lt;a href=&#34;https://github.com/helm/chart-releaser&#34;&gt;chart-releaser&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The first job tells Chart Testing to lint the charts according to the &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Community &lt;a href=&#34;https://helm.sh/docs/chart_best_practices/&#34;&gt;Best Practices Guide&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The second job tells Chart Testing to actually install and test the charts using KIND (Kubernetes IN Docker).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;  lint-charts:
    docker:
      - image: gcr.io/kubernetes-charts-ci/test-image:v3.3.2
    steps:
      - checkout
      - run:
          name: lint
          command: |
            git remote add upstream https://github.com/paulczar/percona-helm-charts
            git fetch upstream master
            ct lint --config .circleci/ct.yaml

  install-charts:
    machine: true
    steps:
      - checkout
      - run:
          no_output_timeout: 12m
          command: .circleci/install_charts.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add a new workflow telling Circle to lint and test any changes.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: it excludes the &lt;code&gt;master&lt;/code&gt; branch as we don&amp;rsquo;t want to try to retest charts as they&amp;rsquo;re merged in after successfully testing the new commit.:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;  lint-and-install:
    jobs:
      - lint-scripts
      - lint-charts:
          filters:
            branches:
              ignore: master
            tags:
              ignore: /.*/
      - install-charts:
          requires:
            - lint-charts
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;circleci-ct-yaml-https-github-com-paulczar-my-helm-charts-blob-part-3-circleci-ct-yaml&#34;&gt;&lt;a href=&#34;https://github.com/paulczar/my-helm-charts/blob/part-3/.circleci/ct.yaml&#34;&gt;./circleci/ct.yaml&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;This file provides configuration for Chart Testing. For now all we need is to tell it to provide &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; with a longer timeout:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;helm-extra-args: --timeout 600
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;circleci-kind-config-yaml-https-github-com-paulczar-my-helm-charts-blob-part-3-circleci-kind-config-yaml&#34;&gt;&lt;a href=&#34;https://github.com/paulczar/my-helm-charts/blob/part-3/.circleci/kind-config.yaml&#34;&gt;./circleci/kind-config.yaml&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;This file provides a configuration for KIND to use:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;kind: Cluster
apiVersion: kind.sigs.k8s.io/v1alpha3
nodes:
  - role: control-plane
  - role: worker
  - role: worker
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;circleci-install-charts-sh-https-github-com-paulczar-my-helm-charts-blob-part-3-circleci-install-charts-sh&#34;&gt;&lt;a href=&#34;https://github.com/paulczar/my-helm-charts/blob/part-3/.circleci/install_charts.sh&#34;&gt;./circleci/install_charts.sh&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;Finally this script will install KIND and will perform test installations for any changed &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Charts:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/usr/bin/env bash

set -o errexit
set -o nounset
set -o pipefail

readonly CT_VERSION=v2.3.3
readonly KIND_VERSION=0.2.1
readonly CLUSTER_NAME=chart-testing
readonly K8S_VERSION=v1.14.0

run_ct_container() {
    echo &#39;Running ct container...&#39;
    docker run --rm --interactive --detach --network host --name ct \
        --volume &amp;quot;$(pwd)/.circleci/ct.yaml:/etc/ct/ct.yaml&amp;quot; \
        --volume &amp;quot;$(pwd):/workdir&amp;quot; \
        --workdir /workdir \
        &amp;quot;quay.io/helmpack/chart-testing:$CT_VERSION&amp;quot; \
        cat
    echo
}

cleanup() {
    echo &#39;Removing ct container...&#39;
    docker kill ct &amp;gt; /dev/null 2&amp;gt;&amp;amp;1

    echo &#39;Done!&#39;
}

docker_exec() {
    docker exec --interactive ct &amp;quot;$@&amp;quot;
}

create_kind_cluster() {
    echo &#39;Installing kind...&#39;

    curl -sSLo kind &amp;quot;https://github.com/kubernetes-sigs/kind/releases/download/$KIND_VERSION/kind-linux-amd64&amp;quot;
    chmod +x kind
    sudo mv kind /usr/local/bin/kind

    kind create cluster --name &amp;quot;$CLUSTER_NAME&amp;quot; --config .circleci/kind-config.yaml --image &amp;quot;kindest/node:$K8S_VERSION&amp;quot; --wait 60s

    docker_exec mkdir -p /root/.kube

    echo &#39;Copying kubeconfig to container...&#39;
    local kubeconfig
    kubeconfig=&amp;quot;$(kind get kubeconfig-path --name &amp;quot;$CLUSTER_NAME&amp;quot;)&amp;quot;
    docker cp &amp;quot;$kubeconfig&amp;quot; ct:/root/.kube/config

    docker_exec kubectl cluster-info
    echo

    docker_exec kubectl get nodes
    echo
}

install_local_path_provisioner() {
    docker_exec kubectl delete storageclass standard
    docker_exec kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml
}

install_tiller() {
    echo &#39;Installing Tiller...&#39;
    docker_exec kubectl --namespace kube-system create sa tiller
    docker_exec kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller
    docker_exec helm init --service-account tiller --upgrade --wait
    echo
}

install_charts() {
    docker_exec ct install
    echo
}

main() {
    run_ct_container
    trap cleanup EXIT

    changed=$(docker_exec ct list-changed)
    if [[ -z &amp;quot;$changed&amp;quot; ]]; then
        echo &#39;No chart changes detected.&#39;
        return
    fi

    echo &#39;Chart changes detected.&#39;
    create_kind_cluster
    install_local_path_provisioner
    install_tiller
    install_charts
}

main
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;commit-the-changes&#34;&gt;Commit the changes&lt;/h3&gt;

&lt;p&gt;Next up commit these new changes to your master branch:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git add .
$ git commit -m &#39;add chart testing on PRs&#39;
$ git push origin master
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;test-the-new-automation&#34;&gt;Test the new Automation&lt;/h2&gt;

&lt;p&gt;Create a new branch:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git checkout -b update-app2-chart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Modify the app2 &lt;code&gt;Chart.yaml&lt;/code&gt; to be a new version number:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
appVersion: &amp;quot;1.0&amp;quot;
description: A Helm chart for Kubernetes
name: app2
version: 0.1.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Commit and Push the changes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git add charts/app2/Chart.yaml
$ git commit -m &#39;bump chart2 version&#39;
$ git push origin update-app2-chart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Circle CI should run tests and should fail:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;circle-test-fail.png&#34; alt=&#34;circle fail test&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This failure is because when &lt;code&gt;helm create&lt;/code&gt; creates your chart, it doesn&amp;rsquo;t implement all of our best practices. If you check in the Circle CI job log you&amp;rsquo;ll see:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;Error validating data /root/project/charts/app2/Chart.yaml with schema /etc/ct/chart_schema.yaml
  home: Required field missing
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The error is quite clear, we should have a field &lt;code&gt;home&lt;/code&gt; in our &lt;code&gt;Chart.yaml&lt;/code&gt;. In fact there should also be a &lt;code&gt;maintainers&lt;/code&gt; field. Let&amp;rsquo;s add those into both chart&amp;rsquo;s &lt;code&gt;Chart.yaml&lt;/code&gt; files:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;home: http://github.com/paulczar/my-helm-charts
maintainers:
  - name: paulczar
    email: username.taken@gmail.com
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: Since you&amp;rsquo;re also changing App1, you should bump its version a patch level to &lt;code&gt;0.1.2&lt;/code&gt;, all changes to a Chart, even non functional one should bump the chart version.&lt;/p&gt;

&lt;p&gt;Note: Ensure you leave a blank line at the end of the &lt;code&gt;Chart.yaml&lt;/code&gt; file. I forgot to and had to resubmit.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Push these new changes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git add .
$ git commit -m &#39;comply to chart best practices&#39;
$ git push origin update-app2-chart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After a few seconds you should see the new jobs start in CircleCI and this time all three tasks should complete successfully:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;circle-lint-and-install.png&#34; alt=&#34;circle ci lint and install&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It took about 6 minutes to run, because it did a full install of both Charts (as we changed them both) to a disposable KIND cluster.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: Since this was a branch, the charts were not released to the Chart Repository as that job is only triggered on the &lt;code&gt;master branch&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Next you&amp;rsquo;ll want to create a pull request for this change, you can do that via the &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; web ui:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;github-pull-request.png&#34; alt=&#34;github pull request&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: Since Circle CI has already tested the commits in this PR (Pull Request) it shows handy little test pass/fail marks against the commits.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Since the PR is showing as passing tests, you can go ahead and Merge it by clicking that green &lt;code&gt;Merge&lt;/code&gt; button (although I like to use &lt;code&gt;Squash and Merge&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;This Merge into the &lt;code&gt;master&lt;/code&gt; branch will kick off the &lt;code&gt;release-charts&lt;/code&gt; workflow and after a few seconds we&amp;rsquo;ll have an updated &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Repository:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ curl http://tech.paulcz.net/my-helm-charts/index.yaml
apiVersion: v1
entries:
...
...
  app1:
    name: app1
    version: 0.1.2
...
...
  app2:
    name: app2
    version: 0.1.1
...
...
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;testing-pull-requests&#34;&gt;Testing Pull Requests&lt;/h2&gt;

&lt;p&gt;In the advanced settings of Circle CI you can tell it to automatically test Pull Requests that come from forks of your &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; repository. Adding this is a great feature if you want others to work on your code with you. However you do need to protect your secrets.&lt;/p&gt;

&lt;p&gt;For example a bad actor could add &amp;ldquo;echo $CH_TOKEN&amp;rdquo; to one of the scripts and steal my &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; token which they could then use to mess with my Repositories.&lt;/p&gt;

&lt;p&gt;For that reason I&amp;rsquo;ve opted not to include that in this example.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In &lt;a href=&#34;https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-1&#34;&gt;Part 1&lt;/a&gt; we created set of &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Charts managed in source control (GitHub).&lt;/p&gt;

&lt;p&gt;In &lt;a href=&#34;https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-2&#34;&gt;Part 2&lt;/a&gt; we added automation via CircleCI to automate building and deploying Chart packages to a &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; Chart Repository hosted in &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Pages and &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; Releases.&lt;/p&gt;

&lt;p&gt;In &lt;strong&gt;Part 3&lt;/strong&gt; we added further automation to test changes in those &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; charts and to pass them through rigorous testing before allowing them to be merged into the &lt;code&gt;master&lt;/code&gt; branch.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Spring into Kubernetes - Using Kubernetes as a Config Server</title>
      <link>https://tech.paulcz.net/blog/spring-into-kubernetes-config-server/</link>
      <pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/spring-into-kubernetes-config-server/</guid>
      <description>In previous installments of Spring into Kubernetes I&amp;rsquo;ve shown you how to build images, deploy applications and write a Helm Chart for Spring applications. In this installment we&amp;rsquo;ll look at Spring Cloud Kubernetes integrations, specifically using Kubernetes Config Maps as a Config Server.
Usually I would use a Pivotal Container Service cluster to demonstrate, but in this demonstration I&amp;rsquo;ll use a local minikube cluster.
Spring Cloud Kubernetes Spring Cloud Kubernetes brings in a ton of integrations with Kubernetes.</description>
      <content>

&lt;p&gt;In previous installments of Spring into Kubernetes I&amp;rsquo;ve shown you how to &lt;a href=&#34;https://tech.paulcz.net/blog/building-spring-docker-images/&#34;&gt;build images&lt;/a&gt;, &lt;a href=&#34;https://tech.paulcz.net/blog/spring-into-kubernetes-part-1/&#34;&gt;deploy applications&lt;/a&gt; and write a &lt;a href=&#34;https://tech.paulcz.net/blog/spring-into-kubernetes-part-2/&#34;&gt;Helm Chart&lt;/a&gt; for Spring applications. In this installment we&amp;rsquo;ll look at &lt;a href=&#34;https://github.com/spring-cloud/spring-cloud-kubernetes&#34;&gt;Spring Cloud Kubernetes&lt;/a&gt; integrations, specifically using Kubernetes &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#create-configmaps-from-files&#34;&gt;Config Maps&lt;/a&gt; as a Config Server.&lt;/p&gt;

&lt;p&gt;Usually I would use a &lt;a href=&#34;https://pivotal.io/platform/pivotal-container-service&#34;&gt;Pivotal Container Service&lt;/a&gt; cluster to demonstrate, but in this demonstration I&amp;rsquo;ll use a local &lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-minikube/&#34;&gt;minikube&lt;/a&gt; cluster.&lt;/p&gt;

&lt;h2 id=&#34;spring-cloud-kubernetes&#34;&gt;Spring Cloud Kubernetes&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/spring-cloud/spring-cloud-kubernetes&#34;&gt;Spring Cloud Kubernetes&lt;/a&gt; brings in a ton of integrations with Kubernetes. This demonstration will focus just on the ability to integrate Kubernetes as a &lt;a href=&#34;https://github.com/spring-cloud/spring-cloud-kubernetes#kubernetes-propertysource-implementations&#34;&gt;configuration server&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Minimal changes are needed to your applications, you need to simply add the following classes to your &lt;code&gt;pom.xml&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;spring-boot-starter-actuator&amp;lt;/artifactId&amp;gt;
&amp;lt;/dependency&amp;gt;
&amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;org.springframework.cloud&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;spring-cloud-starter-kubernetes-config&amp;lt;/artifactId&amp;gt;
&amp;lt;/dependency&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You also need to enable it in your &lt;code&gt;bootstrap.yaml&lt;/code&gt; (or &lt;code&gt;.properties&lt;/code&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;spring.cloud.kubernetes.config.enabled: true
spring.cloud.kubernetes.reload.enabled: true
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;getting-started&#34;&gt;Getting Started&lt;/h2&gt;

&lt;p&gt;You&amp;rsquo;ll need to &lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-minikube/&#34;&gt;install minikube&lt;/a&gt; by following the instructions provided for your Operating System. You&amp;rsquo;ll also need &lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-kubectl/&#34;&gt;kubectl&lt;/a&gt; so that you can give instructions to Kubernetes.&lt;/p&gt;

&lt;p&gt;Start Minikube:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ minikube start
😄  minikube v0.34.1 on linux (amd64)
💡  Tip: Use &#39;minikube start -p &amp;lt;name&amp;gt;&#39; to create a new cluster, or &#39;minikube delete&#39; to delete this one.
🔄  Restarting existing virtualbox VM for &amp;quot;minikube&amp;quot; ...
⌛  Waiting for SSH access ...
📶  &amp;quot;minikube&amp;quot; IP address is 192.168.99.103
🐳  Configuring Docker as the container runtime ...
✨  Preparing Kubernetes environment ...
🚜  Pulling images required by Kubernetes v1.13.3 ...
🔄  Relaunching Kubernetes v1.13.3 using kubeadm ...
⌛  Waiting for kube-proxy to come back up ...
🤔  Verifying component health ......
💗  kubectl is now configured to use &amp;quot;minikube&amp;quot;
🏄  Done! Thank you for using minikube!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ensure that you can communicate with minikube:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl get nodes
NAME       STATUS   ROLES    AGE   VERSION
minikube   Ready    master   36h   v1.13.3
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;deploy-spring-hello-world&#34;&gt;Deploy Spring Hello World&lt;/h2&gt;

&lt;p&gt;The source for this demo can be found at &lt;a href=&#34;https://github.com/paulczar/spring-helloworld&#34;&gt;paulczar/spring-hello&lt;/a&gt; on github. Of importance it will respond to a web request with the contents of the application property &lt;code&gt;message&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Deploy the example Hello World application and expose it via a service:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl run hello --image=paulczar/spring-hello:k8s001 --port=8080
deployment.apps/hello created

$ kubectl expose deployment hello --type=LoadBalancer --port 80 --target-port 8080
service/hello exposed
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can use &lt;code&gt;minikube service list&lt;/code&gt; to get a list of services and the URLs for those services. This helps make up for the lack of LoadBalancer support in minikube:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ minikube service list
|-------------|------------|-----------------------------|
|  NAMESPACE  |    NAME    |             URL             |
|-------------|------------|-----------------------------|
| default     | hello      | http://192.168.99.103:30871 |
| default     | kubernetes | No node port                |
| kube-system | kube-dns   | No node port                |
|-------------|------------|-----------------------------|
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Use the URL provided for the &lt;code&gt;hello&lt;/code&gt; service:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl http://192.168.99.103:30871
hello development
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our application is running and responding with &lt;code&gt;hello development&lt;/code&gt;. This is the default value for &lt;code&gt;message&lt;/code&gt; in the &lt;code&gt;development&lt;/code&gt; spring profile.&lt;/p&gt;

&lt;h2 id=&#34;configure-kubernetes-support-for-spring-hello-world&#34;&gt;Configure Kubernetes support for Spring Hello World&lt;/h2&gt;

&lt;p&gt;If you have &lt;code&gt;rbac&lt;/code&gt; enabled in your cluster (which you should, we&amp;rsquo;re not animals) the service account your application is running and will be unable to view kubernetes resources. You can see these errors in the pod&amp;rsquo;s logs:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;k logs deployment/hello
2019-03-01 16:13:02.629  WARN 1 --- [           main] o.s.cloud.kubernetes.StandardPodUtils    : Failed to get pod with name:[hello-bb9cf575d-rqt6n]. You should look into this if things aren&#39;t working as you expect. Are you missing serviceaccount permissions?

io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: GET at: https://10.96.0.1/api/v1/namespaces/default/pods/hello-bb9cf575d-rqt6n. Message: Forbidden!Configured service account doesn&#39;t have access. Service account may have been revoked. pods &amp;quot;hello-bb9cf575d-rqt6n&amp;quot; is forbidden: User &amp;quot;system:serviceaccount:default:default&amp;quot; cannot get resource &amp;quot;pods&amp;quot; in API group &amp;quot;&amp;quot; in the namespace &amp;quot;default&amp;quot;.
...
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You could create a new service user and give it the appropriate permissions, or you could give permissions to the default service account. Do the latter using a kubernetes manifest found &lt;a href=&#34;https://raw.githubusercontent.com/paulczar/spring-helloworld/master/deploy/rbac.yaml&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Update the &lt;code&gt;rolebinding&lt;/code&gt; for the default user:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl apply -f https://raw.githubusercontent.com/paulczar/spring-helloworld/master/deploy/rbac.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Delete the current pod to have the kubernetes deployment start a new one which should now have permissions:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl get pods
NAME                    READY   STATUS    RESTARTS   AGE
hello-bb9cf575d-rqt6n   1/1     Running   0          12m

$ kubectl delete pod hello-bb9cf575d-rqt6n
pod &amp;quot;hello-bb9cf575d-rqt6n&amp;quot; deleted

$ kubectl get pods
NAME                    READY   STATUS    RESTARTS   AGE
hello-bb9cf575d-8mwxq   1/1     Running   0          21s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you look at the logs you&amp;rsquo;ll see the error has disappeared and you can see it is looking for a &lt;code&gt;configmap&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl logs deployment/hello
2019-03-01 16:26:33.080 DEBUG 1 --- [           main] o.s.cloud.kubernetes.config.ConfigUtils  : Config Map name has not been set, taking it from property/env spring.application.name (default=application)
2019-03-01 16:26:33.080 DEBUG 1 --- [           main] o.s.cloud.kubernetes.config.ConfigUtils  : Config Map namespace has not been set, taking it from client (ns=default)
2019-03-01 16:26:33.188  INFO 1 --- [           main] b.c.PropertySourceBootstrapConfiguration : Located property source: CompositePropertySource {name=&#39;composite-configmap&#39;, propertySources=[ConfigMapPropertySource@872306601 {name=&#39;configmap.hello.default&#39;, properties={}}]}

Since you haven&#39;t yet created a `configmap` the response from the application should still be the default:

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;console
$ curl &lt;a href=&#34;http://192.168.99.103:30871&#34;&gt;http://192.168.99.103:30871&lt;/a&gt;
hello development&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
Next create a `configmap` that the application will use, by default it will look for a `configmap` with the same name as the application:

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;console
$ kubectl create configmap hello &amp;ndash;from-literal=message=&amp;ldquo;HELLO KUBERNETES&amp;rdquo;
configmap/hello created&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
Run the `curl` command again and you should see the new response:

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;console
$ curl &lt;a href=&#34;http://192.168.99.103:30871&#34;&gt;http://192.168.99.103:30871&lt;/a&gt;
HELLO KUBERNETES
```&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;While this was a fairly simple demonstration of the Spring Cloud Kubernetes integrations you can see how useful it can be. By integrating directly into Kubernetes you can avoid running a &lt;a href=&#34;https://spring.io/projects/spring-cloud-config&#34;&gt;Spring Cloud Config&lt;/a&gt; service to get dynamic configuration of your application.&lt;/p&gt;

&lt;p&gt;You can also load up an entire application properties file (either &lt;code&gt;.properties&lt;/code&gt; or &lt;code&gt;.yaml&lt;/code&gt;) inside a &lt;code&gt;configmap&lt;/code&gt;, you can also store passwords and keys in a Kubernetes &lt;code&gt;secret&lt;/code&gt; and dynamically load those.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Spring into Kubernetes - Deploying with Helm</title>
      <link>https://tech.paulcz.net/blog/spring-into-kubernetes-part-2/</link>
      <pubDate>Thu, 07 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/spring-into-kubernetes-part-2/</guid>
      <description>Introduction In this installment of Spring into Kubernetes we&amp;rsquo;ll be looking at using Helm to install our Spring application to Kubernetes.
About Helm Helm is the package manager for Kubernetes. It provides tooling to create, template, package, and share Kubernetes manifests. A helm chart is effectively a signed tarball that contains a set of templated Kubernetes manifests, a metadata file and a set of default values.
Helm has two major components, the Helm client and the Tiller server.</description>
      <content>

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;In this installment of Spring into Kubernetes we&amp;rsquo;ll be looking at using &lt;a href=&#34;https://helm.sh/&#34;&gt;Helm&lt;/a&gt; to install our Spring application to Kubernetes.&lt;/p&gt;

&lt;h1 id=&#34;about-helm&#34;&gt;About Helm&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://helm.sh/&#34;&gt;Helm&lt;/a&gt; is the package manager for Kubernetes. It provides tooling to create, template, package, and &lt;a href=&#34;https://hub.helm.sh/&#34;&gt;share&lt;/a&gt; Kubernetes manifests. A helm chart is effectively a signed tarball that contains a set of templated Kubernetes manifests, a metadata file and a set of default values.&lt;/p&gt;

&lt;p&gt;Helm has two major components, the Helm client and the Tiller server. The Helm client is a CLI tool that you use to create, package and deploy helm charts. The Tiller server is installed into your Kubernetes cluster and is responsible for managing the lifecycle of your applications as instructed by the Helm client.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: The Tiller server was created before Kubernetes extensions existed and will be removed in Helm 3 in favor of utilizing Kubernetes extensions. You can also use &amp;ldquo;tillerless&amp;rdquo; Helm but that&amp;rsquo;s out of scope for this blog post.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Helm charts can be shared via a Helm Repository which gives you an easy way to upload, share, and download packages from a central location. The public &lt;a href=&#34;https://hub.helm.sh/&#34;&gt;Helm Hub&lt;/a&gt; has prebuilt Helm packages for most common open source applications.&lt;/p&gt;

&lt;h1 id=&#34;step-1-install-helm&#34;&gt;Step 1 - Install Helm&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.helm.sh/using_helm/#installing-helm&#34;&gt;Installing Helm&lt;/a&gt; is quite simple, if you&amp;rsquo;re on a Mac you can install Helm via Homebrew, otherwise check the &lt;a href=&#34;https://docs.helm.sh/using_helm/#installing-helm&#34;&gt;Helm install documentation&lt;/a&gt; for platform specific instructions:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;brew install kubernetes-helm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With the Helm client installed you can install the Tiller server to your Kubernetes cluster.&lt;/p&gt;

&lt;p&gt;If you are using minikube or a single tenant Kubernetes cluster without Role Based Authentication Control (RBAC) enabled you can deploy Tiller by simply running &lt;code&gt;helm init&lt;/code&gt;. On most clusters you should create a service account and role binding first like so:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;kubectl -n kube-system create serviceaccount tiller
kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller
helm init --service-account=tiller
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After a minute or so you can check it has finished installing by running &lt;code&gt;helm version&lt;/code&gt; which will give you the version of both the client and the server. If the server doesn&amp;rsquo;t respond just want a few more moments and try again:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm version
Client: &amp;amp;version.Version{SemVer:&amp;quot;v2.12.2&amp;quot;, GitCommit:&amp;quot;7d2b0c73d734f6586ed222a567c5d103fed435be&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;}
Server: &amp;amp;version.Version{SemVer:&amp;quot;v2.12.2&amp;quot;, GitCommit:&amp;quot;7d2b0c73d734f6586ed222a567c5d103fed435be&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;step-2-create-and-deploy-a-new-helm-chart&#34;&gt;Step 2 - Create and Deploy a new Helm Chart&lt;/h1&gt;

&lt;p&gt;Use the Helm client to create a new Helm chart:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm create petclinic
Creating petclinic

$ cd petclinic

$ tree
├── charts
├── Chart.yaml
├── templates
│   ├── deployment.yaml
│   ├── _helpers.tpl
│   ├── ingress.yaml
│   ├── NOTES.txt
│   ├── service.yaml
│   └── tests
│       └── test-connection.yaml
└── values.yaml

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can see that the Helm chart sets up a boilerplate Helm chart to get you started. This is actually a fully functioning chart so you can go ahead and deploy it right away to ensure that everything is working correctly. You could simply run &lt;code&gt;helm install .&lt;/code&gt; but generally you&amp;rsquo;ll want to specify a namespace and release name by using the &lt;code&gt;--namespace&lt;/code&gt; and &lt;code&gt;--name&lt;/code&gt; for better management.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm install --namespace test --name test .
NAME:   test
LAST DEPLOYED: Tue Feb  5 06:26:19 2019
NAMESPACE: test
STATUS: DEPLOYED
...
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can use &lt;code&gt;kubectl port-forward&lt;/code&gt; to test that everything worked right:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl --namespace test port-forward deployment/test-petclinic 8080:80
Visit http://127.0.0.1:8080 to use your application
Forwarding from [::1]:8080 -&amp;gt; 80
Forwarding from 127.0.0.1:8080 -&amp;gt; 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Point your browser at &lt;a href=&#34;http://localhost:8080&#34;&gt;http://localhost:8080&lt;/a&gt; and you should see the following:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./welcome-nginx.png&#34; alt=&#34;Nginx Welcome Screen&#34; /&gt;&lt;/p&gt;

&lt;p&gt;But this isn&amp;rsquo;t Petclinic it&amp;rsquo;s an empty nginx container. Since our petclinic app is pretty simple we can use this boilerplate chart to deploy Petclinic with just a few minor changes.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The nginx app inside the container listens on port 80, petclinic listens on port 8080 so you&amp;rsquo;ll need to edit the file &lt;code&gt;templates/deployment.yaml&lt;/code&gt; and find the YAML &lt;code&gt;containerPort: 80&lt;/code&gt; under the container spec and change the value to &lt;code&gt;8080&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Also in the &lt;code&gt;templates/deployment.yaml&lt;/code&gt; file you want to increase the initial timeout for the &lt;code&gt;livenessCheck&lt;/code&gt; check as Java takes longer to be ready than nginx. Find the YAML key &lt;code&gt;livenessProbe:&lt;/code&gt; and add the keypair &lt;code&gt;initialDelaySeconds: 60&lt;/code&gt; to it.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The updated section of the &lt;code&gt;templates/deployment.yaml&lt;/code&gt; should look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;    spec:
      containers:
        - name: {{ .Chart.Name }}
          image: &amp;quot;{{ .Values.image.repository }}:{{ .Values.image.tag }}&amp;quot;
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          livenessProbe:
            initialDelaySeconds: 60
            httpGet:
              path: /
              port: http

&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: the double curly brace &lt;code&gt;{{ }}&lt;/code&gt; signifies for the golang templating engine to process some code, usually print out some values, but can also be used for if/then statements and loops.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You also need to set the image and tag to petclinic. You could edit &lt;code&gt;values.yaml&lt;/code&gt; to change the defaults, but you can also override default values with &lt;code&gt;--set&lt;/code&gt;. Since we&amp;rsquo;ve already deployed the helm chart we can &lt;em&gt;upgrade&lt;/em&gt; it by running a &lt;code&gt;helm upgrade&lt;/code&gt; and providing the appropriate &lt;code&gt;--set&lt;/code&gt; flag like so:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: &lt;code&gt;helm upgrade&lt;/code&gt; helps you to manage the release lifecycle of your application, Tiller keeps track of the releases that you&amp;rsquo;ve deployed and helps you both upgrade and rollback deployments.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm upgrade test \
  --set image.repository=paulczar/spring-petclinic \
  --set image.tag=latest .
Release &amp;quot;test&amp;quot; has been upgraded. Happy Helming!
LAST DEPLOYED: Tue Feb  5 06:54:07 2019
...
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Forward port 8080 through to the deployment again and check its working via your web browser:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: the command shown in the &lt;code&gt;helm upgrade&lt;/code&gt; output is wrong because we&amp;rsquo;re using port 8080 now, you could fix the output by editing the file &lt;code&gt;templates/NOTES.txt&lt;/code&gt;, but for now just run the command below.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl --namespace test port-forward deployment/test-petclinic 8080
Forwarding from [::1]:8080 -&amp;gt; 8080
Forwarding from 127.0.0.1:8080 -&amp;gt; 8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./pc-localhost.png&#34; alt=&#34;petclinic&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;step-3-cleanup&#34;&gt;Step 3 - Cleanup&lt;/h1&gt;

&lt;p&gt;To uninstall the helm chart simple run the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm delete --purge test
release &amp;quot;test&amp;quot; deleted
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;step-4-optional-package-your-helm-chart&#34;&gt;Step 4 (optional) - Package your Helm Chart&lt;/h1&gt;

&lt;p&gt;Helm charts are designed to be packaged and shared via a Helm repository. Simply running &lt;code&gt;helm package .&lt;/code&gt; will create the Helm package like so:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ mkdir pkg
$ helm package -d pkg .
Successfully packaged chart and saved it to: /tmp/petclinic/petclinic-0.1.0.tgz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These packages combined with an index file can be hosted on any static website (github pages is very common) to create a Helm Repository. You can generate the index file with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm repo index pkg
$ cat pkg/index.yaml
apiVersion: v1
entries:
  petclinic:
  - apiVersion: v1
    appVersion: &amp;quot;1.0&amp;quot;
    created: 2019-02-06T02:45:10.99990157-06:00
    description: A Helm chart for Kubernetes
    digest: 86f3740e6bc325ea330428e42af091d6613ca9b92678b3aecdf680f0302b4685
    name: petclinic
    urls:
    - petclinic-0.1.0.tgz
    version: 0.1.0
generated: 2019-02-06T02:45:10.99955354-06:00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can use any basic web server as a Helm repository, many folks are using &lt;a href=&#34;https://github.com/int128/helm-github-pages&#34;&gt;github pages&lt;/a&gt; which is quite a clever way to host your Helm Repository right next to the actual code for your Helm chart.&lt;/p&gt;

&lt;h1 id=&#34;step-5-use-a-helm-repository&#34;&gt;Step 5 - Use a Helm Repository&lt;/h1&gt;

&lt;p&gt;I have created a more featureful Helm Chart designed specifically to generically run most Spring applications which can be found on my github repo &lt;a href=&#34;https://github.com/paulczar/helm-chart-spring&#34;&gt;helm-chart-spring&lt;/a&gt;. It installs petclinic by default and has options to be able to automatically set up LoadBalancers and Ingress as well as more advanced kubernetes resources such as &lt;code&gt;podDisruptionBudgets&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;You can install it straight from my Helm Repository like so:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm repo add paulczar http://tech.paulcz.net/helm-chart-spring/repo
&amp;quot;paulczar&amp;quot; has been added to your repositories

$ helm install --namespace test --name test \
    paulczar/spring --set service.type=LoadBalancer \
    --version 0.0.6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After a few moments you should be able to access the petclinic app via the LoadBalancer created by the above command.  Run &lt;code&gt;kubectl get svc&lt;/code&gt; to find the IP:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ k get svc
NAME          TYPE           CLUSTER-IP       EXTERNAL-IP      PORT(S)        AGE
test-spring   LoadBalancer   10.100.200.137   35.238.37.241   80:30509/TCP   89s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Point your browser at the &lt;code&gt;EXTERNAL-IP&lt;/code&gt; value:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./kube-lb-pc.png&#34; alt=&#34;petclinic&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Uninstall the Helm chart:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm delete --purge test
release &amp;quot;test&amp;quot; deleted
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Helm has made it incredibly easy to get started creating and customizing charts to install Applications on Kubernetes. By templating and packaging your Kubernetes manifests you get an easy to share package for your application and are able to enable a lot of deployment flexibility to deploy multiple releases of your application to one or many Kubernetes clusters and customize things like the service type giving you tremendous control over how people access it.&lt;/p&gt;

&lt;p&gt;Better yet, if you are looking to install an open source application you should first look to the &lt;a href=&#34;https://hub.helm.sh/&#34;&gt;Helm Hub&lt;/a&gt; which is a public repository of hundreds of pre-packaged helm charts that install with just a few keypresses.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Cloud Native Operations - Kubernetes Controllers</title>
      <link>https://tech.paulcz.net/blog/cloud-native-operations-k8s-controllers/</link>
      <pubDate>Tue, 29 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/cloud-native-operations-k8s-controllers/</guid>
      <description>Note: If this topic has peaked your interest, you can join me for a Webinar on August 15 where I&amp;rsquo;ll dive deep into Cloud Native Operations with Kubernetes and CI/CD Pipelines.
 Lulz what? Cloud Native Operations ?!?!?! Historically Operations practices have lagged behind development. During the 90s a number of lightweight software development practices evolved such as Scrum and Extreme Programming. During the early 2000&amp;rsquo;s it became pretty common to practice (or at least claim to) some form of Agile in software development.</description>
      <content>

&lt;blockquote&gt;
&lt;p&gt;Note: If this topic has peaked your interest, you can join me for a Webinar on August 15 where I&amp;rsquo;ll dive deep into &lt;a href=&#34;https://content.pivotal.io/webinars/aug-15-cloud-native-operations-with-kubernetes-and-ci-cd-webinar?utm_campaign=cno-k8s-ci-cd-q319&amp;amp;utm_source=blog&amp;amp;utm_medium=website&#34;&gt;Cloud Native Operations with Kubernetes and CI/CD Pipelines&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;lulz-what-cloud-native-operations&#34;&gt;Lulz what? Cloud Native Operations ?!?!?!&lt;/h1&gt;

&lt;p&gt;Historically Operations practices have lagged behind development. During the 90s a number of lightweight software development practices evolved such as Scrum and Extreme Programming. During the early 2000&amp;rsquo;s it became pretty common to practice (or at least claim to) some form of Agile in software development.&lt;/p&gt;

&lt;p&gt;It wasn&amp;rsquo;t until the last year of that decade that we started to see an uptick in Operations folks wanting to adopt Agile type methodologies and as the devops (and later SRE) movements took off we started to borrow heavily from Lean principals such as Kanban and Value Stream Mapping.&lt;/p&gt;

&lt;p&gt;Cloud Computing has brought about another shift in software development, going from large monolithic applications to collections of microservices that work together, and even further into being event based via messages and streams which now falls under the umbrella of &amp;ldquo;cloud native&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;With the rise of Kubernetes and similar platforms as well as companies like Hashicorp and stalwarts of Agile Operations such as Google and Pivotal we&amp;rsquo;re starting to see that same shift in Operations as we start to talk about &lt;strong&gt;Platform as Product&lt;/strong&gt; and turning engineering [operations] teams into product teams.&lt;/p&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;this means we all need to learn to get better at product engineering, kittens.  and turning infra engineering teams into infra product teams. &lt;a href=&#34;https://t.co/t1Vj6RKbdd&#34;&gt;pic.twitter.com/t1Vj6RKbdd&lt;/a&gt;&lt;/p&gt;&amp;mdash; Charity Majors (@mipsytipsy) &lt;a href=&#34;https://twitter.com/mipsytipsy/status/1088673759291011073?ref_src=twsrc%5Etfw&#34;&gt;January 25, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;h1 id=&#34;kubernetes-controllers&#34;&gt;Kubernetes Controllers&lt;/h1&gt;

&lt;p&gt;There&amp;rsquo;s a lot more to be said about Cloud Native Operations and Platform as Product (the two go hand-in-hand) but for now I want to focus on a fundamental aspect of Kubernetes that will be a force multiplier for making the composable building blocks of Cloud Native Operations.&lt;/p&gt;

&lt;p&gt;Most resources in Kubernetes are managed by a Controller. A Kubernetes Controller is to microservices what a Chef recipe is to a Monolith.&lt;/p&gt;

&lt;p&gt;Each resource is controlled by its own control loop. This is a step forward from previous systems like Chef or Puppet which both have control loops but at the server level, not the resource.&lt;/p&gt;

&lt;p&gt;A Controller is a fairly simple piece of code that creates a control loop over a single resource to ensure that resource is behaving correctly. These Control loops cam stack together to create complex functionality with simple interfaces.&lt;/p&gt;

&lt;p&gt;The canonical example of this in action is in how we manage pods in Kubernetes. A Pod is [effectively] a running copy of your application that a specific worker node is asked to run. If that application crashes the kubelet running on that node will start it again.&lt;/p&gt;

&lt;p&gt;However if that node crashes the Pod is not recovered as the control loop (via the kubelet process) responsible for the resource no longer exists. To make applications more resiliant Kubernetes has the ReplicaSet controller.&lt;/p&gt;

&lt;p&gt;Kubernetes has a process running on the masters called a &lt;code&gt;controller-manager&lt;/code&gt; that run the controllers for these more advanced resources. This is where the ReplicaSet controller runs, and it is responsible for ensuring that a set number of copies of your application are always running.&lt;/p&gt;

&lt;p&gt;To do this the ReplicaSet controller requests that the provided number of Pods are created and then it routinely checks that the correct number of Pods are still running and will request more pods, or destroy existing pods to do so.&lt;/p&gt;

&lt;p&gt;By requesting a ReplicaSet from Kubernetes you get a self-healing deployment of your application. You can further add lifecycle management to your workload by requesting a Deployment which is a controller that manages ReplicaSets.&lt;/p&gt;

&lt;p&gt;These Controllers are great for managing Kubernetes resources, but are also fantastic for managing resources outside of Kubernetes. You can extend Kubernetes by writing a Controller that watches for events and annotations and performs extra work, or by writing a Custom Resource Definition.&lt;/p&gt;

&lt;h1 id=&#34;example-external-dns-controller&#34;&gt;Example - External DNS Controller&lt;/h1&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/kubernetes-incubator/external-dns&#34;&gt;external-dns&lt;/a&gt; controller is a perfect example of a watcher. You configure it with your DNS provider and it will watch resources such as Services and Ingresses. When one of those resources changes it will inspect them for annotations which will tell it if it needs to perform an action.&lt;/p&gt;

&lt;p&gt;With the &lt;code&gt;external-dns&lt;/code&gt; controller running in your cluster you can simply add the following annotation to a service and it will go out and create a matching DNS A record for that resource:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;kubectl annotate service nginx \
    &amp;quot;external-dns.alpha.kubernetes.io/hostname=nginx.example.org.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can change other characteristics such as the TTL value of the DNS record:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;kubectl annotate service nginx \
    &amp;quot;external-dns.alpha.kubernetes.io/ttl=10&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Just like that you now have automatic DNS management for your applications and services in Kubernetes that reacts to any changes in your cluster to ensure your DNS is correct.&lt;/p&gt;

&lt;h1 id=&#34;example-certificate-manager-operator&#34;&gt;Example - Certificate Manager Operator&lt;/h1&gt;

&lt;p&gt;Like the &lt;code&gt;external-dns&lt;/code&gt; controller the &lt;a href=&#34;http://docs.cert-manager.io/en/latest/&#34;&gt;cert-manager&lt;/a&gt; will react to changes in resources, but also comes with a Custom Resource Definition that will allow you to request certificates as a resource in of themselves, not just a byproduct of an annotation.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;cert-manager&lt;/code&gt; works with &lt;a href=&#34;https://letsencrypt.org/&#34;&gt;Lets Encrypt&lt;/a&gt; and other sources of Certificates to request valid signed TLS certificates. You can even use it in combination with &lt;code&gt;external-dns&lt;/code&gt; like the following which will register &lt;code&gt;web.example.com&lt;/code&gt; and retrieve a TLS certificate from Lets Encrypt and store that in a Secret.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    certmanager.k8s.io/acme-http01-edit-in-place: &amp;quot;true&amp;quot;
    certmanager.k8s.io/cluster-issuer: letsencrypt-prod
    kubernetes.io/tls-acme: &amp;quot;true&amp;quot;
  name: example
spec:
  rules:
  - host: web.example.com
    http:
      paths:
      - backend:
          serviceName: example
          servicePort: 80
        path: /*
  tls:
  - hosts:
    - web.example.com
    secretName: example-tls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can also request a certificate directly from the &lt;code&gt;cert-manager&lt;/code&gt; CRD like so which like above will result in a certificate keypair being stored in a Kubernetes secret:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: certmanager.k8s.io/v1alpha1
kind: Certificate
metadata:
  name: example-com
  namespace: default
spec:
  secretName: example-com-tls
  issuerRef:
    name: letsencrypt-staging
  commonName: example.com
  dnsNames:
  - www.example.com
  acme:
    config:
    - http01:
        ingressClass: nginx
      domains:
      - example.com
    - http01:
        ingress: my-ingress
      domains:
      - www.example.com
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;This was just a quick look at one of the ways that Kubernetes is helping enable a new wave of changes to how we operate software. This is a favorite topic of mine, so look forward to hearing more.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Building Spring Docker Images</title>
      <link>https://tech.paulcz.net/blog/building-spring-docker-images/</link>
      <pubDate>Fri, 25 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/building-spring-docker-images/</guid>
      <description>While investigating running Spring applications on Kubernetes I discovered that a lot of the existing example Spring applications do not have a Dockerfile in their git repository. I thought this odd at first (and frankly still do).
What I discovered though, is there&amp;rsquo;s quite a number of ways to build Spring (and Java in general) container images that don&amp;rsquo;t necessarily rely on writing a Dockerfile.
Full disclosure, I am a firm believe that any opensource project of consequence (where feasible) should ship a Dockerfile in their git repo, and ideally have images up on the Docker hub (or other public container registry) as it allows for newcomers to experience your application or project in just a few seconds with no need to play detective to try and figure out how to get it running.</description>
      <content>

&lt;p&gt;While investigating running &lt;a href=&#34;https://spring.io&#34;&gt;Spring&lt;/a&gt; applications on Kubernetes I discovered that a lot of the existing example Spring applications do not have a &lt;code&gt;Dockerfile&lt;/code&gt; in their git repository. I thought this odd at first (and frankly still do).&lt;/p&gt;

&lt;p&gt;What I discovered though, is there&amp;rsquo;s quite a number of ways to build &lt;a href=&#34;https://spring.io/guides/gs/spring-boot-docker/&#34;&gt;Spring (and Java in general) container images&lt;/a&gt; that don&amp;rsquo;t necessarily rely on writing a Dockerfile.&lt;/p&gt;

&lt;p&gt;Full disclosure, I am a firm believe that any opensource project of consequence (where feasible) should ship a Dockerfile in their git repo, and ideally have images up on the Docker hub (or other public container registry) as it allows for newcomers to experience your application or project in just a few seconds with no need to play detective to try and figure out how to get it running.&lt;/p&gt;

&lt;p&gt;I will demonstrate building the &lt;a href=&#34;https://github.com/spring-projects/spring-petclinic&#34;&gt;Spring Pet Clinic example application&lt;/a&gt; into container images.&lt;/p&gt;

&lt;p&gt;If you want to follow along at home start by cloning down the repo to your local machine:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;git clone https://github.com/spring-projects/spring-petclinic.git

cd spring-petclinic
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;option-1-dockerfile&#34;&gt;Option 1 - Dockerfile&lt;/h1&gt;

&lt;p&gt;The Pet Clinic app uses Maven to build a .jar file, so we have a few options here.&lt;/p&gt;

&lt;h2 id=&#34;build-jar-and-then-copy-it-into-a-java-image&#34;&gt;Build .jar and then copy it into a Java Image&lt;/h2&gt;

&lt;p&gt;This assumes that you have a suitable version of Java and Maven on your system.&lt;/p&gt;

&lt;p&gt;Start by building the project into a .jar file with Maven:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ mvn install -DskipTests
[INFO] Installing /home/pczarkowski/development/demo/spring-into-kubernetes-1/spring-petclinic/target/spring-petclinic-2.1.0.BUILD-SNAPSHOT.jar to /home/pczarkowski/.m2/repository/org/springframework/samples/spring-petclinic/2.1.0.BUILD-SNAPSHOT/spring-petclinic-2.1.0.BUILD-SNAPSHOT.jar
[INFO] Installing /home/pczarkowski/development/demo/spring-into-kubernetes-1/spring-petclinic/pom.xml to /home/pczarkowski/.m2/repository/org/springframework/samples/spring-petclinic/2.1.0.BUILD-SNAPSHOT/spring-petclinic-2.1.0.BUILD-SNAPSHOT.pom
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 26.984 s
[INFO] Finished at: 2019-01-25T09:23:11-06:00
[INFO] ------------------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see this resulted in a Java file &lt;code&gt;spring-petclinic-2.1.0.BUILD-SNAPSHOT.jar&lt;/code&gt;.  We can create a Dockerfile to ingest this called &lt;code&gt;Dockerfile.cp&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;FROM openjdk:11.0.1-jre-slim-stretch
EXPOSE 8080
ARG JAR=spring-petclinic-2.1.0.BUILD-SNAPSHOT.jar
COPY target/$JAR /app.jar
ENTRYPOINT [&amp;quot;java&amp;quot;,&amp;quot;-jar&amp;quot;,&amp;quot;/app.jar&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: because we already built the Jar we only need a slim JRE image to run it in. We can also use an ARG for the file name in case we need to change it on build with &lt;code&gt;--build-arg JAR=...&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A simple &lt;code&gt;docker build&lt;/code&gt; command should create us an image we can run:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;docker build -f ./Dockerfile.cp -t spring/petclinic .
Sending build context to Docker daemon  98.22MB
Step 1/5 : FROM openjdk:11.0.1-jre-slim-stretch
11.0.1-jre-slim-stretch: Pulling from library/openjdk
5e6ec7f28fb7: Pull complete
1cf4e4a3f534: Pull complete
5d9d21aca480: Pull complete
0a126fb8ec28: Pull complete
1904df324545: Pull complete
e6d9d96381c8: Pull complete
Digest: sha256:965a07951bee0c3b1f8aff4818619ace3e675d91cfb746895e8fb84e3e6b13ca
Status: Downloaded newer image for openjdk:11.0.1-jre-slim-stretch
 ---&amp;gt; 49b31a72a85a
Step 2/5 : EXPOSE 8080
 ---&amp;gt; Running in 1aeaae727a80
Removing intermediate container 1aeaae727a80
 ---&amp;gt; a1a1850f8e8f
Step 3/5 : ARG JAR=spring-petclinic-2.1.0.BUILD-SNAPSHOT.jar
 ---&amp;gt; Running in b6faa7c0faa3
Removing intermediate container b6faa7c0faa3
 ---&amp;gt; 2b55681ac9df
Step 4/5 : COPY target/$JAR /app.jar
 ---&amp;gt; dec4f0d56c9d
Step 5/5 : ENTRYPOINT [&amp;quot;java&amp;quot;,&amp;quot;-jar&amp;quot;,&amp;quot;/app.jar&amp;quot;]
 ---&amp;gt; Running in f492e1668fff
Removing intermediate container f492e1668fff
 ---&amp;gt; f669afd61b8d
Successfully built f669afd61b8d
Successfully tagged spring/petclinic:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Start the new container, wait a minute or so (you can watch the logs with &lt;code&gt;docker logs -f petclinic&lt;/code&gt; if you want), and then test it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d --name petclinic -p 8080:8080 spring/petclinic
a1d51b6f9a47501dfe90f24866e7fb6c82e436323fa4adc09074e8ac7447a1a7

$ curl -s localhost:8080 | head
&amp;lt;!DOCTYPE html&amp;gt;

&amp;lt;html&amp;gt;

  &amp;lt;head&amp;gt;

    &amp;lt;meta http-equiv=&amp;quot;Content-Type&amp;quot; content=&amp;quot;text/html; charset=UTF-8&amp;quot;/&amp;gt;
    &amp;lt;meta charset=&amp;quot;utf-8&amp;quot;&amp;gt;
    &amp;lt;meta http-equiv=&amp;quot;X-UA-Compatible&amp;quot; content=&amp;quot;IE=edge&amp;quot;&amp;gt;
    &amp;lt;meta name=&amp;quot;viewport&amp;quot; content=&amp;quot;width=device-width, initial-scale=1&amp;quot;&amp;gt;

$ docker rm -f petclinic
petclinic
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can look at the resultant image size using &lt;code&gt;docker images&lt;/code&gt;, if you want to dive deeper you can also use &lt;code&gt;docker inspect&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ docker images spring/petclinic
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
spring/petclinic    latest              f669afd61b8d        41 minutes ago      318MB
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;use-a-multi-stage-dockerfile&#34;&gt;Use a multi-stage Dockerfile&lt;/h2&gt;

&lt;p&gt;If you don&amp;rsquo;t have Java and Maven on your system, or you want to delegate the whole thing to Docker you can utilize a multi-stage Dockerfile to build the .jar file and then copy it into a slim image.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-dockerfile&#34;&gt;FROM maven:3.6-jdk-11-slim as BUILD
COPY . /src
WORKDIR /src
RUN mvn install -DskipTests

FROM openjdk:11.0.1-jre-slim-stretch
EXPOSE 8080
WORKDIR /app
ARG JAR=spring-petclinic-2.1.0.BUILD-SNAPSHOT.jar

COPY --from=BUILD /src/target/$JAR /app.jar
ENTRYPOINT [&amp;quot;java&amp;quot;,&amp;quot;-jar&amp;quot;,&amp;quot;/app.jar&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Like before we can use &lt;code&gt;docker build&lt;/code&gt; to build this image, but unlike before we don&amp;rsquo;t need Java or Maven installed locally:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ docker build -f ./Dockerfile.multi -t spring/petclinic .
...
...
Successfully built ee062471d65c
Successfully tagged spring/petclinic:latest

REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
spring/petclinic    latest              ee062471d65c        18 minutes ago      318MB
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you&amp;rsquo;d expect the Docker Image size is the same as the previous build given we effectively did the same thing, build the Jar and then Copy it into a slim image.&lt;/p&gt;

&lt;h1 id=&#34;option-2-google-jib&#34;&gt;Option 2 - Google JIB&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/GoogleContainerTools/jib&#34;&gt;Jib&lt;/a&gt; builds optimized Docker and OCI images for your Java applications without a Docker daemon - and without deep mastery of Docker best-practices. It is available as plugins for Maven and Gradle and as a Java library.&lt;/p&gt;

&lt;p&gt;Normally you&amp;rsquo;d add JIB to your maven build via the pom.xml [as shown here].(&lt;a href=&#34;https://github.com/GoogleContainerTools/jib/tree/master/jib-maven-plugin#setup&#34;&gt;https://github.com/GoogleContainerTools/jib/tree/master/jib-maven-plugin#setup&lt;/a&gt;), To kick the tires we can just pass some extra arguments to maven and get the same result.&lt;/p&gt;

&lt;p&gt;You can build your image with JIB (you don&amp;rsquo;t even need Docker running!) and ship it straight up to the docker registry by running the following:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: In this example I am using my docker registry username in the image name so that it is uploaded correctly, you&amp;rsquo;ll want to swap out &lt;code&gt;paulczar&lt;/code&gt; for your own username.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ mvn compile com.google.cloud.tools:jib-maven-plugin:1.0.0:build -Dimage=paulczar/petclinic:jib -DskipTests
[INFO] Containerizing application to paulczar/petclinic:jib...
[WARNING] Base image &#39;gcr.io/distroless/java&#39; does not use a specific image digest - build may not be reproducible
[INFO]
[INFO] Container entrypoint set to [java, -cp, /app/resources:/app/classes:/app/libs/*, org.springframework.samples.petclinic.PetClinicApplication]
[INFO]
[INFO] Built and pushed image as paulczar/petclinic:jib
[INFO] Executing tasks:
[INFO] [==============================] 100.0% complete
[INFO]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 57.478 s
[INFO] Finished at: 2019-01-25T11:03:25-06:00
[INFO] ------------------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: This provides a warning &lt;code&gt;build may not be reproducible&lt;/code&gt;. You can pass an argument to use your own base Java image to make it more deterministic by adding &lt;code&gt;-Djib.from.image=openjdk:11.0.1-jre-slim-stretch&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If you want JIB to build against your local docker install and not push the image to the registry you can run the following:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ mvn compile com.google.cloud.tools:jib-maven-plugin:1.0.0:dockerBuild
...
...
[INFO] Built image to Docker daemon as spring-petclinic:2.1.0.BUILD-SNAPSHOT
[INFO] Executing tasks:
[INFO] [==============================] 100.0% complete
[INFO]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 10.485 s
[INFO] Finished at: 2019-01-25T11:14:21-06:00
[INFO] ------------------------------------------------------------------------

docker images spring-petclinic:2.1.0.BUILD-SNAPSHOT
REPOSITORY          TAG                    IMAGE ID            CREATED             SIZE
spring-petclinic    2.1.0.BUILD-SNAPSHOT   79d677deeedb        49 years ago        164MB
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: This image is much smaller than the rest, this is because by default JIB creates a distroless Java image. This might seem like a good idea for the size, but will like the warning from the previous build give you an image that may not be reproducable. I recommend always using the &lt;code&gt;-Djib.from.image=openjdk:11.0.1-jre-slim-stretch&lt;/code&gt; argument to choose your upstream Java image which will again give you a 318Mb image like the previous builds.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;others&#34;&gt;Others&lt;/h1&gt;

&lt;p&gt;I&amp;rsquo;ve shown you what I believe are the best methods for building a Docker image for your Spring application. There are some other maven plugins that do the same thing:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/spotify/dockerfile-maven&#34;&gt;Spotify/dockerfile-maven&lt;/a&gt; builds a Jar and then uses a user provided Dockerfile to copy it in.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/spotify/docker-maven-plugin&#34;&gt;spotify/docker-maven&lt;/a&gt; builds the whole image for you much like JIB.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/fabric8io/docker-maven-plugin&#34;&gt;fabricate/docker-maven&lt;/a&gt; also builds the whole image like JIB.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Hopefully after reading this you have a better idea how to build Docker images for your Spring (or general Java) Application. Personally I prefer the multi-stage Dockerfile as your Dockerfile becomes the contract on how your image is built, however I do really like the way i can use JIB to build an image without needing Docker as this simplifies my build environment and means I can very easily use tools like &lt;a href=&#34;https://travis-ci.org&#34;&gt;Travis CI&lt;/a&gt; or &lt;a href=&#34;https://drone.io&#34;&gt;Drone&lt;/a&gt; to build my images for me.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Spring into Kubernetes - Part 1</title>
      <link>https://tech.paulcz.net/blog/spring-into-kubernetes-part-1/</link>
      <pubDate>Thu, 24 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/spring-into-kubernetes-part-1/</guid>
      <description>Introduction Welcome to the first part in a series of blog posts looking at running Spring applications on Kubernetes. To kick the series off we&amp;rsquo;re going to take a look at the Spring Pet Clinic example application and demonstrate how we can quickly and easily get it running on Kubernetes.
Step 1. Install a Kubernetes Before we get started I would encourage you to read (or at least skim) through Kelsey Hightower&amp;rsquo;s seminal Kubernetes The Hard Way to get an idea for the complexity of installing and operating a Kubernetes cluster.</description>
      <content>

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Welcome to the first part in a series of blog posts looking at running Spring applications on Kubernetes. To kick the series off we&amp;rsquo;re going to take a look at the &lt;a href=&#34;https://github.com/spring-projects/spring-petclinic&#34;&gt;Spring Pet Clinic&lt;/a&gt; example application and demonstrate how we can quickly and easily get it running on Kubernetes.&lt;/p&gt;

&lt;h1 id=&#34;step-1-install-a-kubernetes&#34;&gt;Step 1.  Install a Kubernetes&lt;/h1&gt;

&lt;p&gt;Before we get started I would encourage you to read (or at least skim) through Kelsey Hightower&amp;rsquo;s seminal &lt;a href=&#34;https://github.com/kelseyhightower/kubernetes-the-hard-way&#34;&gt;Kubernetes The Hard Way&lt;/a&gt; to get an idea for the complexity of installing and operating a Kubernetes cluster.&lt;/p&gt;

&lt;p&gt;Once you&amp;rsquo;ve done that you should be thoroughly discouraged from installing Kubernetes. That&amp;rsquo;s a good thing! Most people should have no need to run their own production grade Kubernetes cluster. Every major public cloud has a Kubernetes service, and Pivotal provides the &lt;a href=&#34;https://pivotal.io/platform/pivotal-container-service&#34;&gt;Pivotal Container Service&lt;/a&gt; to ease the burden of installing and managing Kubernetes in your own Datacenter.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ll be using &lt;a href=&#34;https://pivotal.io/platform/pivotal-container-service&#34;&gt;Pivotal Container Service&lt;/a&gt;. You should be able to use any flavor of Kubernetes to follow along with this series of blog posts. The simplest way to do so is to probably install and run &lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-minikube/&#34;&gt;Minikube&lt;/a&gt; which will give you a single node Kubernetes cluster running in a VM on your laptop.&lt;/p&gt;

&lt;h1 id=&#34;step-2-validate-your-kubernetes-cluster-is-ready&#34;&gt;Step 2. Validate your Kubernetes cluster is ready&lt;/h1&gt;

&lt;p&gt;However you got your Kubernetes cluster is between you and your deity of choice and I won&amp;rsquo;t ask any questions. You should have received [or had one automatically created] a Kubernetes config file and you should have downloaded the &lt;code&gt;kubectl&lt;/code&gt; command line tool.&lt;/p&gt;

&lt;p&gt;Run the following commands to ensure that your Kubernetes config and cluster are working correctly:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl version
kubClient Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;13&amp;quot;, GitVersion:&amp;quot;v1.13.2&amp;quot;, GitCommit:&amp;quot;cff46ab41ff0bb44d8584413b598ad8360ec1def&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, BuildDate:&amp;quot;2019-01-10T23:35:51Z&amp;quot;, GoVersion:&amp;quot;go1.11.4&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;linux/amd64&amp;quot;}
Server Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;12&amp;quot;, GitVersion:&amp;quot;v1.12.4&amp;quot;, GitCommit:&amp;quot;f49fa022dbe63faafd0da106ef7e05a29721d3f1&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, BuildDate:&amp;quot;2018-12-14T06:59:37Z&amp;quot;, GoVersion:&amp;quot;go1.10.4&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;linux/amd64&amp;quot;}

$ kubectl get nodes
NAME                                      STATUS   ROLES    AGE     VERSION
vm-16ff7fc4-88fa-406d-5f6a-7ccc92286c08   Ready    &amp;lt;none&amp;gt;   2d11h   v1.12.4
vm-5bfb8f6e-d715-45ba-77df-047447995ac1   Ready    &amp;lt;none&amp;gt;   2d11h   v1.12.4
vm-c2c16e34-1a73-4a78-50b4-bdec12d3de77   Ready    &amp;lt;none&amp;gt;   2d11h   v1.12.4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;looking great! Create a namespace to work in that you can delete when finished to easily clean up after the demo.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl create namespace spring-into-kubernetes-1
namespace/spring-into-kubernetes-1 created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Update your Kubernetes config to use this new namespace by default:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl config set-context $(kubectl config current-context) --namespace=spring-into-kubernetes-1
Context &amp;quot;cluster1&amp;quot; modified.
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;step-3-build-a-docker-image-for-spring-pet-clinic&#34;&gt;Step 3. Build a Docker image for Spring Pet Clinic&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: If you are using minikube you can use the &lt;a href=&#34;https://github.com/kubernetes/minikube/blob/master/docs/reusing_the_docker_daemon.md&#34;&gt;minikube docker socket&lt;/a&gt; and skip pushing the image up to a registry.&lt;/p&gt;

&lt;p&gt;Note: If you don&amp;rsquo;t want to build your own image you can use the one that I&amp;rsquo;ve already built &lt;code&gt;paulczar/petclinic:spring-k8s-1&lt;/code&gt; and skip straight to running it.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Use &lt;code&gt;git&lt;/code&gt; to clone down the Spring Pet Clinic git repo locally:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ git clone https://github.com/spring-projects/spring-petclinic.git
Cloning into &#39;spring-petclinic&#39;...
remote: Enumerating objects: 7860, done.
remote: Total 7860 (delta 0), reused 0 (delta 0), pack-reused 7860
Receiving objects: 100% (7860/7860), 6.99 MiB | 14.57 MiB/s, done.
Resolving deltas: 100% (2908/2908), done.

$ cd spring-petclinic
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ordinarily you&amp;rsquo;d use &lt;code&gt;docker build&lt;/code&gt; to create a docker image, however this repo does not have a &lt;code&gt;Dockerfile&lt;/code&gt;, so thankfully we have some less obvious ways through maven to do this using tools like the &lt;a href=&#34;https://github.com/GoogleContainerTools/jib&#34;&gt;Google JIB project&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: For more ways to build Docker images for Spring Applications see the official &lt;a href=&#34;https://spring.io/guides/gs/spring-boot-docker/&#34;&gt;Spring documentation&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ mvn compile -Dimage=spring/petclinic:spring-k8s-1 \
    com.google.cloud.tools:jib-maven-plugin:1.0.0:dockerBuild
...
...
[INFO] Built image to Docker daemon as spring-petclinic:2.1.0.BUILD-SNAPSHOT
[INFO] Executing tasks:
[INFO] [==============================] 100.0% complete
[INFO]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 20.561 s
[INFO] Finished at: 2019-01-24T08:56:30-06:00
[INFO] ------------------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If like me you don&amp;rsquo;t have &lt;code&gt;java&lt;/code&gt; installed on your local desktop you can cheat your way through by mapping your docker socket through to a maven container like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ docker run -ti --rm --workdir /src -v $(pwd):/src \
    -v /var/run/docker.sock:/var/run/docker.sock \
    maven:3.6-jdk-11 mvn compile -Dimage=spring/petclinic:spring-k8s-1 \
    com.google.cloud.tools:jib-maven-plugin:1.0.0:dockerBuild
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will have built an image called &lt;code&gt;spring/petclinic:spring-k8s-1&lt;/code&gt;. Verify it:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ docker images spring/petclinic:spring-k8s-1
REPOSITORY          TAG                    IMAGE ID            CREATED             SIZE
spring/petclinic    spring-k8s-1   6afb36ee7749        15 seconds ago        164MB
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tag this image for your Docker registry username (anywhere you see my registry username you should swap it for yours) and push it up:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ docker tag spring/petclinic:spring-k8s-1 paulczar/petclinic:spring-k8s-1
$ docker push paulczar/petclinic:spring-k8s-1
The push refers to repository [docker.io/paulczar/petclinic]
1f5a2dd7582f: Layer already exists
6e01ddadb469: Pushed
185699264bb4: Layer already exists
5b0bbc8b30cc: Pushed
6189abe095d5: Pushed
c5204564c844: Pushed
spring-k8s-1: digest: sha256:0a71768b0a3b199b6ec78dff983f26c4103e6089ed50c17985b58c120c3aaf72 size: 1581

&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;step-4-optional-validate-the-docker-image-works-locally&#34;&gt;Step 4. (optional) Validate the Docker Image works locally&lt;/h1&gt;

&lt;p&gt;It&amp;rsquo;s usually a good idea to validate things locally in Docker before moving ahead, so go ahead and do that by running Spring Petclinic via your local docker daemon:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: We&amp;rsquo;re running docker with the &lt;code&gt;-ti&lt;/code&gt; flag to keep it in the foreground, &lt;code&gt;--rm&lt;/code&gt; to delete the container when done and &lt;code&gt;-p&lt;/code&gt; to map a port from localhost.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ docker run -ti --rm -p 8080:8080 paulczar/petclinic:spring-k8s-1
...
...
2019-01-24 15:07:35.928  INFO 1 --- [  restartedMain] o.s.b.a.e.web.EndpointLinksResolver      : Exposing 15 endpoint(s) beneath base path &#39;/manage&#39;
2019-01-24 15:07:36.057  INFO 1 --- [  restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 8080 (http) with context path &#39;&#39;
2019-01-24 15:07:36.061  INFO 1 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 7.151 seconds (JVM running for 7.522)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Validate it works by pointint your web browser at localhost port 8080:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./pc-localhost.png&#34; alt=&#34;Spring Pet Clinic via Docker&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Stop and delete your running container by hitting &lt;code&gt;control c&lt;/code&gt; in the terminal running the docker image.&lt;/p&gt;

&lt;h1 id=&#34;step-5-run-pet-clinic-in-kubernetes&#34;&gt;Step 5. Run Pet Clinic in Kubernetes&lt;/h1&gt;

&lt;p&gt;There are two main ways of interacting with Kubernetes. &lt;strong&gt;declarative&lt;/strong&gt; and &lt;strong&gt;imperative&lt;/strong&gt;. With declarative you run &lt;code&gt;kubectl apply&lt;/code&gt; with a local copy of the Kubernetes manifest and let Kubernetes determine how to ensure the running resources matches. When you use Kubernetes imperatively you give it more precise commands like &lt;code&gt;kubectl create&lt;/code&gt; and &lt;code&gt;kubectl run&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Most advanced users of Kubernetes use the declarative methods, but the imperative are perfect for when you&amp;rsquo;re just getting started or want to do something quickly.&lt;/p&gt;

&lt;p&gt;Create a Kubernetes deployment by running the &lt;code&gt;kubectl create&lt;/code&gt; command and then validate it with &lt;code&gt;kubectl get all&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl create deployment petclinic --image=paulczar/petclinic:spring-k8s-1
deployment.apps/petclinic created

$ kubectl get all
NAME                             READY   STATUS    RESTARTS   AGE
pod/petclinic-5ffccf75c4-snhdd   1/1     Running   0          36s

NAME                        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/petclinic   1         1         1            1           36s

NAME                                   DESIRED   CURRENT   READY   AGE
replicaset.apps/petclinic-5ffccf75c4   1         1         1       36s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can see that Kubernetes has created several resources. It created a &lt;strong&gt;Deployment&lt;/strong&gt; which is reconciled by a &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/&#34;&gt;Kubernetes controller&lt;/a&gt; for managing the lifecycle of &lt;strong&gt;ReplicaSets&lt;/strong&gt;. The &lt;strong&gt;Deployment&lt;/strong&gt; created the &lt;strong&gt;ReplicaSet&lt;/strong&gt; which has a controller for ensuring a set number of replicas of your application is running. The &lt;strong&gt;ReplicaSet&lt;/strong&gt; created a &lt;strong&gt;Pod&lt;/strong&gt; which is your running application.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/&#34;&gt;Controllers&lt;/a&gt; are a major part of what makes Kubernetes so good. They create tight control loops around resources to add more complex functionality as described above.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You might notice that the Kubernetes output above does not give you an IP address or any hints as to how to access your application. This is because we need to expose the application using a &lt;strong&gt;Service&lt;/strong&gt; resource.&lt;/p&gt;

&lt;h1 id=&#34;step-6-expose-your-application-to-the-internet&#34;&gt;Step 6. Expose your application to the Internet&lt;/h1&gt;

&lt;p&gt;If you don&amp;rsquo;t care about making your application available to the internet and just want to validate that it works you can run &lt;code&gt;kubectl port-forward&lt;/code&gt; and access your application via a &lt;code&gt;localhost&lt;/code&gt; port forward like so:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;kubectl port-forward deployment/petclinic 8080:8080
Forwarding from 127.0.0.1:8080 -&amp;gt; 8080
Forwarding from [::1]:8080 -&amp;gt; 8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Access the petclinic app just like we did earlier by pointing your browser at localhost:8080:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./pc-localhost.png&#34; alt=&#34;Spring Pet Clinic via Docker&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Stop the port forward with a &lt;code&gt;control-C&lt;/code&gt; on in the terminal.&lt;/p&gt;

&lt;p&gt;Use the &lt;code&gt;kubectl expose&lt;/code&gt; command to create a LoadBalancer service in Kubernetes and then validate it with &lt;code&gt;kubectl get services&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl expose deployment petclinic --type=LoadBalancer --port 80 --target-port 8080
service/petclinic exposed

$ kubectl get service
NAME        TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
petclinic   LoadBalancer   10.100.200.110   &amp;lt;pending&amp;gt;     80:31148/TCP   15s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After a few minutes the &lt;code&gt;EXTERNAL-IP&lt;/code&gt; field should go from &lt;code&gt;&amp;lt;pending&amp;gt;&lt;/code&gt; to having an IP address:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl get service
NAME        TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
petclinic   LoadBalancer   10.100.200.110   35.238.37.241    80:31148/TCP   15s
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: If you are running minikube you won&amp;rsquo;t get a real loadbalancer and can run &lt;code&gt;minikube service example-service --url&lt;/code&gt; to get an IP/Port combo that should work.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Point your web browser at that IP address on port 80:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./kube-lb-pc.png&#34; alt=&#34;Spring Pet Clinic via Docker&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;step-7-cleanup&#34;&gt;Step 7. Cleanup&lt;/h1&gt;

&lt;p&gt;Delete the service and deployment by running the &lt;code&gt;kubectl delete&lt;/code&gt; command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ k8s-platform-ops kubectl delete deployment petclinic
deployment.extensions &amp;quot;petclinic&amp;quot; deleted
$ k8s-platform-ops kubectl delete service petclinic
$ k8s-platform-ops kubectl get all
No resources found.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Delete your namespace and switch back to the &lt;code&gt;default&lt;/code&gt; namespace:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl delete namespace spring-into-kubernetes-1
namespace &amp;quot;spring-into-kubernetes-1&amp;quot; deleted

$ kubectl config set-context $(kubectl config current-context) --namespace=default
Context &amp;quot;cluster1&amp;quot; modified.
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Congratulations! You&amp;rsquo;ve successfully deployed a basic Spring application to Kubernetes and it wasn&amp;rsquo;t even all that hard.&lt;/p&gt;

&lt;p&gt;Using Kubernetes imperatively like this is a great way to get started and easily demonstrate running an application on Kubernetes. Of course there&amp;rsquo;s many more things to take into consideration if you want to run an application in production on Kubernetes and we&amp;rsquo;ll explore some of those things in future installments of Spring Into Kubernetes.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>The future of Kubernetes is Virtual Machines</title>
      <link>https://tech.paulcz.net/blog/future-of-kubernetes-is-virtual-machines/</link>
      <pubDate>Tue, 25 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/future-of-kubernetes-is-virtual-machines/</guid>
      <description>Note: If this topic has peaked your interest, you can join me for a Webinar on August 15 where I&amp;rsquo;ll dive deep into Cloud Native Operations with Kubernetes and CI/CD Pipelines.
 Peering into the crystal ball Kubernetes as a technology has been very important to my career this year, and will be even more so for next year too. As 2018 comes to a close its time to drag out the hubris and make a bold prediction.</description>
      <content>

&lt;blockquote&gt;
&lt;p&gt;Note: If this topic has peaked your interest, you can join me for a Webinar on August 15 where I&amp;rsquo;ll dive deep into &lt;a href=&#34;https://content.pivotal.io/webinars/aug-15-cloud-native-operations-with-kubernetes-and-ci-cd-webinar?utm_campaign=cno-k8s-ci-cd-q319&amp;amp;utm_source=blog&amp;amp;utm_medium=website&#34;&gt;Cloud Native Operations with Kubernetes and CI/CD Pipelines&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;peering-into-the-crystal-ball&#34;&gt;Peering into the crystal ball&lt;/h2&gt;

&lt;p&gt;Kubernetes as a technology has been very important to my career this year, and will be even more so for next year too. As 2018 comes to a close its time to drag out the hubris and make a bold prediction. The future of Kubernetes is Virtual Machines, not Containers.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;The future of Kubernetes is Virtual Machines, not Containers.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The Chinese Zodiac calls 2018 the year of the Dog, in tech it was the year of the Kubernetes. There are plenty of people only now learning about Kubernetes. CIOs everywhere are hard at work trying to develop a &amp;ldquo;Kubernetes Strategy&amp;rdquo; [1]. Some organizations are already running large production workloads on Kubernetes.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;[1] If you&amp;rsquo;re trying to develop a Kubernetes strategy you&amp;rsquo;ve already failed, but that&amp;rsquo;s a different blog post.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In other words there are plenty of people at each stage of the Gartner Hype Cycle for Kubernetes. Many are stuck in the trough of disillusionment or have drowned in the pit of despair.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./hype-cycle.png&#34; alt=&#34;Gartner Hype Cycle&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Jeremykemp at &lt;a href=&#34;https://commons.wikimedia.org/wiki/File:Gartner_Hype_Cycle.svg&#34;&gt;Wikipedia&lt;/a&gt; &lt;a href=&#34;https://creativecommons.org/licenses/by-sa/3.0&#34;&gt;Creative Commons CC BY-SA 3.0&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I am a big fan of containers and I&amp;rsquo;m not going to try to suggest that &lt;a href=&#34;https://chrisshort.net/docker-inc-is-dead/&#34;&gt;containers are dead&lt;/a&gt;. What Docker brought us in 2013 was a wrapper around Linux Containers. They showed us an amazing new way to build, package, share, and deploy applications.  This came at exactly the right time as we had started to get serious about Continuous Delivery.  Their model was perfect for the modern delivery pipeline and the emergence of PaaS and later CaaS platforms.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;cloud-native-ops-pipeline.png&#34; alt=&#34;Cloud Native Operations Pipeline&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Engineers working at Google saw the technology community was finally ready for containers. Google has been using (and more or less invented) containers for a very long time already. They started to build Kubernetes which as we all know by now is a re-imagining of Google&amp;rsquo;s own Borg platform built in the open as a community effort.&lt;/p&gt;

&lt;p&gt;It didn&amp;rsquo;t take long for the big public clouds to provide a Kubernetes based platform (GKE, AKS, EKS).  The on premise folks were also quick to build platforms based on Kubernetes as well (Pivotal Container Service, Openshift, etc).&lt;/p&gt;

&lt;h2 id=&#34;soft-and-squishy-multi-tenancy&#34;&gt;Soft and Squishy Multi-Tenancy&lt;/h2&gt;

&lt;p&gt;There&amp;rsquo;s one sticky problem that&amp;rsquo;s left to solve, and this I believe will prove to be the downfall of the container &amp;hellip;  multi-tenancy.&lt;/p&gt;

&lt;p&gt;Linux containers were not built to be secure isolated sandboxes (like Solaris Zones or FreeBSD Jails). Instead they&amp;rsquo;re built upon a shared kernel model that utilizes kernel features to provide basic process isolation. As &lt;a href=&#34;https://blog.jessfraz.com/post/containers-zones-jails-vms/&#34;&gt;Jessie Frazelle&lt;/a&gt; would say &amp;ldquo;Containers aren&amp;rsquo;t a real thing&amp;rdquo;.&lt;/p&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;containers aren&amp;#39;t things&lt;/p&gt;&amp;mdash; Jessie Frazelle (@jessfraz) &lt;a href=&#34;https://twitter.com/jessfraz/status/1015407561187655680?ref_src=twsrc%5Etfw&#34;&gt;July 7, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;Compounding this is the fact that most Kubernetes components are not Tenant aware. Sure you have &lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/&#34;&gt;Namespaces&lt;/a&gt; and &lt;a href=&#34;https://kubernetes.io/docs/concepts/policy/pod-security-policy/&#34;&gt;Pod Security Policies&lt;/a&gt; but the API itself is not. Nor are the internal components like the &lt;code&gt;kubelet&lt;/code&gt; or &lt;code&gt;kube-proxy&lt;/code&gt;. This leads to Kubernetes having a &amp;ldquo;Soft Tenancy&amp;rdquo; model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./k8s-arch.png&#34; alt=&#34;Kubernetes Architecture&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Abstractions leak. A platform built on top of containers will inherit many of the soft tenancy aspects of containers. Platforms built on top of hard multi-tenancy Virtual Machines all inherit that hard tenancy (VMware, Amazon Web Services, OpenStack, etc).&lt;/p&gt;

&lt;h2 id=&#34;kubesprawl-rules-everything-around-me&#34;&gt;Kubesprawl Rules Everything Around Me&lt;/h2&gt;

&lt;p&gt;Kubernetes&amp;rsquo; soft tenancy model leaves service providers and distributions in a weird place. The Kubernetes cluster itself becomes the line of &amp;ldquo;Hard Tenanacy&amp;rdquo;. There are many reasons (even inside the same organisation) to require hard tenancy between users (or appplications). Since the public clouds provide fully managed Kubernetes as a Service offerings its easy enough for each Tenant to get their own cluster and use the cluster boundary as the isolation point.&lt;/p&gt;

&lt;p&gt;Some Kubernetes distributions like &lt;a href=&#34;https://pivotal.io/platform/pivotal-container-service&#34;&gt;Pivotal Container Service (PKS)&lt;/a&gt; are very aware of this &lt;a href=&#34;https://content.pivotal.io/youtube-uberflip/kubernetes-one-cluster-or-many-3&#34;&gt;tenancy issue&lt;/a&gt; and have taken a similar model by providing that same Kubernetes as a Service experience you&amp;rsquo;d get from a public cloud but in your own datacenter.&lt;/p&gt;

&lt;p&gt;This leads to the emerging pattern of &amp;ldquo;many clusters&amp;rdquo; rather than &amp;ldquo;one big shared&amp;rdquo; cluster. Its not uncommon to see customers of Google&amp;rsquo;s GKE Service have dozens of Kubernetes clusters deployed for multiple teams. Often each developer gets their own cluster. This kind of behavior leads to a shocking amount of Kubesprawl.&lt;/p&gt;

&lt;p&gt;Alternatively Kubernetes operators running self Kubernetes clusters (either upstream or distribution based) in their own datacenters are left to take on the extra work of managing lots of clusters on their own, or to accept the soft tenancy on just one or two larger clusters.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;This kind of behavior leads to a shocking amount of Kubesprawl&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Generally the smallest cluster you get is 4 machines (or VMs). One (or 3 for HA) for the Kubernetes Master, three for the Kubernetes Workers. This ties up a ton of money in systems that are for the most part just sitting there idle.&lt;/p&gt;

&lt;p&gt;So we need to somehow move Kubernetes to a hard-tenancy model. The Kubernetes community is very aware of this need and has a &lt;a href=&#34;https://groups.google.com/forum/#!forum/kubernetes-wg-multitenancy&#34;&gt;multi-tenancy working group&lt;/a&gt;. This group has working hard on that problem and they have several suggested models and proposals on how to solve each model.&lt;/p&gt;

&lt;p&gt;Jessie Frazelle wrote a &lt;a href=&#34;https://blog.jessfraz.com/post/hard-multi-tenancy-in-kubernetes/&#34;&gt;blog post&lt;/a&gt;  about this exact topic which is great because she&amp;rsquo;s much smarter than me so I can link you to her and save myself about ten years of hard study trying to catch up to her. If you have not read it, stop reading here and go read it first.&lt;/p&gt;

&lt;h2 id=&#34;just-make-really-small-vms-optimized-for-speed&#34;&gt;Just make really small VMs optimized for speed&amp;hellip;&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Kata Containers is an open source project and community working to build a standard implementation of lightweight Virtual Machines (VMs) that feel and perform like containers, but provide the workload isolation and security advantages of VMs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Jessie suggests using a VM Container technology such as &lt;a href=&#34;https://katacontainers.io/&#34;&gt;Kata Containers&lt;/a&gt;. Kata Containers combine Virtual Machine level isolation that act and perform like Containers. This allows Kubernetes to give each Tenant (we&amp;rsquo;ll assume a tenant per namespace) its own set of Kubernetes system services running in nested VM Containers (A VM Container running inside a VM provided by the underlying IaaS).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./jessie-tenancy1.png&#34; alt=&#34;Hard Tenancy Kubernetes&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;image from &lt;a href=&#34;https://blog.jessfraz.com/post/hard-multi-tenancy-in-kubernetes/&#34;&gt;Jessie Frazelle - Hard Multi-Tenancy in Kubernetes&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is an elegant solutions to Kubernetes multi-tenancy. Her suggestion goes even further to suggest that Kubernetes use nested Container VMs for the workloads (Pods) running on Kubernetes which provides a significant increase of resource utilization.&lt;/p&gt;

&lt;p&gt;We have at least one more optimization to make here. Build out a suitable Hypervisor for the underlying IaaS or cloud provider. If a VM Container is a first level abstraction provided by the IaaS then we increase our resource utilization even further. The minimal number of VMs required to run a Kubernetes cluster goes down to one (or three for HA) to host the Kubernetes control plane exposed to the &amp;ldquo;Superuser&amp;rdquo;.&lt;/p&gt;

&lt;h2 id=&#34;resource-cost-optimized-multi-tenancy&#34;&gt;Resource (cost) Optimized Multi-tenancy&lt;/h2&gt;

&lt;p&gt;A Kubernetes deployment with two namespaces both with a number of applications running would look something like this.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./hard-tenancy-k8s.png&#34; alt=&#34;Hard Tenancy Hosted Kubernetes&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: there are other cloud tenant workloads running on the same IaaS infrastructure. Since these are VM Containers they have the same level of isolation as a regular Cloud VM. Thus they can run on the same hypervisor with minimal risk.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Initially there is zero infrastructure deployed to the cloud, thus there is zero cost to the Superuser.&lt;/p&gt;

&lt;p&gt;The Superuser requests a Kubernetes cluster from the cloud. The Cloud provider creates a single Container VM (or 3 for HA) which is running the main Kubernetes API and System Services. The Superuser could choose to deploy pods in the system namespace, or create new namespaces to delegate access to other users.&lt;/p&gt;

&lt;p&gt;The Superuser create two Namespaces &lt;code&gt;foo&lt;/code&gt; and &lt;code&gt;bar&lt;/code&gt;. Kubernetes requests two VM containers from the Cloud for each Namespace&amp;rsquo;s Control Plane (Kubernetes API and System Services). The Superuser delegates access to those Namespaces to some users who each deploy some workloads (Pods), their respective Control Planes request VM containers for each of those workloads.&lt;/p&gt;

&lt;p&gt;At all stages of this, the superuser is only paying for the actual consumed resources. The cloud provider owns the capacity which is available to any user of the cloud.&lt;/p&gt;

&lt;h2 id=&#34;i-m-not-actually-saying-anything-new-here&#34;&gt;I&amp;rsquo;m not actually saying anything new here &amp;hellip;&lt;/h2&gt;

&lt;p&gt;The cloud providers are already working towards this future. You can see this forming by watching what is happening in the Open Source Communities. (Arguably Amazon is already doing this opaquely with Fargate).&lt;/p&gt;

&lt;p&gt;The first hint is &lt;a href=&#34;https://github.com/virtual-kubelet/virtual-kubelet&#34;&gt;Virtual Kubelet&lt;/a&gt; which is an open source tool designed to masquerade as a kubelet.  It connects Kubernetes to other APIs. This would allow Kubernetes to request Container VMs from a Cloud&amp;rsquo;s Container VM scheduler.&lt;/p&gt;

&lt;p&gt;Other hints are the number of emerging VM Container technologies, the already mentioned &lt;a href=&#34;https://katacontainers.io/&#34;&gt;Kata Containers&lt;/a&gt;, but also &lt;a href=&#34;https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/&#34;&gt;Firecracker from Amazon&lt;/a&gt; and &lt;a href=&#34;https://github.com/google/gvisor&#34;&gt;gvisor&lt;/a&gt; from Google.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Tied together with the correct improvements to the Kubernetes hard tenancy model and you&amp;rsquo;ll arrive at the holy grail of Kubernetes.  Full isolation of Kubernetes workloads and a pure per Pod consumption cost model to run workloads on Kubernetes.&lt;/p&gt;

&lt;p&gt;For those who aren&amp;rsquo;t on the public cloud, we don&amp;rsquo;t get the same consumption model as the onus for capacity remains with the infrastructure provider (in this case yourself). You will still get the benefits of higher resource utilization which pays off in lower capacity requirements.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s hope VMware and OpenStack are paying attention and bring us lightweight VM Container technology based hypervisors and the appropriate Virtual Kubelet implementations.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Building a Habitat Supervisor for Kubernetes</title>
      <link>https://tech.paulcz.net/blog/habitat-supervisors-in-kubernetes/</link>
      <pubDate>Fri, 06 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/habitat-supervisors-in-kubernetes/</guid>
      <description>Habitat is a project from Chef that provides you a reasonably simple way to build, package, and configure your application.
 &amp;ldquo;Habitat is an integrated solution to building, running, and maintaining your application throughout its lifetime. It builds your application and its services into immutable artifacts with declarative dependencies, and then provides automatic rebuilds of your application and its services as your application code and dependencies have upstream updates.&amp;rdquo; - Habitat Getting Started Guide.</description>
      <content>

&lt;p&gt;&lt;a href=&#34;https://habitat.sh&#34;&gt;Habitat&lt;/a&gt; is a project from &lt;a href=&#34;https://chef.io&#34;&gt;Chef&lt;/a&gt; that
provides you a reasonably simple way to build, package, and configure your
application.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Habitat is an integrated solution to building, running, and maintaining your application throughout its lifetime. It builds your application and its services into immutable artifacts with declarative dependencies, and then provides automatic rebuilds of your application and its services as your application code and dependencies have upstream updates.&amp;rdquo; - &lt;a href=&#34;https://www.habitat.sh/tutorials/get-started/&#34;&gt;Habitat Getting Started Guide&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;One of Habitat&amp;rsquo;s core features is that its Supervisor creates a gossip based cluster
for managing configuration and state of your applications. Kubernetes also provides similar functionality this ability with the user defined Kubernetes manifests and the Kubernetes APIs. Initially it may seem odd that you would skip using Kubernetes to provide this functionality, however it does provide a way to have a universal system for your application management.&lt;/p&gt;

&lt;p&gt;Personally I&amp;rsquo;m still on the fence about how useful it is to have this extra abstraction for application lifecycle management on top of what Kubernetes already offers, but I don&amp;rsquo;t discount it as something that could be useful in a lot of organizations.&lt;/p&gt;

&lt;p&gt;Documentation for running Habitat built applications on Kubernetes is scant and feels fairly incomplete so I figured I would spend some time to work it out and come up with something myself.&lt;/p&gt;

&lt;p&gt;Of course the first thing I had to do was decide on an application to build to demonstrate it. Initially I was going to use the &lt;a href=&#34;https://www.habitat.sh/tutorials/get-started/&#34;&gt;basic tutorial&lt;/a&gt; app from the Habitat getting started tutorial, but instead decided I should write a very lightweight golang app to reduce the dependencies required to build and run it.&lt;/p&gt;

&lt;p&gt;The application I wrote is dead simple. Just a &lt;a href=&#34;https://github.com/paulczar/habsup/blob/master/main.go&#34;&gt;few lines&lt;/a&gt; of Golang to provide an API that responds to a &lt;code&gt;GET /health&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func main() {
  handler := health.NewHandler()
  http.Handle(&amp;quot;/health/&amp;quot;, handler)
  http.ListenAndServe(&amp;quot;:8080&amp;quot;, nil)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After writing this simplest of applications I realized I had inadvertently created a way to run the Habitat Supervisor effectively standalone which would also allow me to bootstrap a Habitat Gossip Cluster that other applications can join as needed.&lt;/p&gt;

&lt;p&gt;Next I had to get my Habitat environment set up. I was able to follow the Habitat Tutorial and figure out how to build this Golang app instead of a Ruby app. This was fairly &lt;a href=&#34;https://github.com/paulczar/habsup/tree/master/habitat&#34;&gt;straight forward&lt;/a&gt;
and was some edits to a &lt;a href=&#34;https://github.com/paulczar/habsup/blob/master/habitat/plan.sh&#34;&gt;plan.sh&lt;/a&gt;
file and a few &lt;a href=&#34;https://github.com/paulczar/habsup/tree/master/habitat/hooks&#34;&gt;hooks&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Performing the Build and exporting the &lt;code&gt;.hart&lt;/code&gt; file to a Docker image was fairly easy after I stumbled my way through &lt;code&gt;hab setup&lt;/code&gt; and getting a key etc working (the documentation for this could be improved to provide a more delightful experience).&lt;/p&gt;

&lt;h2 id=&#34;habitat-build-demo&#34;&gt;Habitat Build Demo&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone https://github.com/paulczar/habsup.git
$ cd habsup
$ hab studio enter
$ build
$ hab pkg export docker ./results/paulczar-habsuper-...hart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://tech.paulcz.net/habitat-supervisors-in-kubernetes/images/habstudio.gif&#34; alt=&#34;habitat build&#34; /&gt;&lt;/p&gt;

&lt;p&gt;My next step was to test it using Docker to make sure the app started and cluster formed etc. This mean writing a simple &lt;a href=&#34;https://github.com/paulczar/habsup/blob/master/docker-compose.yaml&#34;&gt;docker-compose.yaml&lt;/a&gt; file to launch three containers and tell them how to connect to eachother with links. and then launch the containers and check that the exposed Habitat Supervisor API is accessible.&lt;/p&gt;

&lt;h2 id=&#34;docker-demo&#34;&gt;Docker Demo&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone https://github.com/paulczar/habsup.git
$ cd habsup
$ docker-compose up -d
$ docker-compose logs -f
$ curl http://localhost:9631/services
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://tech.paulcz.net/habitat-supervisors-in-kubernetes/images/docker.gif&#34; alt=&#34;habitat docker compose&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: you can see the habitat supervisors running the health check at the end once the containers are running.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Now that I had the Supervisor as a standalone image it was time to put together the appropriate &lt;a href=&#34;https://github.com/paulczar/habsup/blob/master/kubernetes/manifests/supervisor.yaml&#34;&gt;Kubernetes manifest&lt;/a&gt;. To do so I had to do some research on the various Kubernetes resources and how they&amp;rsquo;d help me achieve my goal.&lt;/p&gt;

&lt;p&gt;After some experimentation I decided that it made sense to use the &lt;code&gt;StatefulSet&lt;/code&gt; resource for to run the supervisor in and run two services, the first being a headless service (meaning it is internal only) for the gossip protocol and the second being a regular service (with external access possible) for the API. Using a &lt;code&gt;StatefulSet&lt;/code&gt; gave me predictable pod names and starts up the replicas in order which makes it much easier for the gossip protocol to work.&lt;/p&gt;

&lt;p&gt;Initially I was using a single service for both the gossip and API ports but I wanted the Gossip to be internal only, but allow access (if needed) to the API. Creating two services gives me the ability to do both of those things. A headless service also has the benefit of creating a predictable KubeDNS entry for both the service and each pod which can come in handy.&lt;/p&gt;

&lt;p&gt;Another interesting thing I discovered is that Kubernetes doesn&amp;rsquo;t publish the service DNS until at least one pod is running. This created a chicken-and-egg issue if I tried to use a &lt;code&gt;readinessProbe&lt;/code&gt; for the hab api as habitat wouldn&amp;rsquo;t start until DNS was ready and DNS wouldn&amp;rsquo;t be created as it was waiting for a success from the probe. Thankfully there is an alpha feature that you can enable with an annotation &lt;code&gt;service.alpha.kubernetes.io/tolerate-unready-endpoints: &amp;quot;true&amp;quot;&lt;/code&gt; that allows you to use DNS before the pods are ready.&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-demo&#34;&gt;Kubernetes Demo&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone https://github.com/paulczar/habsup.git
$ cd habsup
$ kubectl create -f kubernetes/manifests
$ kubectl get pods -w
$ kubectl logs habitat-supervisor-0
$ curl $(minikube service habitat-supervisor-http --url)/services
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://tech.paulcz.net/habitat-supervisors-in-kubernetes/images/kubernetes.gif&#34; alt=&#34;habitat kubernetes&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Hopefully this was enough to bootstrap a person looking to use Habitat on Kubernetes. It would be fairly trivial to use the manifests I provided and do one of the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Use the Habitat cluster created here as a permanent Habitat cluster and have your applications join and leave that cluster as they come up.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Swap out the use of &lt;code&gt;paulczar/habsup&lt;/code&gt; image with your own image and adjust the ports and other values accordingly and have it run as a self contained cluster.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Getting Habitat to work in Kubernetes was fairly straight forward, however I had to do a few tricky things that shouldn&amp;rsquo;t be necessary. In order for Habitat to get solid adoption on Kubernetes I believe the following needs to be addressed:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Gossip cluster bootstrap relies on an ordered startup with &lt;code&gt;--peer ip-or-dns-of-first&lt;/code&gt;. Habitat should support a Kuberenetes based discovery which would ask the Kubernetes API to provide the peer details to join.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The API should come online with an approriate health status before the cluster is created. This would allow the use of a readinessProbe and avoid the problem I suggested earlier.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Habitat should consider a mode that uses the Kubernetes APIs and the contents of the Manifest to configure itself rather than forming the gossip cluster.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</content>
    </item>
    
    <item>
      <title>Writing Your First Helm Chart</title>
      <link>https://tech.paulcz.net/blog/getting-started-with-helm/</link>
      <pubDate>Tue, 03 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://tech.paulcz.net/blog/getting-started-with-helm/</guid>
      <description>I recently found myself writing instructions on how to deploy an application to several Kubernetes platform and ended up writing a different Kubernetes manifests for each platform. 95% of the content was the same with just a few different directives based on how the particular platform handles ingress, or if we needed a Registry secret or a TLS certificate.
Kubernetes manifests are very declarative and don&amp;rsquo;t offer any way to put conditionals or variables that could be set in them.</description>
      <content>

&lt;p&gt;I recently found myself writing &lt;a href=&#34;https://github.com/IBM/activator-lagom-java-chirper/blob/master/docs/README.md&#34;&gt;instructions&lt;/a&gt; on how to deploy an
application to several Kubernetes platform and ended up writing a different Kubernetes manifests for each
platform. 95% of the content was the same with just a few different directives
based on how the particular platform handles ingress, or if we needed a Registry secret or a TLS certificate.&lt;/p&gt;

&lt;p&gt;Kubernetes manifests are very declarative and don&amp;rsquo;t offer any way to put conditionals or variables that could be set in them. This
is both a good and a bad thing. Enter &lt;a href=&#34;https://docs.helm.sh/&#34;&gt;Helm&lt;/a&gt; a Package Manager for Kubernetes. Helm allows you to package up
your Kubernetes application as a package that can be deployed easily to Kubernetes, One of its features (and the one that interested me)
the ability to template out your Kubernetes manifests.&lt;/p&gt;

&lt;p&gt;If you already have a Kubernetes manifest its very easy to turn it into a Helm Chart that you can then
iterate over and improve as you need to add more flexibility to it. In fact your first iteration of a
Helm chart can be as simple as moving your manifests into a new directory and adding a few lines
to a Chart.yaml file.&lt;/p&gt;

&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;

&lt;p&gt;You&amp;rsquo;ll need the following installed to follow along with this tutorial:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-minikube/&#34;&gt;Minikube&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/helm/blob/master/docs/install.md&#34;&gt;Helm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-kubectl/&#34;&gt;kubectl&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;prepare-environment&#34;&gt;Prepare Environment&lt;/h2&gt;

&lt;p&gt;Bring up a test Kubernetes environment using Minikube:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ minikube start
Starting local Kubernetes v1.7.5 cluster...
Starting VM...
Getting VM IP address...
Moving files into cluster...
Setting up certs...
Connecting to cluster...
Setting up kubeconfig...
Starting cluster components...
Kubectl is now configured to use the cluster.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Wait a minute or so and install Helm&amp;rsquo;s tiller service to Kubernetes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm init
$HELM_HOME has been configured at /home/pczarkowski/.helm.

Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.
Happy Helming!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a path to work in:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ mkdir -p ~/development/my-first-helm-chart
$ cd ~/development/my-first-helm-chart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;If it fails out you may need to wait a few more minutes for minikube to become
accessible.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;create-example-kubernetes-manifest&#34;&gt;Create Example Kubernetes Manifest.&lt;/h2&gt;

&lt;p&gt;Writing a Helm Chart is easier when you&amp;rsquo;re starting with an existing set of
Kubernetes manifests. One of the easiest ways to get a basic working manifest
is to ask Kubernetes to
&lt;a href=&#34;https://blog.heptio.com/using-kubectl-to-jumpstart-a-yaml-file-heptioprotip-6f5b8a63a3ea&#34;&gt;run something and then fetch the manifest&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ mkdir manifests
$ kubectl run example --image=nginx:1.13.5-alpine \
    -o yaml &amp;gt; manifests/deployment.yaml
$ kubectl expose deployment example --port=80 --type=NodePort \
    -o yaml &amp;gt; manifests/service.yaml
$ minikube service example --url       
http://192.168.99.100:30254
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All going well you should be able to hit the provided URL and get the &amp;ldquo;Welcome to nginx!&amp;rdquo;
page. You&amp;rsquo;ll see you now have two Kubernetes manifests saved. We can use these
to bootstrap our helm charts:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ tree manifests
manifests
├── deployment.yaml
└── service.yaml
0 directories, 2 files
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Before we move on we should clean up our environment.  We can use the newly
created manifests to help:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl delete -f manifests
deployment &amp;quot;example&amp;quot; deleted
service &amp;quot;example&amp;quot; deleted
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;create-and-deploy-a-basic-helm-chart&#34;&gt;Create and Deploy a Basic Helm Chart&lt;/h2&gt;

&lt;p&gt;Helm has some tooling to create the scaffolding needed to start developing a
new Helm Chart. We&amp;rsquo;ll create it with a placeholder name of &lt;code&gt;helm&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm create helm
Creating helm
tree helm
helm
├── charts
├── Chart.yaml
├── templates
│   ├── deployment.yaml
│   ├── _helpers.tpl
│   ├── ingress.yaml
│   ├── NOTES.txt
│   └── service.yaml
└── values.yaml
2 directories, 7 files
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Helm will have created a number of files and directories.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Chart.yaml&lt;/code&gt; - the metadata for your Helm Chart.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;values.yaml&lt;/code&gt; - values that can be used as variables in your templates.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;templates/*.yaml&lt;/code&gt; - Example Kubernetes manifests.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;_helpers.tpl&lt;/code&gt; - helper functions that can be used inside the templates.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;templates/NOTES.txt&lt;/code&gt; - templated notes that are displayed on Chart install.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Edit &lt;code&gt;Chart.yaml&lt;/code&gt; so that it looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
description: My First Helm Chart - NGINX Example
name: my-first-helm-chart
version: 0.1.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Copy our example Kubernetes manifests over the provided templates and remove the
currently unused &lt;code&gt;ingress.yaml&lt;/code&gt; and &lt;code&gt;NOTES.txt&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cp manifests/* helm/templates/
$ rm helm/templates/ingress.yaml
$ rm helm/templates/NOTES.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next we should be able to install our helm chart which will deploy our application
to Kubernetes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm install -n my-first-helm-chart helm
NAME:   my-first-helm-chart
LAST DEPLOYED: Tue Oct  3 10:20:57 2017
NAMESPACE: default
STATUS: DEPLOYED

RESOURCES:
==&amp;gt; v1/Service
NAME     CLUSTER-IP  EXTERNAL-IP  PORT(S)       AGE
example  10.0.0.210  &amp;lt;nodes&amp;gt;      80:30254/TCP  0s

==&amp;gt; v1beta1/Deployment
NAME     DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
example  1        1        1           0          0s

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Like before we can use &lt;code&gt;minikube&lt;/code&gt; to get the URL:&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ minikube service example --url    
http://192.168.99.100:30254
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Again accessing that URL via your we browser should get you the default NGINX welcome page.&lt;/p&gt;

&lt;p&gt;Congratulations!  You&amp;rsquo;ve just created and deployed your first Helm chart. However we&amp;rsquo;re
not quite done yet. use Helm to delete your deployment and then lets move on to customizing
the Helm Chart with variables and values:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm del --purge my-first-helm-chart
release &amp;quot;my-first-helm-chart&amp;quot; deleted
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;add-variables-to-your-helm-chart&#34;&gt;Add variables to your Helm Chart&lt;/h2&gt;

&lt;p&gt;Check out &lt;code&gt;helm/values.yaml&lt;/code&gt; and you&amp;rsquo;ll see there&amp;rsquo;s a lot of variables already
defined by the example that helm provided when you created the helm chart. You&amp;rsquo;ll
notice that it is has values for &lt;code&gt;nginx&lt;/code&gt; in there. This is because Helm also uses
nginx as their example. We can re-use some of the values provided but we should clean
it up a bit.&lt;/p&gt;

&lt;p&gt;Edit &lt;code&gt;helm/values.yaml&lt;/code&gt; to look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;replicaCount: 1
image:
  repository: nginx
  tag: 1.13.5-alpine
  pullPolicy: IfNotPresent
  pullSecret:
service:
  type: NodePort
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can access any of these values in our templates using the golang templating
engine. For example accessing &lt;code&gt;replicaCount&lt;/code&gt; would be written as &lt;code&gt;{{ .Values.replicaCount }}&lt;/code&gt;.
Helm also provides information about the Chart and Release which we&amp;rsquo;ll also utilize.&lt;/p&gt;

&lt;p&gt;Update your &lt;code&gt;helm/templates/deployment.yaml&lt;/code&gt; to utilize our values:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: apps/v1beta1
kind: Deployment
metadata:
  creationTimestamp: 2017-10-03T15:03:17Z
  generation: 1
  labels:
    run: &amp;quot;{{ .Release.Name }}&amp;quot;
    chart: &amp;quot;{{ .Chart.Name }}-{{ .Chart.Version }}&amp;quot;
    release: &amp;quot;{{ .Release.Name }}&amp;quot;
    heritage: &amp;quot;{{ .Release.Service }}&amp;quot;     
  name: &amp;quot;{{ .Release.Name }}&amp;quot;
  namespace: default
  resourceVersion: &amp;quot;3030&amp;quot;
  selfLink: /apis/extensions/v1beta1/namespaces/default/deployments/example
  uid: fd03ac95-a84b-11e7-a417-0800277e13b3
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      run: &amp;quot;{{ .Release.Name }}&amp;quot;
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        run: &amp;quot;{{ .Release.Name }}&amp;quot;
    spec:
      {{- if .Values.image.pullSecret }}    
            imagePullSecrets:
              - name: &amp;quot;{{ .Values.image.pullSecret }}&amp;quot;
      {{- end }}          
      containers:
      - image: {{ .Values.image.repository }}:{{ .Values.image.tag }}
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        name: example
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status: {}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Note the use of the &lt;code&gt;if&lt;/code&gt; statement around &lt;code&gt;image.pullSecret&lt;/code&gt; being set. This
sort of conditional becomes very important when making your Helm Chart portable across
different Kubernetes platforms.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Next edit your &lt;code&gt;helm/templates/service.yaml&lt;/code&gt; to look like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: Service
metadata:
  creationTimestamp: 2017-10-03T15:03:30Z
  labels:
    run: &amp;quot;{{ .Release.Name }}&amp;quot;
    chart: &amp;quot;{{ .Chart.Name }}-{{ .Chart.Version }}&amp;quot;
    release: &amp;quot;{{ .Release.Name }}&amp;quot;
    heritage: &amp;quot;{{ .Release.Service }}&amp;quot;  
  name: &amp;quot;{{ .Release.Name }}&amp;quot;
  namespace: default
  resourceVersion: &amp;quot;3066&amp;quot;
  selfLink: /api/v1/namespaces/default/services/example
  uid: 044d2b7e-a84c-11e7-a417-0800277e13b3
spec:
  clusterIP:
  externalTrafficPolicy: Cluster
  ports:
  - nodePort: 30254
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    run: &amp;quot;{{ .Release.Name }}&amp;quot;
  sessionAffinity: None
  type: &amp;quot;{{ .Values.service.type }}&amp;quot;
status:
  loadBalancer: {}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once your files are written out you should be able to install the Helm Chart:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm install -n second helm
NAME:   second
LAST DEPLOYED: Tue Oct  3 10:59:41 2017
NAMESPACE: default
STATUS: DEPLOYED

RESOURCES:
==&amp;gt; v1/Service
NAME    CLUSTER-IP  EXTERNAL-IP  PORT(S)       AGE
second  10.0.0.160  &amp;lt;nodes&amp;gt;      80:30254/TCP  1s

==&amp;gt; v1beta1/Deployment
NAME    DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
second  1        1        1           0          1s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next use minikube to get the URL of the service, but since we templated the
service name to match the release you&amp;rsquo;ll want to use this new name:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ minikube service second --url
http://192.168.99.100:30254
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now lets try something fun. Change the image we&amp;rsquo;re using by upgrading the helm release
and overriding some values on the command line:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ helm upgrade --set image.repository=httpd --set image.tag=2.2.34-alpine second helm
Release &amp;quot;second&amp;quot; has been upgraded. Happy Helming!
LAST DEPLOYED: Tue Oct  3 11:09:30 2017
NAMESPACE: default
STATUS: DEPLOYED

RESOURCES:
==&amp;gt; v1/Service
NAME    CLUSTER-IP  EXTERNAL-IP  PORT(S)       AGE
second  10.0.0.160  &amp;lt;nodes&amp;gt;      80:30254/TCP  9m

==&amp;gt; v1beta1/Deployment
NAME    DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
second  1        1        1           0          9m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then go to our minikube provided URL and you&amp;rsquo;ll see a different message &lt;code&gt;It works!&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;clean-up&#34;&gt;Clean up&lt;/h2&gt;

&lt;p&gt;use &lt;code&gt;minikube delete&lt;/code&gt; to clean up your environment:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;minikube delete
Deleting local Kubernetes cluster...
Machine deleted.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Helm is a very powerful way to package up your Kubernetes manifests to make them
extensible and portable. While it is quite complicated its fairly easy to get started
with it and if you&amp;rsquo;re like me you&amp;rsquo;ll find yourself replacing the Kubernetes manifests
in your code repos with Helm Charts.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s a lot more you can do with Helm, we&amp;rsquo;ve just scratched the surface. Enjoy
using and learning more about them!&lt;/p&gt;
</content>
    </item>
    
  </channel>
</rss>